{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing for Data Scientists\n",
    "## 06\n",
    "#### Using CUDA, Jupyter, PyCUDA, ArrayFire and Thrust\n",
    "\n",
    "\n",
    "https://github.com/QuantScientist/Data-Science-ArrayFire-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycuda\n",
    "%reset -f\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "# imports\n",
    "import numpy as np                     # numeric python lib\n",
    "import matplotlib.image as mpimg       # reading images to numpy arrays\n",
    "\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots\n",
    "\n",
    "import tensorflow as tf \n",
    "print(\"tensorflow:\" + tf.__version__)\n",
    "!set \"KERAS_BACKEND=tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot   # Library to plot\n",
    "import matplotlib.cm as colormap   # Library to plot\n",
    "import caffe\n",
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "# http://christopher5106.github.io/deep/learning/2015/09/04/Deep-learning-tutorial-on-Caffe-Technology.html\n",
    "import time\n",
    "\n",
    "import multiprocessing\n",
    "import threading\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from multiprocessing import Process, Pipe, sharedctypes, Lock\n",
    "from threading import Thread\n",
    "\n",
    "MAXCPU=0\n",
    "try:\n",
    "    MAXCPU = multiprocessing.cpu_count()\n",
    "except:\n",
    "    MAXCPU = 0\n",
    "print 'MAXCPU:' + str(MAXCPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 device(s) found.\n",
      "Device #0: GeForce GTX 1080\n",
      "<type 'list'>\n",
      "MAX_THREADS_PER_BLOCK:[]\n",
      "<module 'pycuda.driver' from '/usr/local/lib/python2.7/dist-packages/pycuda/driver.pyc'>\n"
     ]
    }
   ],
   "source": [
    "from pycuda.compiler import SourceModule\n",
    "import pycuda\n",
    "from pycuda import compiler\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit             # PyCuda autoinit\n",
    "import pycuda.driver as cuda       # PyCuda In, Out helpers\n",
    "import matplotlib.pyplot as plot   # Library to plot\n",
    "import matplotlib.cm as colormap   # Library to plot\n",
    "import numpy                       # Fast math library\n",
    "import time\n",
    "import pycuda.driver as drv\n",
    "drv.init()\n",
    "MAX_THREADS_PER_BLOCK=512 # backward compatible\n",
    "print(\"%d device(s) found.\" % cuda.Device.count())           \n",
    "\n",
    "# All GPU attributes\n",
    "for ordinal in range(drv.Device.count()):\n",
    "    dev = drv.Device(ordinal)    \n",
    "    print \"Device #%d: %s\" % (ordinal, dev.name())   \n",
    "    \n",
    "    atts = [(str(att), value) \n",
    "            for att, value in dev.get_attributes().iteritems()]    \n",
    "    atts.sort    \n",
    "    print type(atts)\n",
    "\n",
    "# MAX_THREADS_PER_BLOCK=filter(lambda s: s == 'MAX_THREADS_PER_BLOCK' , atts) \n",
    "# print 'MAX_THREADS_PER_BLOCK:' + str(MAX_THREADS_PER_BLOCK)\n",
    "    \n",
    "print cuda\n",
    "\n",
    "# Get the max possible therads per block limit\n",
    "# MAX_THREADS_PER_BLOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Blocks  &  Threads\n",
    "\n",
    "  MAX_BLOCK_DIM_X: 1024\n",
    "  MAX_BLOCK_DIM_Y: 1024\n",
    "  MAX_BLOCK_DIM_Z: 64\n",
    "  MAX_GRID_DIM_X: 2147483647\n",
    "  MAX_GRID_DIM_Y: 65535\n",
    "  MAX_GRID_DIM_Z: 65535\n",
    "  MAX_PITCH: 2147483647\n",
    "  MAX_REGISTERS_PER_BLOCK: 65536\n",
    "  MAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
    "  MAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
    "  MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 98304\n",
    "  MAX_THREADS_PER_BLOCK: 1024\n",
    "  MAX_THREADS_PER_MULTIPROCESSOR: 2048\n",
    "  MEMORY_CLOCK_RATE: 5005000\n",
    "  MULTIPROCESSOR_COUNT: 20\n",
    "  MULTI_GPU_BOARD: 0\n",
    "  MULTI_GPU_BOARD_GROUP_ID: 0\n",
    "  PCI_BUS_ID: 1\n",
    "  PCI_DEVICE_ID: 0\n",
    "  PCI_DOMAIN_ID: 0\n",
    "  STREAM_PRIORITIES_SUPPORTED: 1\n",
    "  SURFACE_ALIGNMENT: 512\n",
    "  TCC_DRIVER: 0\n",
    "  TEXTURE_ALIGNMENT: 512\n",
    "  TEXTURE_PITCH_ALIGNMENT: 32\n",
    "  TOTAL_CONSTANT_MEMORY: 65536\n",
    "  UNIFIED_ADDRESSING: 1\n",
    "  WARP_SIZE: 32\n",
    "  \n",
    "-  Maximum thread size for GPU is dependent on GPU, but normally 512.\n",
    "-  Threads per block should be a multiple of 32.\n",
    "-  Block and Grid Size is dependent on the image.\n",
    "-  This example uses a 256x256 pixel image. A 2D block (16x16) and a 1D grid (256,1) is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pycuda.compiler.SourceModule object at 0x7fefb972e5d0>\n",
      "<pycuda._driver.Function object at 0x7fefb99c2950>\n"
     ]
    }
   ],
   "source": [
    "#Kernel text\n",
    "kernel = \"\"\"\n",
    " \n",
    "    __global__ void bw( float *inIm, int check ){\n",
    " \n",
    "        int idx = (threadIdx.x ) + blockDim.x * blockIdx.x ;\n",
    " \n",
    "        if(idx *3 < check*3)\n",
    "        {\n",
    "        int val = 0.21 *inIm[idx*3] + 0.71*inIm[idx*3+1] + 0.07 * inIm[idx*3+2];        \n",
    "        //int val = (inIm[idx*3] + inIm[idx*3+1] + inIm[idx*3+2])/3.0;\n",
    " \n",
    "        inIm[idx*3]= val;\n",
    "        inIm[idx*3+1]= val;\n",
    "        inIm[idx*3+2]= val;\n",
    "        }\n",
    "    }         \n",
    "    \"\"\"\n",
    " \n",
    "#Compile and get kernel function\n",
    "mod = SourceModule(kernel)\n",
    "print mod\n",
    "func = mod.get_function(\"bw\")\n",
    "print func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "def filterImageOnGPU(inPath, filterFunc):              \n",
    "    im = PILImage.open(inPath)            \n",
    "    px = numpy.array(im).astype(numpy.float32)\n",
    "        \n",
    "    print 'Size:' + str(im.size)\n",
    "    print 'Pixels:' + str (im.size[0]*im.size[1])\n",
    "    \n",
    "    d_px = cuda.mem_alloc(px.nbytes)\n",
    "    cuda.memcpy_htod(d_px, px)\n",
    "      \n",
    "    BLOCK_SIZE = 1024\n",
    "    block = (BLOCK_SIZE,1,1)\n",
    "    print ('Block:' + str (block))\n",
    "    totalPixels = numpy.int32(im.size[0]*im.size[1])\n",
    "    print ('TotalPixels:' + str (totalPixels))\n",
    "    gridRounded=int(im.size[0]*im.size[1]/BLOCK_SIZE)+1\n",
    "    print ('GridRounded:' + str (gridRounded))\n",
    "    print ('BLOCK_SIZE * GridRounded:' + str (gridRounded*BLOCK_SIZE))\n",
    "    grid = (gridRounded,1,1)\n",
    "    print ('Grid:' + str (grid))\n",
    "   \n",
    "    filterFunc(d_px,totalPixels, block=block,grid = grid)\n",
    "   \n",
    "    bwPx = numpy.empty_like(px)\n",
    "    cuda.memcpy_dtoh(bwPx, d_px)\n",
    "    # On monochrome images, Pixels are uint8 [0,255].    \n",
    "#     numpy.clip(bwPx, 0, 255, out=bwPx)\n",
    "#     bwPx = bwPx.astype('uint8')\n",
    "    bwPx = (numpy.uint8(bwPx))       \n",
    "    pil_im = PILImage.fromarray(bwPx,mode =\"RGB\")           \n",
    "    return pil_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:(10700, 7600)\n",
      "Pixels:81320000\n",
      "Block:(1024, 1, 1)\n",
      "TotalPixels:81320000\n",
      "GridRounded:79415\n",
      "BLOCK_SIZE * GridRounded:81320960\n",
      "Grid:(79415, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefb972e2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plot.figure()\n",
    "figure.set_size_inches(10, 10)\n",
    "filterImageOnGPU('images/nematode.jpg',func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}