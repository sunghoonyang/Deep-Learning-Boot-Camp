{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Bootcamp November 2017, GPU Computing for Data Scientists\n",
    "\n",
    "<img src=\"../images/bcamp.png\" align=\"center\">\n",
    "\n",
    "## 18  PyTorch NUMER.AI  Deep Learning Binary Classification using BCELoss \n",
    "\n",
    "Web: https://www.meetup.com/Tel-Aviv-Deep-Learning-Bootcamp/events/241762893/\n",
    "\n",
    "Notebooks: <a href=\"https://github.com/QuantScientist/Data-Science-PyCUDA-GPU\"> On GitHub</a>\n",
    "\n",
    "*Shlomo Kashani*\n",
    "\n",
    "<img src=\"../images/pt.jpg\" width=\"35%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "- This tutorial was written in order to demonstrate a **fully working** example of a PyTorch NN on a real world use case, namely a Binary Classification problem. If you are interested in the sk-learn version of this problem please refer to: https://github.com/QuantScientist/deep-ml-meetups/tree/master/hacking-kaggle/python/numer-ai \n",
    "\n",
    "- For the scientific foundation behind Binary Classification and Logistic Regression, refer to: https://github.com/QuantScientist/Deep-Learning-Boot-Camp/tree/master/Data-Science-Interviews-Book\n",
    "\n",
    "- Every step, from reading the CSV into numpy arrays, converting to GPU based tensors, training and validation, are meant to aid newcomers in their first steps in PyTorch. \n",
    "\n",
    "- Additionally, commonly used Kaggle metrics such as ROC_AUC and LOG_LOSS are logged and plotted both for the training set as well as for the validation set. \n",
    "\n",
    "- Thus, the NN architecture is naive and by no means **optimized**. Hopefully, I will improve it over time and I am working on a second CNN based version of the same problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Data\n",
    "- Download from https://numer.ai/leaderboard\n",
    "\n",
    "<img src=\"../images/Numerai.png\" width=\"35%\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:1.2.1\n",
      "__Python VERSION: 2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "__pyTorch VERSION: 0.2.0+42448cf\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: None\n",
      "__Number CUDA Devices: 0\n",
      "__Devices\n",
      "OS:  linux2\n",
      "Python:  2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "PyTorch:  0.2.0+42448cf\n",
      "Numpy:  1.13.1\n",
      "2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "0.0\n",
      "svmem(total=67469099008, available=59280621568, percent=12.1, used=7519043584, free=44637093888, active=16349835264, inactive=5060030464, buffers=1092038656, cached=14220922880, shared=121167872)\n",
      "memory GB: 0.21879196167\n"
     ]
    }
   ],
   "source": [
    "# !pip install pycuda\n",
    "%reset -f\n",
    "# %%timeit\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "import logging\n",
    "import numpy\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots\n",
    "import tensorflow as tf \n",
    "print(\"tensorflow:\" + tf.__version__)\n",
    "!set \"KERAS_BACKEND=tensorflow\"\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu75/torch-0.2.0.post1-cp27-cp27mu-manylinux1_x86_64.whl\n",
    "# !pip install torchvision \n",
    "# ! pip install cv2\n",
    "# import cv2\n",
    "\n",
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:USE CUDA=False\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))\n",
    "\n",
    "# ! watch -n 0.1 'ps f -o user,pgrp,pid,pcpu,pmem,start,time,command -p `lsof -n -w -t /dev/nvidia*`'\n",
    "# sudo apt-get install dstat #install dstat\n",
    "# sudo pip install nvidia-ml-py #install Python NVIDIA Management Library\n",
    "# wget https://raw.githubusercontent.com/datumbox/dstat/master/plugins/dstat_nvidia_gpu.py\n",
    "# sudo mv dstat_nvidia_gpu.py /usr/share/dstat/ #move file to the plugins directory of dstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN params\n",
    "DROPOUT_PROB = 0.75\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.005\n",
    "MOMENTUM= 0.9\n",
    "PIN_MEMORY=use_cuda # True IF CUDA\n",
    "\n",
    "# Data params\n",
    "TARGET_VAR= 'target'\n",
    "TOURNAMENT_DATA_CSV = 'numerai_tournament_data.csv'\n",
    "TRAINING_DATA_CSV = 'numerai_training_data.csv'\n",
    "BASE_FOLDER = 'numerai/'\n",
    "\n",
    "# fix seed\n",
    "seed=17*19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Load a CSV file for Binary classification (numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135682</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.53352</td>\n",
       "      <td>0.64336</td>\n",
       "      <td>0.46577</td>\n",
       "      <td>0.53001</td>\n",
       "      <td>0.55734</td>\n",
       "      <td>0.45773</td>\n",
       "      <td>0.41169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51224</td>\n",
       "      <td>0.50484</td>\n",
       "      <td>0.41929</td>\n",
       "      <td>0.50954</td>\n",
       "      <td>0.47383</td>\n",
       "      <td>0.48797</td>\n",
       "      <td>0.38373</td>\n",
       "      <td>0.46233</td>\n",
       "      <td>0.33341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110546</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.54196</td>\n",
       "      <td>0.81576</td>\n",
       "      <td>0.46632</td>\n",
       "      <td>0.62320</td>\n",
       "      <td>0.52427</td>\n",
       "      <td>0.64378</td>\n",
       "      <td>0.55662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52643</td>\n",
       "      <td>0.63809</td>\n",
       "      <td>0.67121</td>\n",
       "      <td>0.49421</td>\n",
       "      <td>0.45291</td>\n",
       "      <td>0.46932</td>\n",
       "      <td>0.54445</td>\n",
       "      <td>0.30997</td>\n",
       "      <td>0.19023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76047</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.49158</td>\n",
       "      <td>0.69131</td>\n",
       "      <td>0.57816</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.43064</td>\n",
       "      <td>0.49986</td>\n",
       "      <td>0.61902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.43310</td>\n",
       "      <td>0.72286</td>\n",
       "      <td>0.76257</td>\n",
       "      <td>0.36600</td>\n",
       "      <td>0.55330</td>\n",
       "      <td>0.56566</td>\n",
       "      <td>0.67528</td>\n",
       "      <td>0.34960</td>\n",
       "      <td>0.25721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66098</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.54519</td>\n",
       "      <td>0.42473</td>\n",
       "      <td>0.63472</td>\n",
       "      <td>0.39003</td>\n",
       "      <td>0.37485</td>\n",
       "      <td>0.43810</td>\n",
       "      <td>0.59557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.41658</td>\n",
       "      <td>0.63417</td>\n",
       "      <td>0.50189</td>\n",
       "      <td>0.40883</td>\n",
       "      <td>0.58705</td>\n",
       "      <td>0.63785</td>\n",
       "      <td>0.56225</td>\n",
       "      <td>0.55989</td>\n",
       "      <td>0.58642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88227</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.44307</td>\n",
       "      <td>0.74076</td>\n",
       "      <td>0.52210</td>\n",
       "      <td>0.56543</td>\n",
       "      <td>0.51125</td>\n",
       "      <td>0.66457</td>\n",
       "      <td>0.42263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45851</td>\n",
       "      <td>0.58805</td>\n",
       "      <td>0.49860</td>\n",
       "      <td>0.48023</td>\n",
       "      <td>0.52606</td>\n",
       "      <td>0.53253</td>\n",
       "      <td>0.38361</td>\n",
       "      <td>0.43829</td>\n",
       "      <td>0.25014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   era data_type  feature1  feature2  feature3  feature4  feature5  \\\n",
       "0  135682  era1     train   0.53352   0.64336   0.46577   0.53001   0.55734   \n",
       "1  110546  era1     train   0.54196   0.81576   0.46632   0.62320   0.52427   \n",
       "2   76047  era1     train   0.49158   0.69131   0.57816   0.54010   0.43064   \n",
       "3   66098  era1     train   0.54519   0.42473   0.63472   0.39003   0.37485   \n",
       "4   88227  era1     train   0.44307   0.74076   0.52210   0.56543   0.51125   \n",
       "\n",
       "   feature6  feature7   ...    feature13  feature14  feature15  feature16  \\\n",
       "0   0.45773   0.41169   ...      0.51224    0.50484    0.41929    0.50954   \n",
       "1   0.64378   0.55662   ...      0.52643    0.63809    0.67121    0.49421   \n",
       "2   0.49986   0.61902   ...      0.43310    0.72286    0.76257    0.36600   \n",
       "3   0.43810   0.59557   ...      0.41658    0.63417    0.50189    0.40883   \n",
       "4   0.66457   0.42263   ...      0.45851    0.58805    0.49860    0.48023   \n",
       "\n",
       "   feature17  feature18  feature19  feature20  feature21  target  \n",
       "0    0.47383    0.48797    0.38373    0.46233    0.33341       0  \n",
       "1    0.45291    0.46932    0.54445    0.30997    0.19023       0  \n",
       "2    0.55330    0.56566    0.67528    0.34960    0.25721       1  \n",
       "3    0.58705    0.63785    0.56225    0.55989    0.58642       0  \n",
       "4    0.52606    0.53253    0.38361    0.43829    0.25014       0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "df_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Feature enrichement\n",
    "- This would be usually not required when using NN's; it is here for demonstration purposes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genBasicFeatures(inDF):\n",
    "    print('Generating basic features ...')\n",
    "    df_copy=inDF.copy(deep=True)\n",
    "    magicNumber=21\n",
    "    feature_cols = list(inDF.columns)\n",
    "\n",
    "    inDF['x_mean'] = np.mean(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "    inDF['x_median'] = np.median(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "    inDF['x_std'] = np.std(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "    inDF['x_skew'] = scipy.stats.skew(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "    inDF['x_kurt'] = scipy.stats.kurtosis(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "    inDF['x_var'] = np.var(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "    inDF['x_max'] = np.max(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "    inDF['x_min'] = np.min(df_copy.ix[:, 0:magicNumber], axis=1)    \n",
    "\n",
    "    return inDF\n",
    "\n",
    "def addPolyFeatures(inDF, deg=2):\n",
    "    print('Generating poly features ...')\n",
    "    df_copy=inDF.copy(deep=True)\n",
    "    poly=PolynomialFeatures(degree=deg)\n",
    "    p_testX = poly.fit(df_copy)\n",
    "    # AttributeError: 'PolynomialFeatures' object has no attribute 'get_feature_names'\n",
    "    target_feature_names = ['x'.join(['{}^{}'.format(pair[0],pair[1]) for pair in tuple if pair[1]!=0]) for tuple in [zip(df_copy.columns,p) for p in poly.powers_]]\n",
    "    df_copy = pd.DataFrame(p_testX.transform(df_copy),columns=target_feature_names)\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Train / Validation / Test Split\n",
    "- Numerai provides a data set that is allready split into train, validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train, Validation, Test Split\n",
    "def loadDataSplit():\n",
    "    df_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)\n",
    "    # TOURNAMENT_DATA_CSV has both validation and test data provided by NumerAI\n",
    "    df_test_valid = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)\n",
    "\n",
    "    answers_1_SINGLE = df_train[TARGET_VAR]\n",
    "    df_train.drop(TARGET_VAR, axis=1,inplace=True)\n",
    "    df_train.drop('id', axis=1,inplace=True)\n",
    "    df_train.drop('era', axis=1,inplace=True)\n",
    "    df_train.drop('data_type', axis=1,inplace=True)    \n",
    "    \n",
    "    # Add polynomial features    \n",
    "#     df_train=genBasicFeatures(df_train)\n",
    "#     df_train = addPolyFeatures(df_train)\n",
    "\n",
    "    df_train.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=False,  index = False)    \n",
    "    df_train= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=None, dtype=np.float32)    \n",
    "    df_train = pd.concat([df_train, answers_1_SINGLE], axis=1)\n",
    "    feature_cols = list(df_train.columns[:-1])\n",
    "#     print (feature_cols)\n",
    "    target_col = df_train.columns[-1]\n",
    "    trainX, trainY = df_train[feature_cols], df_train[target_col]\n",
    "    \n",
    "    \n",
    "    # TOURNAMENT_DATA_CSV has both validation and test data provided by NumerAI\n",
    "    # Validation set\n",
    "    df_validation_set=df_test_valid.loc[df_test_valid['data_type'] == 'validation'] \n",
    "    df_validation_set=df_validation_set.copy(deep=True)\n",
    "    answers_1_SINGLE_validation = df_validation_set[TARGET_VAR]\n",
    "    df_validation_set.drop(TARGET_VAR, axis=1,inplace=True)    \n",
    "    df_validation_set.drop('id', axis=1,inplace=True)\n",
    "    df_validation_set.drop('era', axis=1,inplace=True)\n",
    "    df_validation_set.drop('data_type', axis=1,inplace=True)\n",
    "    \n",
    "   # Add polynomial features    \n",
    "#     df_validation_set=genBasicFeatures(df_validation_set)\n",
    "#     df_validation_set = addPolyFeatures(df_validation_set)\n",
    "    \n",
    "    df_validation_set.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=False,  index = False)    \n",
    "    df_validation_set= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=None, dtype=np.float32)    \n",
    "    df_validation_set = pd.concat([df_validation_set, answers_1_SINGLE_validation], axis=1)\n",
    "    feature_cols = list(df_validation_set.columns[:-1])\n",
    "\n",
    "    target_col = df_validation_set.columns[-1]\n",
    "    valX, valY = df_validation_set[feature_cols], df_validation_set[target_col]\n",
    "                            \n",
    "    # Test set for submission (not labeled)    \n",
    "    df_test_set = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)\n",
    "#     df_test_set=df_test_set.loc[df_test_valid['data_type'] == 'live'] \n",
    "    df_test_set=df_test_set.copy(deep=True)\n",
    "    df_test_set.drop(TARGET_VAR, axis=1,inplace=True)\n",
    "    tid_1_SINGLE = df_test_set['id']\n",
    "    df_test_set.drop('id', axis=1,inplace=True)\n",
    "    df_test_set.drop('era', axis=1,inplace=True)\n",
    "    df_test_set.drop('data_type', axis=1,inplace=True)   \n",
    "    \n",
    "    # Add polynomial features    \n",
    "#     df_test_set=genBasicFeatures(df_test_set)\n",
    "#     df_test_set = addPolyFeatures(df_test_set)\n",
    "   \n",
    "    \n",
    "    feature_cols = list(df_test_set.columns) # must be run here, we dont want the ID    \n",
    "#     print (feature_cols)\n",
    "    df_test_set = pd.concat([tid_1_SINGLE, df_test_set], axis=1)            \n",
    "    testX = df_test_set[feature_cols].values\n",
    "        \n",
    "    return trainX, trainY, valX, valY, testX, df_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108405, 21)\n",
      "(108405,)\n",
      "(16686, 21)\n",
      "(16686,)\n",
      "(45647, 21)\n",
      "(45647, 22)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "trainX, trainY, valX, valY, testX, df_test_set = loadDataSplit()\n",
    "# # Number of features for the input layer\n",
    "N_FEATURES=trainX.shape[1]\n",
    "print (trainX.shape)\n",
    "print (trainY.shape)\n",
    "print (valX.shape)\n",
    "print (valY.shape)\n",
    "print (testX.shape)\n",
    "print (df_test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Create PyTorch GPU tensors from numpy arrays\n",
    "\n",
    "- Note how we transfrom the np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np.values, dtype=np.float32)        \n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")\n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "    \n",
    "    print(type(X_tensor.data)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))    \n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n",
    "\n",
    "    print(type(Y_tensor.data)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))    \n",
    "    return Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The NN model\n",
    "\n",
    "### MLP model\n",
    "- A multilayer perceptron is a logistic regressor where instead of feeding the input to the logistic regression you insert a intermediate layer, called the hidden layer, that has a nonlinear activation function (usually tanh or sigmoid) . One can use many such hidden layers making the architecture deep.\n",
    "\n",
    "- Here we define a simple MLP structure. We map the input feature vector to a higher space, then later gradually decrease the dimension, and in the end into a 1-dimension space. Because we are calculating the probability of each genre independently, after the final layer we need to use a sigmoid layer. \n",
    "\n",
    "###  Initial weights selection\n",
    "\n",
    "- There are many ways to select the initial weights to a neural network architecture. A common initialization scheme is random initialization, which sets the biases and weights of all the nodes in each hidden layer randomly.\n",
    "\n",
    "- Before starting the training process, an initial value is assigned to each variable. This is done by pure randomness, using for example a uniform or Gaussian distribution. But if we start with weights that are too small, the signal could decrease so much that it is too small to be useful. On the other side, when the parameters are initialized with high values, the signal can end up to explode while propagating through the network.\n",
    "\n",
    "- In consequence, a good initialization can have a radical effect on how fast the network will learn useful patterns.For this purpose, some best practices have been developed. One famous example used is **Xavier initialization**. Its formulation is based on the number of input and output neurons and uses sampling from a uniform distribution with zero mean and all biases set to zero.\n",
    "\n",
    "- In effect (according to theory) initializing the weights of the network to values that would be closer to the optimal, and therefore require less epochs to train.\n",
    "\n",
    "### References: \n",
    "* **`nninit.xavier_uniform(tensor, gain=1)`** - Fills `tensor` with values according to the method described in [\"Understanding the difficulty of training deep feedforward neural networks\" - Glorot, X. and Bengio, Y.](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf), using a uniform distribution.\n",
    "* **`nninit.xavier_normal(tensor, gain=1)`** - Fills `tensor` with values according to the method described in [\"Understanding the difficulty of training deep feedforward neural networks\" - Glorot, X. and Bengio, Y.](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf), using a normal distribution.\n",
    "* **`nninit.kaiming_uniform(tensor, gain=1)`** - Fills `tensor` with values according to the method described in [\"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification\" - He, K. et al.](https://arxiv.org/abs/1502.01852) using a uniform distribution.\n",
    "* **`nninit.kaiming_normal(tensor, gain=1)`** - Fills `tensor` with values according to the method described in [\"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification\" - He, K. et al.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sequential (\n",
      "  (0): Linear (21 -> 1024)\n",
      "  (1): Dropout (p = 0.05)\n",
      "  (2): Tanh ()\n",
      "  (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (4): Linear (1024 -> 128)\n",
      "  (5): Dropout (p = 0.05)\n",
      "  (6): Tanh ()\n",
      "  (7): Linear (128 -> 64)\n",
      "  (8): Dropout (p = 0.05)\n",
      "  (9): LeakyReLU (0.01)\n",
      "  (10): Linear (64 -> 32)\n",
      "  (11): Dropout (p = 0.05)\n",
      "  (12): Tanh ()\n",
      "  (13): Linear (32 -> 16)\n",
      "  (14): Dropout (p = 0.05)\n",
      "  (15): LeakyReLU (0.01)\n",
      "  (16): Linear (16 -> 1)\n",
      "  (17): Sigmoid ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# p is the probability of being dropped in PyTorch\n",
    "# At each layer, DECREASE dropout\n",
    "dropout = torch.nn.Dropout(p=1 - (DROPOUT_PROB +0.20))\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output,initKernel='uniform'):\n",
    "        super(Net2, self).__init__()\n",
    "        self.dis = nn.Sequential(\n",
    "            nn.Linear(n_feature, n_hidden),\n",
    "            dropout,\n",
    "            nn.LeakyReLU(0.1),\n",
    "                                    \n",
    "            nn.Linear(n_hidden, int(n_hidden /2)),\n",
    "            dropout,\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Linear(int(n_hidden /2), int(n_hidden /4)),\n",
    "            dropout,\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Linear(int(n_hidden /4), 1),            \n",
    "            nn.Sigmoid()\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        x = self.dis(x)\n",
    "        return x\n",
    "\n",
    "hiddenLayer1Size=1024\n",
    "hiddenLayer2Size=int(hiddenLayer1Size/8)\n",
    "hiddenLayer3Size=int(hiddenLayer1Size/16)\n",
    "hiddenLayer4Size=int(hiddenLayer1Size/32)\n",
    "hiddenLayer5Size=int(hiddenLayer1Size/64)\n",
    "\n",
    "linear1=torch.nn.Linear(N_FEATURES, hiddenLayer1Size, bias=True) \n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "\n",
    "linear2=torch.nn.Linear(hiddenLayer1Size, hiddenLayer2Size)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "\n",
    "linear3=torch.nn.Linear(hiddenLayer2Size, hiddenLayer3Size)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "\n",
    "linear4=torch.nn.Linear(hiddenLayer3Size, hiddenLayer4Size)\n",
    "torch.nn.init.xavier_uniform(linear4.weight)\n",
    "\n",
    "linear5=torch.nn.Linear(hiddenLayer4Size, hiddenLayer5Size)\n",
    "torch.nn.init.xavier_uniform(linear5.weight)\n",
    "\n",
    "linear6=torch.nn.Linear(hiddenLayer5Size, 1)\n",
    "torch.nn.init.xavier_uniform(linear6.weight)\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "tanh=torch.nn.Tanh()\n",
    "relu=torch.nn.LeakyReLU()\n",
    "\n",
    "net = torch.nn.Sequential(linear1,dropout,tanh,nn.BatchNorm1d(hiddenLayer1Size),\n",
    "                          linear2,dropout,tanh,\n",
    "                          linear3,dropout,relu,\n",
    "                          linear4,dropout,tanh,\n",
    "                          linear5,dropout,relu,\n",
    "                          linear6,sigmoid\n",
    "                          )\n",
    "\n",
    "# net = Net(n_feature=N_FEATURES, n_hidden=1024, n_output=1)   # define the network\n",
    "# net = Net2(n_feature=N_FEATURES, n_hidden=512, n_output=1)   # define the network\n",
    "\n",
    "lgr.info(net)  # net architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Print the full net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sequential (\n",
      "  (0): Linear (21 -> 1024), weights=((1024L, 21L), (1024L,)), parameters=22528\n",
      "  (1): Dropout (p = 0.05), weights=(), parameters=0\n",
      "  (2): Tanh (), weights=(), parameters=0\n",
      "  (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True), weights=((1024L,), (1024L,)), parameters=2048\n",
      "  (4): Linear (1024 -> 128), weights=((128L, 1024L), (128L,)), parameters=131200\n",
      "  (5): Dropout (p = 0.05), weights=(), parameters=0\n",
      "  (6): Tanh (), weights=(), parameters=0\n",
      "  (7): Linear (128 -> 64), weights=((64L, 128L), (64L,)), parameters=8256\n",
      "  (8): Dropout (p = 0.05), weights=(), parameters=0\n",
      "  (9): LeakyReLU (0.01), weights=(), parameters=0\n",
      "  (10): Linear (64 -> 32), weights=((32L, 64L), (32L,)), parameters=2080\n",
      "  (11): Dropout (p = 0.05), weights=(), parameters=0\n",
      "  (12): Tanh (), weights=(), parameters=0\n",
      "  (13): Linear (32 -> 16), weights=((16L, 32L), (16L,)), parameters=528\n",
      "  (14): Dropout (p = 0.05), weights=(), parameters=0\n",
      "  (15): LeakyReLU (0.01), weights=(), parameters=0\n",
      "  (16): Linear (16 -> 1), weights=((1L, 16L), (1L,)), parameters=17\n",
      "  (17): Sigmoid (), weights=(), parameters=0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Taken from https://stackoverflow.com/questions/42480111/model-summary-in-pytorch/42616812\n",
    "from torch.nn.modules.module import _addindent\n",
    "import torch\n",
    "import numpy as np\n",
    "def torch_summarize(model, show_weights=True, show_parameters=True):\n",
    "    \"\"\"Summarizes torch model by showing trainable parameters and weights.\"\"\"\n",
    "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
    "    for key, module in model._modules.items():\n",
    "        # if it contains layers let call it recursively to get params and weights\n",
    "        if type(module) in [\n",
    "            torch.nn.modules.container.Container,\n",
    "            torch.nn.modules.container.Sequential\n",
    "        ]:\n",
    "            modstr = torch_summarize(module)\n",
    "        else:\n",
    "            modstr = module.__repr__()\n",
    "        modstr = _addindent(modstr, 2)\n",
    "\n",
    "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
    "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
    "\n",
    "        tmpstr += '  (' + key + '): ' + modstr \n",
    "        if show_weights:\n",
    "            tmpstr += ', weights={}'.format(weights)\n",
    "        if show_parameters:\n",
    "            tmpstr +=  ', parameters={}'.format(params)\n",
    "        tmpstr += '\\n'   \n",
    "\n",
    "    tmpstr = tmpstr + ')'\n",
    "    return tmpstr\n",
    "\n",
    "lgr.info(torch_summarize(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Loss and Optimizer\n",
    "\n",
    "###  BCELoss\n",
    "- In addition, we will calculate the binary cross entropy loss (BCELoss). Luckily we have one loss function already present. For details please checkout http://pytorch.org/docs/master/nn.html. \n",
    "\n",
    "- ** NOTE this BCELoss may not be numerical stable, although it's fine during my training process.**\n",
    "\n",
    "### Optimization\n",
    "\n",
    "- if return F.log_softmax(x) then loss = F.nll_loss(output, target) (MNIST)\n",
    "- print(nn.BCEWithLogitsLoss()(o, t)) is equivalent to print(nn.BCELoss()(sigmoid(o), t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\mathbf{Loss Function:} J(x, z) = -\\sum_k^d[x_k \\log z_k + (1-x_k)log(1-z_k)]$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ! pip install sympy\n",
    "import sympy as sp\n",
    "sp.interactive.printing.init_printing(use_latex=True)\n",
    "from IPython.display import display, Math, Latex\n",
    "maths = lambda s: display(Math(s))\n",
    "latex = lambda s: display(Latex(s))\n",
    "\n",
    "#the loss function is as follows:\n",
    "maths(\"\\mathbf{Loss Function:} J(x, z) = -\\sum_k^d[x_k \\log z_k + (1-x_k)log(1-z_k)]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:<torch.optim.adam.Adam object at 0x7f8321797a50>\n",
      "INFO:__main__:BCELoss (\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=5e-4)\n",
    "#L2 regularization can easily be added to the entire model via the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR,weight_decay=5e-4) #  L2 regularization\n",
    "\n",
    "loss_func=torch.nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "# http://andersonjo.github.io/artificial-intelligence/2017/01/07/Cost-Functions/\n",
    "\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    net.cuda()\n",
    "    loss_func.cuda()\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Training in batches  + Measuring the performance of the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the CPU\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:24: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "INFO:__main__:Using the CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108405, 21)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "(108405, 21)\n",
      "<type 'numpy.ndarray'>\n",
      "(108405, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "(108405, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'> <class 'torch.FloatTensor'>\n",
      "0 [ 0.7003122]\n",
      "ACC=0.0, LOG_LOSS=0.710944232764, ROC_AUC=0.5098880681 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6f37cacf07cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()    \n",
    "epochs=100 # change to 1500 for better results\n",
    "all_losses = []\n",
    "\n",
    "X_tensor_train= XnumpyToTensor(trainX)\n",
    "Y_tensor_train= YnumpyToTensor(trainY)\n",
    "\n",
    "print(type(X_tensor_train.data), type(Y_tensor_train.data)) # should be 'torch.cuda.FloatTensor'\n",
    "\n",
    "# From here onwards, we must only use PyTorch Tensors\n",
    "for step in range(epochs):    \n",
    "    out = net(X_tensor_train)                 # input x and predict based on x\n",
    "    cost = loss_func(out, Y_tensor_train)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    cost.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "                   \n",
    "        \n",
    "    if step % 10 == 0:        \n",
    "        loss = cost.data[0]\n",
    "        all_losses.append(loss)\n",
    "        print(step, cost.data.cpu().numpy())\n",
    "        # RuntimeError: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). \n",
    "        # Use .cpu() to move the tensor to host memory first.        \n",
    "        prediction = (net(X_tensor_train).data).float() # probabilities         \n",
    "#         prediction = (net(X_tensor).data > 0.5).float() # zero or one\n",
    "#         print (\"Pred:\" + str (prediction)) # Pred:Variable containing: 0 or 1\n",
    "#         pred_y = prediction.data.numpy().squeeze()            \n",
    "        pred_y = prediction.cpu().numpy().squeeze()\n",
    "        target_y = Y_tensor_train.cpu().data.numpy()\n",
    "                        \n",
    "        tu = ((pred_y == target_y).mean(),log_loss(target_y, pred_y),roc_auc_score(target_y,pred_y ))\n",
    "        print ('ACC={}, LOG_LOSS={}, ROC_AUC={} '.format(*tu))        \n",
    "                \n",
    "end_time = time.time()\n",
    "print ('{} {:6.3f} seconds'.format('GPU:', end_time-start_time))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_losses)\n",
    "plt.show()\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y,pred_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('LOG_LOSS=' + str(log_loss(target_y, pred_y)))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Performance of the deep learning model on the Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:24: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16686, 29)\n",
      "(16686,)\n",
      "(16686, 29)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "(16686, 29)\n",
      "<type 'numpy.ndarray'>\n",
      "(16686, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "(16686, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.cuda.FloatTensor'> <class 'torch.cuda.FloatTensor'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "acc=0.0 log_loss=0.693924313567 roc_auc=0.522455062657 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCmOMMUFZojDGGBPU/wOkpCcqB/R+RgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6643a9650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.eval()\n",
    "# Validation data\n",
    "print (valX.shape)\n",
    "print (valY.shape)\n",
    "\n",
    "X_tensor_val= XnumpyToTensor(valX)\n",
    "Y_tensor_val= YnumpyToTensor(valY)\n",
    "\n",
    "\n",
    "print(type(X_tensor_val.data), type(Y_tensor_val.data)) # should be 'torch.cuda.FloatTensor'\n",
    "\n",
    "predicted_val = (net(X_tensor_val).data).float() # probabilities \n",
    "# predicted_val = (net(X_tensor_val).data > 0.5).float() # zero or one\n",
    "pred_y = predicted_val.cpu().numpy()\n",
    "target_y = Y_tensor_val.cpu().data.numpy()                \n",
    "\n",
    "print (type(pred_y))\n",
    "print (type(target_y))\n",
    "\n",
    "tu = (str ((pred_y == target_y).mean()),log_loss(target_y, pred_y),roc_auc_score(target_y,pred_y ))\n",
    "print ('\\n')\n",
    "print ('acc={} log_loss={} roc_auc={} '.format(*tu))\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y,pred_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('LOG_LOSS=' + str(log_loss(target_y, pred_y)))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# print (pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Submission on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45647, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97040.0</td>\n",
       "      <td>0.519708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65399.0</td>\n",
       "      <td>0.517178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147258.0</td>\n",
       "      <td>0.518780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129573.0</td>\n",
       "      <td>0.519850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134978.0</td>\n",
       "      <td>0.522069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  probability\n",
       "0   97040.0     0.519708\n",
       "1   65399.0     0.517178\n",
       "2  147258.0     0.518780\n",
       "3  129573.0     0.519850\n",
       "4  134978.0     0.522069"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testX, df_test_set\n",
    "# df[df.columns.difference(['b'])]\n",
    "# trainX, trainY, valX, valY, testX, df_test_set = loadDataSplit()\n",
    "\n",
    "print (df_test_set.shape)\n",
    "columns = ['id', 'probability']\n",
    "df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n",
    "df_pred.id.astype(int)\n",
    "\n",
    "for index, row in df_test_set.iterrows():\n",
    "    rwo_no_id=row.drop('id')    \n",
    "#     print (rwo_no_id.values)    \n",
    "    x_data_np = np.array(rwo_no_id.values, dtype=np.float32)        \n",
    "    if use_cuda:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "                    \n",
    "    X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n",
    "    predicted_val = (net(X_tensor_test).data).float() # probabilities     \n",
    "    p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n",
    "    \n",
    "    df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n",
    "\n",
    "df_pred.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Create a CSV with the ID's and the coresponding probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred/pred_0.693924313567_1504720448.33.csv\n"
     ]
    }
   ],
   "source": [
    "df_pred.id=df_pred.id.astype(int)\n",
    "\n",
    "def savePred(df_pred, loss):\n",
    "#     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n",
    "    csv_path = 'pred/pred_{}_{}.csv'.format(loss, (str(time.time())))\n",
    "    df_pred.to_csv(csv_path, columns=('id', 'probability'), index=None)\n",
    "    print (csv_path)\n",
    "    \n",
    "savePred (df_pred, log_loss(target_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "controls": "true",
   "history": "true",
   "mouseWheel": "true",
   "overview": "true",
   "progress": "true",
   "scroll": "true",
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
