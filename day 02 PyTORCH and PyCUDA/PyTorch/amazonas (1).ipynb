{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "_cell_guid": "f3ee9f39-55e1-ee69-2bb6-25c095155e1d"
   },
   "outputs": [],
   "source": [
    "% reset -f\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from torch import np # Torch wrapper for Numpy\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6fde4f7-e8f3-3782-673a-62ce72b652fa"
   },
   "source": [
    "## Setting up global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "_cell_guid": "45d63034-a44c-47e8-7376-2deb00af03a9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT ='/root/data/amz/'\n",
    "\n",
    "IMG_PATH = DATA_ROOT + '/train_small/'\n",
    "IMG_EXT = '.jpg'\n",
    "TRAIN_DATA = DATA_ROOT + '/train_small.csv'\n",
    "\n",
    "IMG_EXTENSIONS = ['.png', '.jpg']\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "_cell_guid": "08a005ca-d963-5434-d60d-72d399cb7fe3"
   },
   "outputs": [],
   "source": [
    "class GenericImageDataset(Dataset):    \n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "        \n",
    "        lgr.info (\"CSV path:\" + csv_path)\n",
    "        lgr.info (\"IMG path:\" + img_path)\n",
    "        tmp_df = pd.read_csv(csv_path, header=None)\n",
    "                        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df[0]        \n",
    "#         self.X_train = self.X_train.ix[1:]\n",
    "        \n",
    "        self.y_train = self.mlb.fit_transform(tmp_df[1].str.split()).astype(np.float32)         \n",
    "                \n",
    "        lgr.info(\"DF:\\n\" + str (self.X_train))\n",
    "        lgr.info (\"self.y_train:\\n\" + str(self.y_train))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lgr.info (\"get item:\" + str(index))\n",
    "        path=self.img_path + self.X_train[index] + self.img_ext\n",
    "#         lgr.info (\" --- get item path:\" + path)\n",
    "        img = Image.open(path)\n",
    "#         img = img.convert('RGB')\n",
    "#         figure = plt.figure()\n",
    "#         figure.set_size_inches(5, 5)        \n",
    "        plt.imshow(img)\n",
    "        if self.transform is not None: # TypeError: batch must contain tensors, numbers, or lists; found <class 'PIL.Image.Image'>\n",
    "            img = self.transform(img)\n",
    "#             print (str (type(img))) # <class 'torch.FloatTensor'>\n",
    "                \n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        l=len(self.X_train.index)\n",
    "        lgr.info (\"Lenght:\" +str(l))\n",
    "        return (l)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "_cell_guid": "98a20a0b-d39e-21a6-232b-990e916f6756"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:CSV path:/root/data/amz//train_small.csv\n",
      "INFO:__main__:IMG path:/root/data/amz//train_small/\n",
      "INFO:__main__:DF:\n",
      "0    train_0\n",
      "1    train_1\n",
      "2    train_2\n",
      "3    train_3\n",
      "4    train_4\n",
      "5    train_5\n",
      "Name: 0, dtype: object\n",
      "INFO:__main__:self.y_train:\n",
      "[[ 0.  0.  1.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.]\n",
      " [ 0.  0.  1.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.]\n",
      " [ 0.  0.  1.  1.  0.]\n",
      " [ 1.  1.  0.  1.  1.]]\n",
      "INFO:__main__:Lenght:6\n",
      "INFO:__main__:get item:0\n",
      "INFO:__main__:get item:2\n",
      "INFO:__main__:get item:4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-400-9c6f492e422e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <class 'torch.FloatTensor'>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torchvision/transforms.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mpic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pic should be Tensor or ndarray'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \"\"\"\n\u001b[0;32m--> 550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "transformations = transforms.Compose([transforms.ToTensor()])\n",
    "dset_train = GenericImageDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,transformations)\n",
    "\n",
    "train_loader = DataLoader(dset_train,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1 # 1 for CUDA\n",
    "                         # pin_memory=True # CUDA only\n",
    "                         )\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print (type(data)) # <class 'torch.FloatTensor'>   \n",
    "    trans = transforms.ToPILImage()\n",
    "    plt.imshow(trans(data))"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 3,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
