{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Bootcamp November 2017, GPU Computing for Data Scientists\n",
    "\n",
    "<img src=\"../images/bcamp.png\" align=\"center\">\n",
    "\n",
    "### 31 Pytorch A simple PyTorch Convolutional Nerual Network (CNN) classifier for Numer.Ai Binary Classification problem using CONV1D (one dimentional convolution). \n",
    "\n",
    "Web: https://www.meetup.com/Tel-Aviv-Deep-Learning-Bootcamp/events/241762893/\n",
    "\n",
    "Notebooks: <a href=\"https://github.com/QuantScientist/Data-Science-PyCUDA-GPU\"> On GitHub</a>\n",
    "\n",
    "*Shlomo Kashani*\n",
    "\n",
    "<img src=\"../images/pt.jpg\" width=\"35%\" align=\"center\">\n",
    "\n",
    "\n",
    "### Data\n",
    "- Download from https://numer.ai/leaderboard\n",
    "\n",
    "<img src=\"../images/Numerai.png\" width=\"35%\" align=\"center\">\n",
    "\n",
    "# One dimetional CNN? Convolutional Nerual Network (CNN) using one dimentional convolution (CONV1D).\n",
    "\n",
    "- Indeed, most of the existing PyTorch examples are using Images, while here we have a CSV with 21 features. Using CONV1D *before or after a Lineer layer* requires the use of **reshaping**, and this is the **whole point of this tutorial**. \n",
    "\n",
    "- Thus, the CNN architecture is naive and by no means **optimized**. Hopefully, I will improve it over time and I am working on a second CNN based version of the same problem. \n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "- This tutorial was written in order to demonstrate a **fully working** example of a PyTorch **CNN** on a real world use case, namely a Binary Classification problem. \n",
    "\n",
    "- If you are interested in the sk-learn version of this problem please refer to: https://github.com/QuantScientist/deep-ml-meetups/tree/master/hacking-kaggle/python/numer-ai \n",
    "\n",
    "- For the scientific foundation behind Binary Classification and Logistic Regression, refer to: https://github.com/QuantScientist/Deep-Learning-Boot-Camp/tree/master/Data-Science-Interviews-Book\n",
    "\n",
    "- Every step, from reading the CSV into numpy arrays, converting to GPU based tensors, training and validation, are meant to aid newcomers in their first steps in PyTorch. \n",
    "\n",
    "- Additionally, commonly used Kaggle metrics such as ROC_AUC and LOG_LOSS are logged and plotted both for the training set as well as for the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:USE CUDA=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "__pyTorch VERSION: 0.2.0+42448cf\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: None\n",
      "__Number CUDA Devices: 0\n",
      "__Devices\n",
      "OS:  linux2\n",
      "Python:  2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "PyTorch:  0.2.0+42448cf\n",
      "Numpy:  1.13.1\n",
      "2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "19.3\n",
      "svmem(total=67469099008, available=45196423168, percent=33.0, used=21583908864, free=31378059264, active=30156333056, inactive=4534312960, buffers=1042288640, cached=13464842240, shared=140500992)\n",
      "memory GB: 1.34589004517\n"
     ]
    }
   ],
   "source": [
    "# !pip install pycuda\n",
    "%reset -f\n",
    "# %%timeit\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "import logging\n",
    "import numpy\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots\n",
    "import tensorflow as tf \n",
    "print(\"tensorflow:\" + tf.__version__)\n",
    "!set \"KERAS_BACKEND=tensorflow\"\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu75/torch-0.2.0.post1-cp27-cp27mu-manylinux1_x86_64.whl\n",
    "# !pip install torchvision \n",
    "# ! pip install cv2\n",
    "# import cv2\n",
    "\n",
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()\n",
    "\n",
    "# %%timeit\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN params\n",
    "LR = 0.005\n",
    "MOMENTUM= 0.9\n",
    "\n",
    "# fix seed\n",
    "seed=17*19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  View the Data\n",
    "- Numerai provides a data set that is allready split into train, validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135682</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.53352</td>\n",
       "      <td>0.64336</td>\n",
       "      <td>0.46577</td>\n",
       "      <td>0.53001</td>\n",
       "      <td>0.55734</td>\n",
       "      <td>0.45773</td>\n",
       "      <td>0.41169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51224</td>\n",
       "      <td>0.50484</td>\n",
       "      <td>0.41929</td>\n",
       "      <td>0.50954</td>\n",
       "      <td>0.47383</td>\n",
       "      <td>0.48797</td>\n",
       "      <td>0.38373</td>\n",
       "      <td>0.46233</td>\n",
       "      <td>0.33341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110546</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.54196</td>\n",
       "      <td>0.81576</td>\n",
       "      <td>0.46632</td>\n",
       "      <td>0.62320</td>\n",
       "      <td>0.52427</td>\n",
       "      <td>0.64378</td>\n",
       "      <td>0.55662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52643</td>\n",
       "      <td>0.63809</td>\n",
       "      <td>0.67121</td>\n",
       "      <td>0.49421</td>\n",
       "      <td>0.45291</td>\n",
       "      <td>0.46932</td>\n",
       "      <td>0.54445</td>\n",
       "      <td>0.30997</td>\n",
       "      <td>0.19023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76047</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.49158</td>\n",
       "      <td>0.69131</td>\n",
       "      <td>0.57816</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.43064</td>\n",
       "      <td>0.49986</td>\n",
       "      <td>0.61902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.43310</td>\n",
       "      <td>0.72286</td>\n",
       "      <td>0.76257</td>\n",
       "      <td>0.36600</td>\n",
       "      <td>0.55330</td>\n",
       "      <td>0.56566</td>\n",
       "      <td>0.67528</td>\n",
       "      <td>0.34960</td>\n",
       "      <td>0.25721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66098</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.54519</td>\n",
       "      <td>0.42473</td>\n",
       "      <td>0.63472</td>\n",
       "      <td>0.39003</td>\n",
       "      <td>0.37485</td>\n",
       "      <td>0.43810</td>\n",
       "      <td>0.59557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.41658</td>\n",
       "      <td>0.63417</td>\n",
       "      <td>0.50189</td>\n",
       "      <td>0.40883</td>\n",
       "      <td>0.58705</td>\n",
       "      <td>0.63785</td>\n",
       "      <td>0.56225</td>\n",
       "      <td>0.55989</td>\n",
       "      <td>0.58642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88227</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.44307</td>\n",
       "      <td>0.74076</td>\n",
       "      <td>0.52210</td>\n",
       "      <td>0.56543</td>\n",
       "      <td>0.51125</td>\n",
       "      <td>0.66457</td>\n",
       "      <td>0.42263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45851</td>\n",
       "      <td>0.58805</td>\n",
       "      <td>0.49860</td>\n",
       "      <td>0.48023</td>\n",
       "      <td>0.52606</td>\n",
       "      <td>0.53253</td>\n",
       "      <td>0.38361</td>\n",
       "      <td>0.43829</td>\n",
       "      <td>0.25014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   era data_type  feature1  feature2  feature3  feature4  feature5  \\\n",
       "0  135682  era1     train   0.53352   0.64336   0.46577   0.53001   0.55734   \n",
       "1  110546  era1     train   0.54196   0.81576   0.46632   0.62320   0.52427   \n",
       "2   76047  era1     train   0.49158   0.69131   0.57816   0.54010   0.43064   \n",
       "3   66098  era1     train   0.54519   0.42473   0.63472   0.39003   0.37485   \n",
       "4   88227  era1     train   0.44307   0.74076   0.52210   0.56543   0.51125   \n",
       "\n",
       "   feature6  feature7   ...    feature13  feature14  feature15  feature16  \\\n",
       "0   0.45773   0.41169   ...      0.51224    0.50484    0.41929    0.50954   \n",
       "1   0.64378   0.55662   ...      0.52643    0.63809    0.67121    0.49421   \n",
       "2   0.49986   0.61902   ...      0.43310    0.72286    0.76257    0.36600   \n",
       "3   0.43810   0.59557   ...      0.41658    0.63417    0.50189    0.40883   \n",
       "4   0.66457   0.42263   ...      0.45851    0.58805    0.49860    0.48023   \n",
       "\n",
       "   feature17  feature18  feature19  feature20  feature21  target  \n",
       "0    0.47383    0.48797    0.38373    0.46233    0.33341       0  \n",
       "1    0.45291    0.46932    0.54445    0.30997    0.19023       0  \n",
       "2    0.55330    0.56566    0.67528    0.34960    0.25721       1  \n",
       "3    0.58705    0.63785    0.56225    0.55989    0.58642       0  \n",
       "4    0.52606    0.53253    0.38361    0.43829    0.25014       0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data params\n",
    "TARGET_VAR= 'target'\n",
    "TOURNAMENT_DATA_CSV = 'numerai_tournament_data.csv'\n",
    "TRAINING_DATA_CSV = 'numerai_training_data.csv'\n",
    "BASE_FOLDER = 'numerai/'\n",
    "\n",
    "df_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Train / Validation / Test Split\n",
    "- Numerai provides a data set that is allready split into train, validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "import spectrum\n",
    "\n",
    "def featureFFT(y):\n",
    "    fs = 400\n",
    "    n = len(y)\n",
    "    dt = 1 / float(fs)  # Get time resolution\n",
    "\n",
    "    fft_output = np.fft.rfft(y)  # Perform real fft\n",
    "    rfreqs = np.fft.rfftfreq(n, dt)  # Calculatle frequency bins\n",
    "    fft_mag = np.abs(fft_output)  # Take only the magnitude of the spectrum\n",
    "    fft_mag = fft_mag * 2 / n\n",
    "\n",
    "    return fft_mag\n",
    "\n",
    "def featureAR(ch):\n",
    "    ar_coeffs, dnr, reflection_coeffs = spectrum.aryule(ch, order=8)\n",
    "    return np.abs(ar_coeffs)\n",
    "\n",
    "\n",
    "def featureEnt( row, base=2):\n",
    "    x_ent= -((np.log(row) / np.log(base)) * row).sum(axis=0)\n",
    "    return x_ent\n",
    "\n",
    "\n",
    "def featureShannonEnt(row):\n",
    "    row=row.div(row.sum())\n",
    "    # print (row)\n",
    "    return -sum([p * math.log(p) for p in row if p != 0])\n",
    "\n",
    "\n",
    "def enrichFeatures( row):\n",
    "    # print (len(row))\n",
    "    x_fft = featureFFT(row)\n",
    "    x_ent = featureShannonEnt(row)\n",
    "    x_ar =  featureAR(row)\n",
    "    s = pd.Series({'x_ent': x_ent, 'ar1': x_ar[0], 'ar2': x_ar[1], 'ar3': x_ar[2], 'ar4': x_ar[3], 'ar5': x_ar[4],\n",
    "                   'ar6': x_ar[5], 'ar7': x_ar[6], 'ar8': x_ar[7],\n",
    "                   'x_fft1': x_fft[0], 'x_fft2': x_fft[1], 'x_fft3': x_fft[2], 'x_fft4': x_fft[3],\n",
    "                   'x_fft5': x_fft[4], 'x_fft6': x_fft[5], 'x_fft7': x_fft[6], 'x_fft8': x_fft[7],\n",
    "                   'x_fft9': x_fft[8], 'x_fft10': x_fft[9],'x_fft11': x_fft[10]})\n",
    "    # print (s)\n",
    "    return s\n",
    "\n",
    "def genBasicFeatures(inDF):\n",
    "    print('Generating basic features ...')\n",
    "    df_copy=inDF.copy(deep=True)\n",
    "    magicNumber=21\n",
    "    feature_cols = list(inDF.columns)\n",
    "\n",
    "#     inDF['x_mean'] = np.mean(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "#     inDF['x_median'] = np.median(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "#     inDF['x_std'] = np.std(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "#     inDF['x_skew'] = scipy.stats.skew(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "#     inDF['x_kurt'] = scipy.stats.kurtosis(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "#     inDF['x_var'] = np.var(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "#     inDF['x_max'] = np.max(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "#     inDF['x_min'] = np.min(df_copy.ix[:, 0:magicNumber], axis=1)\n",
    "    # http://stackoverflow.com/questions/16236684/apply-pandas-function-to-column-to-create-multiple-new-columns\n",
    "    inDF=inDF.merge(df_copy.ix[:, 0:magicNumber].apply(lambda row: enrichFeatures(row), axis=1),\n",
    "                    left_index=True, right_index=True)\n",
    "    return inDF\n",
    "\n",
    "# Train, Validation, Test Split\n",
    "def loadDataSplit():\n",
    "    df_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)\n",
    "    # TOURNAMENT_DATA_CSV has both validation and test data provided by NumerAI\n",
    "    df_test_valid = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)\n",
    "\n",
    "    answers_1_SINGLE = df_train[TARGET_VAR]\n",
    "    df_train.drop(TARGET_VAR, axis=1,inplace=True)\n",
    "    df_train.drop('id', axis=1,inplace=True)\n",
    "    df_train.drop('era', axis=1,inplace=True)\n",
    "    df_train.drop('data_type', axis=1,inplace=True)    \n",
    "\n",
    "#     df_train=genBasicFeatures(df_train)\n",
    "    \n",
    "    df_train.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=False,  index = False)    \n",
    "    df_train= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=None, dtype=np.float32)    \n",
    "    df_train = pd.concat([df_train, answers_1_SINGLE], axis=1)\n",
    "    feature_cols = list(df_train.columns[:-1])\n",
    "    target_col = df_train.columns[-1]\n",
    "    trainX, trainY = df_train[feature_cols], df_train[target_col]\n",
    "    \n",
    "        \n",
    "    df_validation_set=df_test_valid.loc[df_test_valid['data_type'] == 'validation'] \n",
    "    df_validation_set=df_validation_set.copy(deep=True)\n",
    "    answers_1_SINGLE_validation = df_validation_set[TARGET_VAR]\n",
    "    df_validation_set.drop(TARGET_VAR, axis=1,inplace=True)    \n",
    "    df_validation_set.drop('id', axis=1,inplace=True)\n",
    "    df_validation_set.drop('era', axis=1,inplace=True)\n",
    "    df_validation_set.drop('data_type', axis=1,inplace=True)\n",
    "    \n",
    "#     df_validation_set=genBasicFeatures(df_validation_set)\n",
    "        \n",
    "    df_validation_set.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=False,  index = False)    \n",
    "    df_validation_set= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=None, dtype=np.float32)    \n",
    "    df_validation_set = pd.concat([df_validation_set, answers_1_SINGLE_validation], axis=1)\n",
    "    feature_cols = list(df_validation_set.columns[:-1])\n",
    "\n",
    "    target_col = df_validation_set.columns[-1]\n",
    "    valX, valY = df_validation_set[feature_cols], df_validation_set[target_col]\n",
    "                                \n",
    "    df_test_set = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)\n",
    "    df_test_set=df_test_set.copy(deep=True)\n",
    "    df_test_set.drop(TARGET_VAR, axis=1,inplace=True)\n",
    "    tid_1_SINGLE = df_test_set['id']\n",
    "    df_test_set.drop('id', axis=1,inplace=True)\n",
    "    df_test_set.drop('era', axis=1,inplace=True)\n",
    "    df_test_set.drop('data_type', axis=1,inplace=True)   \n",
    "    \n",
    "#     df_test_set=genBasicFeatures(df_test_set) \n",
    "    \n",
    "    feature_cols = list(df_test_set.columns) # must be run here, we dont want the ID    \n",
    "    df_test_set = pd.concat([tid_1_SINGLE, df_test_set], axis=1)            \n",
    "    testX = df_test_set[feature_cols].values\n",
    "        \n",
    "    return trainX, trainY, valX, valY, testX, df_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108405, 21)\n",
      "(108405,)\n",
      "(16686, 21)\n",
      "(16686,)\n",
      "(45647, 21)\n",
      "(45647, 22)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, valX, valY, testX, df_test_set = loadDataSplit()\n",
    "\n",
    "print (trainX.shape)\n",
    "print (trainY.shape)\n",
    "print (valX.shape)\n",
    "print (valY.shape)\n",
    "print (testX.shape)\n",
    "print (df_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.53352</td>\n",
       "      <td>0.64336</td>\n",
       "      <td>0.46577</td>\n",
       "      <td>0.53001</td>\n",
       "      <td>0.55734</td>\n",
       "      <td>0.45773</td>\n",
       "      <td>0.41169</td>\n",
       "      <td>0.52070</td>\n",
       "      <td>0.36351</td>\n",
       "      <td>0.72262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59757</td>\n",
       "      <td>0.51224</td>\n",
       "      <td>0.50484</td>\n",
       "      <td>0.41929</td>\n",
       "      <td>0.50954</td>\n",
       "      <td>0.47383</td>\n",
       "      <td>0.48797</td>\n",
       "      <td>0.38373</td>\n",
       "      <td>0.46233</td>\n",
       "      <td>0.33341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.54196</td>\n",
       "      <td>0.81576</td>\n",
       "      <td>0.46632</td>\n",
       "      <td>0.62320</td>\n",
       "      <td>0.52427</td>\n",
       "      <td>0.64378</td>\n",
       "      <td>0.55662</td>\n",
       "      <td>0.43845</td>\n",
       "      <td>0.12690</td>\n",
       "      <td>0.72814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.65167</td>\n",
       "      <td>0.52643</td>\n",
       "      <td>0.63809</td>\n",
       "      <td>0.67121</td>\n",
       "      <td>0.49421</td>\n",
       "      <td>0.45291</td>\n",
       "      <td>0.46932</td>\n",
       "      <td>0.54445</td>\n",
       "      <td>0.30997</td>\n",
       "      <td>0.19023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.49158</td>\n",
       "      <td>0.69131</td>\n",
       "      <td>0.57816</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.43064</td>\n",
       "      <td>0.49986</td>\n",
       "      <td>0.61902</td>\n",
       "      <td>0.44719</td>\n",
       "      <td>0.12617</td>\n",
       "      <td>0.64512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.58387</td>\n",
       "      <td>0.43310</td>\n",
       "      <td>0.72286</td>\n",
       "      <td>0.76257</td>\n",
       "      <td>0.36600</td>\n",
       "      <td>0.55330</td>\n",
       "      <td>0.56566</td>\n",
       "      <td>0.67528</td>\n",
       "      <td>0.34960</td>\n",
       "      <td>0.25721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.54519</td>\n",
       "      <td>0.42473</td>\n",
       "      <td>0.63472</td>\n",
       "      <td>0.39003</td>\n",
       "      <td>0.37485</td>\n",
       "      <td>0.43810</td>\n",
       "      <td>0.59557</td>\n",
       "      <td>0.43393</td>\n",
       "      <td>0.44115</td>\n",
       "      <td>0.41381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37095</td>\n",
       "      <td>0.41658</td>\n",
       "      <td>0.63417</td>\n",
       "      <td>0.50189</td>\n",
       "      <td>0.40883</td>\n",
       "      <td>0.58705</td>\n",
       "      <td>0.63785</td>\n",
       "      <td>0.56225</td>\n",
       "      <td>0.55989</td>\n",
       "      <td>0.58642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.44307</td>\n",
       "      <td>0.74076</td>\n",
       "      <td>0.52210</td>\n",
       "      <td>0.56543</td>\n",
       "      <td>0.51125</td>\n",
       "      <td>0.66457</td>\n",
       "      <td>0.42263</td>\n",
       "      <td>0.55584</td>\n",
       "      <td>0.30143</td>\n",
       "      <td>0.80876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.57756</td>\n",
       "      <td>0.45851</td>\n",
       "      <td>0.58805</td>\n",
       "      <td>0.49860</td>\n",
       "      <td>0.48023</td>\n",
       "      <td>0.52606</td>\n",
       "      <td>0.53253</td>\n",
       "      <td>0.38361</td>\n",
       "      <td>0.43829</td>\n",
       "      <td>0.25014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3        4        5        6        7   \\\n",
       "0  0.53352  0.64336  0.46577  0.53001  0.55734  0.45773  0.41169  0.52070   \n",
       "1  0.54196  0.81576  0.46632  0.62320  0.52427  0.64378  0.55662  0.43845   \n",
       "2  0.49158  0.69131  0.57816  0.54010  0.43064  0.49986  0.61902  0.44719   \n",
       "3  0.54519  0.42473  0.63472  0.39003  0.37485  0.43810  0.59557  0.43393   \n",
       "4  0.44307  0.74076  0.52210  0.56543  0.51125  0.66457  0.42263  0.55584   \n",
       "\n",
       "        8        9    ...          11       12       13       14       15  \\\n",
       "0  0.36351  0.72262   ...     0.59757  0.51224  0.50484  0.41929  0.50954   \n",
       "1  0.12690  0.72814   ...     0.65167  0.52643  0.63809  0.67121  0.49421   \n",
       "2  0.12617  0.64512   ...     0.58387  0.43310  0.72286  0.76257  0.36600   \n",
       "3  0.44115  0.41381   ...     0.37095  0.41658  0.63417  0.50189  0.40883   \n",
       "4  0.30143  0.80876   ...     0.57756  0.45851  0.58805  0.49860  0.48023   \n",
       "\n",
       "        16       17       18       19       20  \n",
       "0  0.47383  0.48797  0.38373  0.46233  0.33341  \n",
       "1  0.45291  0.46932  0.54445  0.30997  0.19023  \n",
       "2  0.55330  0.56566  0.67528  0.34960  0.25721  \n",
       "3  0.58705  0.63785  0.56225  0.55989  0.58642  \n",
       "4  0.52606  0.53253  0.38361  0.43829  0.25014  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.head(5) # with new features added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np.values, dtype=np.float32)        \n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")\n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "    \n",
    "    print(type(X_tensor.data)) # should be 'torch.cuda.FloatTensor'            \n",
    "    print((X_tensor.data.shape)) # torch.Size([108405, 29])\n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n",
    "\n",
    "    print(type(Y_tensor.data)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))    \n",
    "    return Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the CPU\n",
      "INFO:__main__:Net2 (\n",
      "  (l1): Sequential (\n",
      "    (0): Linear (21 -> 168)\n",
      "    (1): Dropout (p = 0.15)\n",
      "    (2): LeakyReLU (0.1)\n",
      "    (3): BatchNorm1d(168, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (c1): Sequential (\n",
      "    (0): Conv1d(21, 168, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): Dropout (p = 0.25)\n",
      "    (2): LeakyReLU (0.1)\n",
      "    (3): BatchNorm1d(168, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (out): Sequential (\n",
      "    (0): Linear (1344 -> 1)\n",
      "  )\n",
      "  (sig): Sigmoid ()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108405, 21)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "torch.Size([108405, 21])\n",
      "(b.size():torch.Size([108405, 1])\n"
     ]
    }
   ],
   "source": [
    "# References:\n",
    "# https://github.com/vinhkhuc/PyTorch-Mini-Tutorials/blob/master/5_convolutional_net.py\n",
    "# https://gist.github.com/spro/c87cc706625b8a54e604fb1024106556\n",
    "\n",
    "# use_cuda=False\n",
    "X_tensor_train= XnumpyToTensor(trainX) # default order is NBC for a 3d tensor, but we have a 2d tensor\n",
    "X_shape=X_tensor_train.data.size()\n",
    "\n",
    "# Dimensions\n",
    "# Number of features for the input layer\n",
    "N_FEATURES=trainX.shape[1]\n",
    "# Number of rows\n",
    "NUM_ROWS_TRAINNING=trainX.shape[0]\n",
    "# this number has no meaning except for being divisable by 2\n",
    "N_MULT_FACTOR=8 # min should be 4\n",
    "# Size of first linear layer\n",
    "N_HIDDEN=N_FEATURES * N_MULT_FACTOR\n",
    "# CNN kernel size\n",
    "N_CNN_KERNEL=3\n",
    "MAX_POOL_KERNEL=4\n",
    "\n",
    "DEBUG_ON=False\n",
    "\n",
    "def debug(x):\n",
    "    if DEBUG_ON:\n",
    "        print ('(x.size():' + str (x.size()))\n",
    "    \n",
    "class Net2(nn.Module):    \n",
    "    def __init__(self, n_feature, n_hidden, n_output, n_cnn_kernel, n_mult_factor=N_MULT_FACTOR):\n",
    "        super(Net2, self).__init__()\n",
    "        self.n_feature=n_feature\n",
    "        self.n_hidden=n_hidden\n",
    "        self.n_output= n_output \n",
    "        self.n_cnn_kernel=n_cnn_kernel\n",
    "        self.n_mult_factor=n_mult_factor\n",
    "        self.n_l2_hidden=self.n_hidden * (self.n_mult_factor - self.n_cnn_kernel + 3)\n",
    "#         self.n_out_hidden=int (self.n_l2_hidden/2)\n",
    "                        \n",
    "        self.l1 = nn.Sequential(\n",
    "            torch.nn.Linear(self.n_feature, self.n_hidden),\n",
    "            torch.nn.Dropout(p=1 -.85),            \n",
    "            torch.nn.LeakyReLU (0.1),            \n",
    "            torch.nn.BatchNorm1d(self.n_hidden, eps=1e-05, momentum=0.1, affine=True)            \n",
    "        )                \n",
    "        self.c1= nn.Sequential(            \n",
    "            torch.nn.Conv1d(self.n_feature, self.n_hidden, \n",
    "                            kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(1,)),\n",
    "            torch.nn.Dropout(p=1 -.75),            \n",
    "            torch.nn.LeakyReLU (0.1),\n",
    "            torch.nn.BatchNorm1d(self.n_hidden, eps=1e-05, momentum=0.1, affine=True)        \n",
    "        )                        \n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(self.n_l2_hidden,\n",
    "                            self.n_output),  \n",
    "        )                \n",
    "        self.sig=nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        debug(x)\n",
    "        varSize=x.data.shape[0] # must be calculated here in forward() since its is a dynamic size        \n",
    "        x=self.l1(x)                \n",
    "        debug(x)\n",
    "        # for CNN        \n",
    "        x = x.view(varSize,self.n_feature,self.n_mult_factor)\n",
    "        debug(x)\n",
    "        x=self.c1(x)\n",
    "        debug(x)\n",
    "        # for Linear layer\n",
    "        x = x.view(varSize, self.n_hidden * (self.n_mult_factor -self.n_cnn_kernel + 3))\n",
    "        debug(x)\n",
    "#         x=self.l2(x)                    \n",
    "        x=self.out(x)   \n",
    "        debug(x)\n",
    "        x=self.sig(x)\n",
    "        return x\n",
    "    \n",
    "net = Net2(n_feature=N_FEATURES, n_hidden=N_HIDDEN, n_output=1, n_cnn_kernel=N_CNN_KERNEL)   # define the network    \n",
    "if use_cuda:\n",
    "    net=net.cuda() # very important !!!\n",
    "lgr.info(net)\n",
    "b = net(X_tensor_train)\n",
    "print ('(b.size():' + str (b.size())) # torch.Size([108405, 928])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:<torch.optim.adam.Adam object at 0x7f6754dd2110>\n",
      "INFO:__main__:BCELoss (\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR,weight_decay=5e-4) #  L2 regularization\n",
    "loss_func=torch.nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    net.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the CPU\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "INFO:__main__:Using the CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108405, 21)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "torch.Size([108405, 21])\n",
      "(108405, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "(108405, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'> <class 'torch.FloatTensor'>\n",
      "0 [ 0.70974463]\n",
      "ACC=0.0, LOG_LOSS=0.77632911927, ROC_AUC=0.496740048481 \n",
      "10 [ 0.69717395]\n",
      "ACC=0.0, LOG_LOSS=0.697419923693, ROC_AUC=0.508009778805 \n",
      "20 [ 0.69489557]\n",
      "ACC=0.0, LOG_LOSS=0.694790536928, ROC_AUC=0.500532412995 \n",
      "30 [ 0.69356298]\n",
      "ACC=0.0, LOG_LOSS=0.693470323289, ROC_AUC=0.509168138512 \n",
      "40 [ 0.69294286]\n",
      "ACC=0.0, LOG_LOSS=0.693103163234, ROC_AUC=0.509802476296 \n",
      "50 [ 0.69272161]\n",
      "ACC=0.0, LOG_LOSS=0.69260831799, ROC_AUC=0.517115349435 \n",
      "60 [ 0.69262892]\n",
      "ACC=0.0, LOG_LOSS=0.692349490455, ROC_AUC=0.520382015719 \n",
      "70 [ 0.692168]\n",
      "ACC=0.0, LOG_LOSS=0.692350919956, ROC_AUC=0.521017668476 \n",
      "80 [ 0.69255126]\n",
      "ACC=0.0, LOG_LOSS=0.692755391916, ROC_AUC=0.516892704063 \n",
      "90 [ 0.6922549]\n",
      "ACC=0.0, LOG_LOSS=0.692267138698, ROC_AUC=0.521785201886 \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()    \n",
    "epochs=100\n",
    "all_losses = []\n",
    "\n",
    "X_tensor_train= XnumpyToTensor(trainX)\n",
    "Y_tensor_train= YnumpyToTensor(trainY)\n",
    "\n",
    "print(type(X_tensor_train.data), type(Y_tensor_train.data)) # should be 'torch.cuda.FloatTensor'\n",
    "\n",
    "# From here onwards, we must only use PyTorch Tensors\n",
    "for step in range(epochs):\n",
    "    out = net(X_tensor_train)                 # input x and predict based on x\n",
    "    cost = loss_func(out, Y_tensor_train)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    cost.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "                           \n",
    "    if step % 10 == 0:        \n",
    "        loss = cost.data[0]\n",
    "        all_losses.append(loss)\n",
    "        print(step, cost.data.cpu().numpy())        \n",
    "        prediction = (net(X_tensor_train).data).float() # probabilities             \n",
    "        pred_y = prediction.cpu().numpy().squeeze()\n",
    "        target_y = Y_tensor_train.cpu().data.numpy()                        \n",
    "        tu = ((pred_y == target_y).mean(),log_loss(target_y, pred_y),roc_auc_score(target_y,pred_y ))\n",
    "        print ('ACC={}, LOG_LOSS={}, ROC_AUC={} '.format(*tu))        \n",
    "                \n",
    "end_time = time.time()\n",
    "print ('{} {:6.3f} seconds'.format('GPU:', end_time-start_time))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_losses)\n",
    "plt.show()\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y,pred_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('LOG_LOSS=' + str(log_loss(target_y, pred_y)))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Validation ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the CPU\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "INFO:__main__:Using the CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16686, 21)\n",
      "(16686,)\n",
      "(16686, 21)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "torch.Size([16686, 21])\n",
      "(16686, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "(16686, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'> <class 'torch.FloatTensor'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "acc=0.0 log_loss=0.692840326255 roc_auc=0.523071687249 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6754dd2d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.eval()\n",
    "# Validation data\n",
    "print (valX.shape)\n",
    "print (valY.shape)\n",
    "\n",
    "X_tensor_val= XnumpyToTensor(valX)\n",
    "Y_tensor_val= YnumpyToTensor(valY)\n",
    "\n",
    "\n",
    "print(type(X_tensor_val.data), type(Y_tensor_val.data)) # should be 'torch.cuda.FloatTensor'\n",
    "\n",
    "predicted_val = (net(X_tensor_val).data).float() # probabilities \n",
    "# predicted_val = (net(X_tensor_val).data > 0.5).float() # zero or one\n",
    "pred_y = predicted_val.cpu().numpy()\n",
    "target_y = Y_tensor_val.cpu().data.numpy()                \n",
    "\n",
    "print (type(pred_y))\n",
    "print (type(target_y))\n",
    "\n",
    "tu = (str ((pred_y == target_y).mean()),log_loss(target_y, pred_y),roc_auc_score(target_y,pred_y ))\n",
    "print ('\\n')\n",
    "print ('acc={} log_loss={} roc_auc={} '.format(*tu))\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y,pred_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('LOG_LOSS=' + str(log_loss(target_y, pred_y)))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# print (pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45647, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97040.0</td>\n",
       "      <td>0.510508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65399.0</td>\n",
       "      <td>0.496846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147258.0</td>\n",
       "      <td>0.502671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129573.0</td>\n",
       "      <td>0.512110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134978.0</td>\n",
       "      <td>0.517421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  probability\n",
       "0   97040.0     0.510508\n",
       "1   65399.0     0.496846\n",
       "2  147258.0     0.502671\n",
       "3  129573.0     0.512110\n",
       "4  134978.0     0.517421"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df_test_set.shape)\n",
    "columns = ['id', 'probability']\n",
    "df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n",
    "df_pred.id.astype(int)\n",
    "\n",
    "for index, row in df_test_set.iterrows():\n",
    "    rwo_no_id=row.drop('id')    \n",
    "#     print (rwo_no_id.values)    \n",
    "    x_data_np = np.array(rwo_no_id.values, dtype=np.float32)        \n",
    "    if use_cuda:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "                    \n",
    "    X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n",
    "    predicted_val = (net(X_tensor_test).data).float() # probabilities     \n",
    "    p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n",
    "    \n",
    "    df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n",
    "    \n",
    "df_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred/pred_0.692840326255_1504874705.3.csv\n"
     ]
    }
   ],
   "source": [
    "df_pred.id=df_pred.id.astype(int)\n",
    "\n",
    "def savePred(df_pred, loss):\n",
    "#     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n",
    "    csv_path = 'pred/pred_{}_{}.csv'.format(loss, (str(time.time())))\n",
    "    df_pred.to_csv(csv_path, columns=('id', 'probability'), index=None)\n",
    "    print (csv_path)\n",
    "    \n",
    "savePred (df_pred, log_loss(target_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "controls": "true",
   "history": "true",
   "mouseWheel": "true",
   "overview": "true",
   "progress": "true",
   "scroll": "true",
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
