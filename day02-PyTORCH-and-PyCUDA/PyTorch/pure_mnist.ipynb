{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:1.2.1\n",
      "__Python VERSION: 2.7.6 (default, Oct 26 2016, 20:30:19) \n",
      "[GCC 4.8.4]\n",
      "__pyTorch VERSION: 0.2.0_1\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: 6021\n",
      "__Number CUDA Devices: 0\n",
      "__Devices\n",
      "Filesystem                                                                1K-blocks       Used  Available Use% Mounted on\n",
      "none                                                                      516021104  223931312  271019452  46% /\n",
      "tmpfs                                                                     258004476          0  258004476   0% /dev\n",
      "tmpfs                                                                     258004476          0  258004476   0% /sys/fs/cgroup\n",
      "netstore2:/vol/dev_vol/users/skashan/dev/cto-gamma_2472/CTO/data-science 1288490240 1036460160  252030080  81% /root/sharedfolder\n",
      "netstore2:/vol/CTO_VOL/data                                              6222333888  110079104 6112254784   2% /root/data\n",
      "/dev/vda1                                                                 516021104  223931312  271019452  46% /etc/postgresql\n",
      "shm                                                                           65536       2780      62756   5% /dev/shm\n",
      "tmpfs                                                                     258004476          0  258004476   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "# !pip install pycuda\n",
    "%reset -f\n",
    "import numpy\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots\n",
    "import tensorflow as tf \n",
    "print(\"tensorflow:\" + tf.__version__)\n",
    "!set \"KERAS_BACKEND=tensorflow\"\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# !pip install http://download.pytorch.org/whl/cu75/torch-0.2.0.post1-cp27-cp27mu-manylinux1_x86_64.whl\n",
    "# !pip install torchvision \n",
    "# ! pip install cv2\n",
    "# import cv2\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "! df -k\n",
    "\n",
    "DATA_ROOT ='/root/data/mnist/'\n",
    "IMG_DATA_LABELS = DATA_ROOT + '/train.csv'\n",
    "\n",
    "# classes = ('0', '1', '2', '3', '4','5', '6', '7', '8', '9', '10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "# Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import time \n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "\n",
    "class GenericImageDataset(Dataset):    \n",
    "    def __init__(self, csv_path, transform=None):                \n",
    "        t = time.time()        \n",
    "        lgr.info('CSV path {}'.format(csv_path))                \n",
    "        self.transform=transform\n",
    "        tmp_df= pd.read_csv(csv_path, dtype=np.float32)\n",
    "        \n",
    "        tmp_df.insert(0, 'IMG_ID', range(0, 0 + len(tmp_df)))        \n",
    "        tmp_df = tmp_df.reset_index(drop=True)\n",
    "        tmp_df.set_index('IMG_ID', inplace=True, drop=False)\n",
    "                \n",
    "        self.X_train = tmp_df[tmp_df.columns[0:786]]                \n",
    "        lgr.info('[*] Dataset loading time {}'.format(time.time() - t))\n",
    "        lgr.info('[*] Data size is {}'.format(len(self)))\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        row=self.X_train.loc[[index]] # find the row and return a data frame\n",
    "        img = row[row.columns[2:786]].values # get only the pixels                       \n",
    "#         print (\"img type:\" + str (type(img)))\n",
    "#         print ((\"img:\" + str (img)))    \n",
    "        label =row['label'].values[0] # extract mnist label \n",
    "#         print (\"Label:\" + str (type(label)))\n",
    "#         print ((\"label:\" + str (label)))        \n",
    "        if self.transform is not None:\n",
    "            # reshape input image into [batch_size by 784]\n",
    "#             img = Image.fromarray(numpy.uint8(img))\n",
    "#             img = Image.fromarray(img, mode='L')\n",
    "            img = Image.frombytes('L',(28,28),img)\n",
    "            img = self.transform(img)            \n",
    "                    \n",
    "#         lgr.info (\"__getitem__:\" + str(index) + \" Label:\" + str(label))\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        l=len(self.X_train.index)\n",
    "        return (l)       \n",
    "\n",
    "    @staticmethod        \n",
    "    def imshow(img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "    @staticmethod    \n",
    "    def flaotTensorToImage(img, mean=0, std=1):\n",
    "        \"\"\"convert a tensor to an image\"\"\"\n",
    "#         img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "        img = np.transpose(img.numpy(), (2, 0, 1))\n",
    "        img = (img*std+ mean)*255\n",
    "        img = img.astype(np.uint8)    \n",
    "        return img    \n",
    "    \n",
    "    @staticmethod\n",
    "    def toTensor(img):\n",
    "        \"\"\"convert a numpy array of shape HWC to CHW tensor\"\"\"\n",
    "        img = img.transpose((2, 0, 1)).astype(np.float32)\n",
    "        tensor = torch.from_numpy(img).float()\n",
    "        return tensor/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformations=transforms.Compose([ transforms.ToTensor() # first, convert image to PyTorch tensor\n",
    "#                             transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:CSV path /root/data/mnist//train.csv\n",
      "INFO:__main__:[*] Dataset loading time 5.07674813271\n",
      "INFO:__main__:[*] Data size is 42000\n"
     ]
    }
   ],
   "source": [
    "dset_train = GenericImageDataset(IMG_DATA_LABELS, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validationRatio=0.11    \n",
    "batch_size=4\n",
    "\n",
    "class FullTrainingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainingDataset, self).__init__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.full_ds[i+self.offset]\n",
    "    \n",
    "def trainTestSplit(dataset, val_share=validationRatio):\n",
    "    val_offset = int(len(dataset)*(1-val_share))\n",
    "    return FullTrainingDataset(dataset, 0, val_offset), FullTrainingDataset(dataset, val_offset, len(dataset)-val_offset)\n",
    "\n",
    " \n",
    "train_ds, val_ds = trainTestSplit(dset_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-367-a745ff26b1f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# show images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# print labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%5s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-367-a745ff26b1f8>\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m     \u001b[0;31m# unnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3155\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3157\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3158\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5122\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5124\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5125\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/image.pyc\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    598\u001b[0m         if (self._A.ndim not in (2, 3) or\n\u001b[1;32m    599\u001b[0m                 (self._A.ndim == 3 and self._A.shape[-1] not in (3, 4))):\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAFpCAYAAABu98hvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYg4wgJgDDPB/bPOaG9rG2MUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3cf85c650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision \n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (2, 1, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(val_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"Custom module for a simple convnet classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        # input is 28x28x1\n",
    "        # conv1(kernel=5, filters=10) 28x28x10 -> 24x24x10\n",
    "        # max_pool(kernel=2) 24x24x10 -> 12x12x10\n",
    "         \n",
    "        # Do not be afraid of F's - those are just functional wrappers for modules form nn package\n",
    "        # Please, see for yourself - http://pytorch.org/docs/_modules/torch/nn/functional.html\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "         \n",
    "        # conv2(kernel=5, filters=20) 12x12x20 -> 8x8x20\n",
    "        # max_pool(kernel=2) 8x8x20 -> 4x4x20\n",
    "        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))\n",
    "         \n",
    "        # flatten 4x4x20 = 320\n",
    "        x = x.view(-1, 320)\n",
    "         \n",
    "        # 320 -> 50\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "         \n",
    "        # 50 -> 10\n",
    "        x = self.fc2(x)\n",
    "         \n",
    "        # transform to logits\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "clf = CNNClassifier()\n",
    "opt = optim.SGD(clf.parameters(), lr=0.01, momentum=0.5)\n",
    " \n",
    "loss_history = []\n",
    "acc_history = []\n",
    " \n",
    "def train(epoch):\n",
    "    clf.train() # set model in training mode (need this because of dropout)\n",
    "     \n",
    "    # dataset API gives us pythonic batching \n",
    "    for batch_id, (data, label) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        target = Variable(label)\n",
    "         \n",
    "        # forward pass, calculate loss and backprop!\n",
    "        opt.zero_grad()\n",
    "        preds = clf(data)\n",
    "        loss = F.nll_loss(preds, target)\n",
    "        loss.backward()\n",
    "        loss_history.append(loss.data[0])\n",
    "        opt.step()\n",
    "         \n",
    "        if batch_id % 100 == 0:\n",
    "            print(loss.data[0])\n",
    "            \n",
    "def test(epoch):\n",
    "    clf.eval() # set model in inference mode (need this because of dropout)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "     \n",
    "    for data, target in test_loader:\n",
    "        data = Variable(data, volatile=True) \n",
    "        target = Variable(target)\n",
    "         \n",
    "        output = clf(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    " \n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    acc_history.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    \n",
    "for epoch in range(0, 3):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
