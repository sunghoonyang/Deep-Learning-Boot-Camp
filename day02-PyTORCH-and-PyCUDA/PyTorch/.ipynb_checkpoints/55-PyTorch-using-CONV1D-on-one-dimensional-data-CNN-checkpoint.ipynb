{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Bootcamp November 2017, GPU Computing for Data Scientists\n",
    "\n",
    "<img src=\"../images/bcamp.png\" align=\"center\">\n",
    "\n",
    "### 55 PyTorch Convolutional Nerual Network (CNN) classifier for Numer.Ai Binary Classification problem using CONV1D (one dimentional convolution). \n",
    "\n",
    "Web: https://www.meetup.com/Tel-Aviv-Deep-Learning-Bootcamp/events/241762893/\n",
    "\n",
    "Notebooks: <a href=\"https://github.com/QuantScientist/Data-Science-PyCUDA-GPU\"> On GitHub</a>\n",
    "\n",
    "*Shlomo Kashani*\n",
    "\n",
    "<img src=\"../images/pt.jpg\" width=\"35%\" align=\"center\">\n",
    "\n",
    "\n",
    "### Data\n",
    "- Download from https://numer.ai/leaderboard\n",
    "\n",
    "<img src=\"../images/Numerai.png\" width=\"35%\" align=\"center\">\n",
    "\n",
    "# Why are we doing this? \n",
    "\n",
    "## One dimetional CNN? Convolutional Nerual Network (CNN) using one dimentional convolution (CONV1D).\n",
    "\n",
    "- Indeed, most of the existing PyTorch examples are using Images, while here we have a CSV with 21 features. Using CONV1D *before or after a Lineer layer* requires the use of **reshaping**, and this is the **whole point of this tutorial**. \n",
    "\n",
    "- Thus, the CNN architecture is naive and by no means **optimized**. Hopefully, I will improve it over time and I am working on a second CNN based version of the same problem. \n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- This tutorial was written in order to demonstrate a **fully working** example of a PyTorch **CNN** on a real world use case, namely a Binary Classification problem. \n",
    "\n",
    "- If you are interested in the sk-learn version of this problem please refer to: https://github.com/QuantScientist/deep-ml-meetups/tree/master/hacking-kaggle/python/numer-ai \n",
    "\n",
    "- For the scientific foundation behind Binary Classification and Logistic Regression, refer to: https://github.com/QuantScientist/Deep-Learning-Boot-Camp/tree/master/Data-Science-Interviews-Book\n",
    "\n",
    "- Every step, from reading the CSV into numpy arrays, converting to GPU based tensors, training and validation, are meant to aid newcomers in their first steps in PyTorch. \n",
    "\n",
    "- Additionally, commonly used Kaggle metrics such as ROC_AUC and LOG_LOSS are logged and plotted both for the training set as well as for the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:1.2.1\n",
      "__Python VERSION: 2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "__pyTorch VERSION: 0.2.0+42448cf\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: None\n",
      "__Number CUDA Devices: 0\n",
      "__Devices\n",
      "OS:  linux2\n",
      "Python:  2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "PyTorch:  0.2.0+42448cf\n",
      "Numpy:  1.13.1\n",
      "2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "0.0\n",
      "svmem(total=67469099008, available=58739154944, percent=12.9, used=8082026496, free=53930958848, active=11053948928, inactive=1350250496, buffers=1102790656, cached=4353323008, shared=99651584)\n",
      "memory GB: 0.219120025635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:USE CUDA=False\n"
     ]
    }
   ],
   "source": [
    "# !pip install pycuda\n",
    "%reset -f\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "import logging\n",
    "import numpy\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots\n",
    "import tensorflow as tf \n",
    "print(\"tensorflow:\" + tf.__version__)\n",
    "!set \"KERAS_BACKEND=tensorflow\"\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu75/torch-0.2.0.post1-cp27-cp27mu-manylinux1_x86_64.whl\n",
    "# !pip install torchvision \n",
    "# ! pip install cv2\n",
    "# import cv2\n",
    "\n",
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()\n",
    "memReport()\n",
    "\n",
    "# %%timeit\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "use_cuda=False\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# fix seed\n",
    "seed=17*19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  View the Data\n",
    "- Numerai provides a data set that is allready split into train, validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72774</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.48937</td>\n",
       "      <td>0.56969</td>\n",
       "      <td>0.59150</td>\n",
       "      <td>0.46432</td>\n",
       "      <td>0.42291</td>\n",
       "      <td>0.49616</td>\n",
       "      <td>0.53542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.42195</td>\n",
       "      <td>0.62651</td>\n",
       "      <td>0.51604</td>\n",
       "      <td>0.42938</td>\n",
       "      <td>0.56744</td>\n",
       "      <td>0.60008</td>\n",
       "      <td>0.46966</td>\n",
       "      <td>0.50322</td>\n",
       "      <td>0.42803</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140123</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.57142</td>\n",
       "      <td>0.43408</td>\n",
       "      <td>0.58771</td>\n",
       "      <td>0.44570</td>\n",
       "      <td>0.41471</td>\n",
       "      <td>0.49137</td>\n",
       "      <td>0.52791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46301</td>\n",
       "      <td>0.55103</td>\n",
       "      <td>0.39053</td>\n",
       "      <td>0.48856</td>\n",
       "      <td>0.54305</td>\n",
       "      <td>0.59213</td>\n",
       "      <td>0.44935</td>\n",
       "      <td>0.56685</td>\n",
       "      <td>0.59645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46882</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.75694</td>\n",
       "      <td>0.59942</td>\n",
       "      <td>0.36154</td>\n",
       "      <td>0.65571</td>\n",
       "      <td>0.60520</td>\n",
       "      <td>0.45317</td>\n",
       "      <td>0.49847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68057</td>\n",
       "      <td>0.43763</td>\n",
       "      <td>0.46322</td>\n",
       "      <td>0.63211</td>\n",
       "      <td>0.32947</td>\n",
       "      <td>0.35632</td>\n",
       "      <td>0.56316</td>\n",
       "      <td>0.33888</td>\n",
       "      <td>0.40120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20833</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.46059</td>\n",
       "      <td>0.50856</td>\n",
       "      <td>0.64215</td>\n",
       "      <td>0.41382</td>\n",
       "      <td>0.39550</td>\n",
       "      <td>0.49282</td>\n",
       "      <td>0.54697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38108</td>\n",
       "      <td>0.65446</td>\n",
       "      <td>0.54926</td>\n",
       "      <td>0.36297</td>\n",
       "      <td>0.61482</td>\n",
       "      <td>0.64292</td>\n",
       "      <td>0.52910</td>\n",
       "      <td>0.53582</td>\n",
       "      <td>0.47027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5381</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.61195</td>\n",
       "      <td>0.66684</td>\n",
       "      <td>0.45877</td>\n",
       "      <td>0.56730</td>\n",
       "      <td>0.51889</td>\n",
       "      <td>0.41257</td>\n",
       "      <td>0.56030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.54803</td>\n",
       "      <td>0.59120</td>\n",
       "      <td>0.58160</td>\n",
       "      <td>0.51828</td>\n",
       "      <td>0.43870</td>\n",
       "      <td>0.47011</td>\n",
       "      <td>0.56007</td>\n",
       "      <td>0.36374</td>\n",
       "      <td>0.31552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   era data_type  feature1  feature2  feature3  feature4  feature5  \\\n",
       "0   72774  era1     train   0.48937   0.56969   0.59150   0.46432   0.42291   \n",
       "1  140123  era1     train   0.57142   0.43408   0.58771   0.44570   0.41471   \n",
       "2   46882  era1     train   0.75694   0.59942   0.36154   0.65571   0.60520   \n",
       "3   20833  era1     train   0.46059   0.50856   0.64215   0.41382   0.39550   \n",
       "4    5381  era1     train   0.61195   0.66684   0.45877   0.56730   0.51889   \n",
       "\n",
       "   feature6  feature7   ...    feature13  feature14  feature15  feature16  \\\n",
       "0   0.49616   0.53542   ...      0.42195    0.62651    0.51604    0.42938   \n",
       "1   0.49137   0.52791   ...      0.46301    0.55103    0.39053    0.48856   \n",
       "2   0.45317   0.49847   ...      0.68057    0.43763    0.46322    0.63211   \n",
       "3   0.49282   0.54697   ...      0.38108    0.65446    0.54926    0.36297   \n",
       "4   0.41257   0.56030   ...      0.54803    0.59120    0.58160    0.51828   \n",
       "\n",
       "   feature17  feature18  feature19  feature20  feature21  target  \n",
       "0    0.56744    0.60008    0.46966    0.50322    0.42803       1  \n",
       "1    0.54305    0.59213    0.44935    0.56685    0.59645       1  \n",
       "2    0.32947    0.35632    0.56316    0.33888    0.40120       0  \n",
       "3    0.61482    0.64292    0.52910    0.53582    0.47027       0  \n",
       "4    0.43870    0.47011    0.56007    0.36374    0.31552       1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data params\n",
    "TARGET_VAR= 'target'\n",
    "TOURNAMENT_DATA_CSV = 'numerai_tournament_data.csv'\n",
    "TRAINING_DATA_CSV = 'numerai_training_data.csv'\n",
    "BASE_FOLDER = 'numerai/'\n",
    "\n",
    "df_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Train / Validation / Test Split\n",
    "- Numerai provides a data set that is allready split into train, validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.48937</td>\n",
       "      <td>0.56969</td>\n",
       "      <td>0.59150</td>\n",
       "      <td>0.46432</td>\n",
       "      <td>0.42291</td>\n",
       "      <td>0.49616</td>\n",
       "      <td>0.53542</td>\n",
       "      <td>0.50577</td>\n",
       "      <td>0.31534</td>\n",
       "      <td>0.58043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46203</td>\n",
       "      <td>0.42195</td>\n",
       "      <td>0.62651</td>\n",
       "      <td>0.51604</td>\n",
       "      <td>0.42938</td>\n",
       "      <td>0.56744</td>\n",
       "      <td>0.60008</td>\n",
       "      <td>0.46966</td>\n",
       "      <td>0.50322</td>\n",
       "      <td>0.42803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.57142</td>\n",
       "      <td>0.43408</td>\n",
       "      <td>0.58771</td>\n",
       "      <td>0.44570</td>\n",
       "      <td>0.41471</td>\n",
       "      <td>0.49137</td>\n",
       "      <td>0.52791</td>\n",
       "      <td>0.42731</td>\n",
       "      <td>0.52087</td>\n",
       "      <td>0.40264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37833</td>\n",
       "      <td>0.46301</td>\n",
       "      <td>0.55103</td>\n",
       "      <td>0.39053</td>\n",
       "      <td>0.48856</td>\n",
       "      <td>0.54305</td>\n",
       "      <td>0.59213</td>\n",
       "      <td>0.44935</td>\n",
       "      <td>0.56685</td>\n",
       "      <td>0.59645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75694</td>\n",
       "      <td>0.59942</td>\n",
       "      <td>0.36154</td>\n",
       "      <td>0.65571</td>\n",
       "      <td>0.60520</td>\n",
       "      <td>0.45317</td>\n",
       "      <td>0.49847</td>\n",
       "      <td>0.25173</td>\n",
       "      <td>0.52677</td>\n",
       "      <td>0.47626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.64442</td>\n",
       "      <td>0.68057</td>\n",
       "      <td>0.43763</td>\n",
       "      <td>0.46322</td>\n",
       "      <td>0.63211</td>\n",
       "      <td>0.32947</td>\n",
       "      <td>0.35632</td>\n",
       "      <td>0.56316</td>\n",
       "      <td>0.33888</td>\n",
       "      <td>0.40120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.46059</td>\n",
       "      <td>0.50856</td>\n",
       "      <td>0.64215</td>\n",
       "      <td>0.41382</td>\n",
       "      <td>0.39550</td>\n",
       "      <td>0.49282</td>\n",
       "      <td>0.54697</td>\n",
       "      <td>0.51875</td>\n",
       "      <td>0.30797</td>\n",
       "      <td>0.55321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.42176</td>\n",
       "      <td>0.38108</td>\n",
       "      <td>0.65446</td>\n",
       "      <td>0.54926</td>\n",
       "      <td>0.36297</td>\n",
       "      <td>0.61482</td>\n",
       "      <td>0.64292</td>\n",
       "      <td>0.52910</td>\n",
       "      <td>0.53582</td>\n",
       "      <td>0.47027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.61195</td>\n",
       "      <td>0.66684</td>\n",
       "      <td>0.45877</td>\n",
       "      <td>0.56730</td>\n",
       "      <td>0.51889</td>\n",
       "      <td>0.41257</td>\n",
       "      <td>0.56030</td>\n",
       "      <td>0.39115</td>\n",
       "      <td>0.32634</td>\n",
       "      <td>0.62694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61730</td>\n",
       "      <td>0.54803</td>\n",
       "      <td>0.59120</td>\n",
       "      <td>0.58160</td>\n",
       "      <td>0.51828</td>\n",
       "      <td>0.43870</td>\n",
       "      <td>0.47011</td>\n",
       "      <td>0.56007</td>\n",
       "      <td>0.36374</td>\n",
       "      <td>0.31552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3        4        5        6        7   \\\n",
       "0  0.48937  0.56969  0.59150  0.46432  0.42291  0.49616  0.53542  0.50577   \n",
       "1  0.57142  0.43408  0.58771  0.44570  0.41471  0.49137  0.52791  0.42731   \n",
       "2  0.75694  0.59942  0.36154  0.65571  0.60520  0.45317  0.49847  0.25173   \n",
       "3  0.46059  0.50856  0.64215  0.41382  0.39550  0.49282  0.54697  0.51875   \n",
       "4  0.61195  0.66684  0.45877  0.56730  0.51889  0.41257  0.56030  0.39115   \n",
       "\n",
       "        8        9    ...          11       12       13       14       15  \\\n",
       "0  0.31534  0.58043   ...     0.46203  0.42195  0.62651  0.51604  0.42938   \n",
       "1  0.52087  0.40264   ...     0.37833  0.46301  0.55103  0.39053  0.48856   \n",
       "2  0.52677  0.47626   ...     0.64442  0.68057  0.43763  0.46322  0.63211   \n",
       "3  0.30797  0.55321   ...     0.42176  0.38108  0.65446  0.54926  0.36297   \n",
       "4  0.32634  0.62694   ...     0.61730  0.54803  0.59120  0.58160  0.51828   \n",
       "\n",
       "        16       17       18       19       20  \n",
       "0  0.56744  0.60008  0.46966  0.50322  0.42803  \n",
       "1  0.54305  0.59213  0.44935  0.56685  0.59645  \n",
       "2  0.32947  0.35632  0.56316  0.33888  0.40120  \n",
       "3  0.61482  0.64292  0.52910  0.53582  0.47027  \n",
       "4  0.43870  0.47011  0.56007  0.36374  0.31552  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def addPolyFeatures(inDF, deg=2):\n",
    "    print('Generating poly features ...')\n",
    "    df_copy=inDF.copy(deep=True)\n",
    "    poly=PolynomialFeatures(degree=deg)\n",
    "    p_testX = poly.fit(df_copy)\n",
    "    # AttributeError: 'PolynomialFeatures' object has no attribute 'get_feature_names'\n",
    "    target_feature_names = ['x'.join(['{}^{}'.format(pair[0],pair[1]) for pair in tuple if pair[1]!=0]) for tuple in [zip(df_copy.columns,p) for p in poly.powers_]]\n",
    "    df_copy = pd.DataFrame(p_testX.transform(df_copy),columns=target_feature_names)\n",
    "        \n",
    "    return df_copy\n",
    "\n",
    "# Train, Validation, Test Split\n",
    "def loadDataSplit(poly=False):\n",
    "    df_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)\n",
    "    # TOURNAMENT_DATA_CSV has both validation and test data provided by NumerAI\n",
    "    df_test_valid = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)\n",
    "\n",
    "    answers_1_SINGLE = df_train[TARGET_VAR]\n",
    "    df_train.drop(TARGET_VAR, axis=1,inplace=True)\n",
    "    df_train.drop('id', axis=1,inplace=True)\n",
    "    df_train.drop('era', axis=1,inplace=True)\n",
    "    df_train.drop('data_type', axis=1,inplace=True)    \n",
    "    \n",
    "    # Add polynomial features\n",
    "    if poly:\n",
    "        df_train = addPolyFeatures(df_train)\n",
    "\n",
    "    df_train.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=False,  index = False)    \n",
    "    df_train= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + 'clean.csv', header=None, dtype=np.float32)    \n",
    "    df_train = pd.concat([df_train, answers_1_SINGLE], axis=1)\n",
    "    feature_cols = list(df_train.columns[:-1])\n",
    "#     print (feature_cols)\n",
    "    target_col = df_train.columns[-1]\n",
    "    trainX, trainY = df_train[feature_cols], df_train[target_col]\n",
    "    \n",
    "    \n",
    "    # TOURNAMENT_DATA_CSV has both validation and test data provided by NumerAI\n",
    "    # Validation set\n",
    "    df_validation_set=df_test_valid.loc[df_test_valid['data_type'] == 'validation'] \n",
    "    df_validation_set=df_validation_set.copy(deep=True)\n",
    "    answers_1_SINGLE_validation = df_validation_set[TARGET_VAR]\n",
    "    df_validation_set.drop(TARGET_VAR, axis=1,inplace=True)    \n",
    "    df_validation_set.drop('id', axis=1,inplace=True)\n",
    "    df_validation_set.drop('era', axis=1,inplace=True)\n",
    "    df_validation_set.drop('data_type', axis=1,inplace=True)\n",
    "    \n",
    "   # Add polynomial features    \n",
    "    if poly:\n",
    "        df_validation_set = addPolyFeatures(df_validation_set)\n",
    "    \n",
    "    df_validation_set.to_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=False,  index = False)    \n",
    "    df_validation_set= pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV + '-validation-clean.csv', header=None, dtype=np.float32)    \n",
    "    df_validation_set = pd.concat([df_validation_set, answers_1_SINGLE_validation], axis=1)\n",
    "    feature_cols = list(df_validation_set.columns[:-1])\n",
    "\n",
    "    target_col = df_validation_set.columns[-1]\n",
    "    valX, valY = df_validation_set[feature_cols], df_validation_set[target_col]\n",
    "                            \n",
    "    # Test set for submission (not labeled)    \n",
    "    df_test_set = pd.read_csv(BASE_FOLDER + TOURNAMENT_DATA_CSV)\n",
    "#     df_test_set=df_test_set.loc[df_test_valid['data_type'] == 'live'] \n",
    "    df_test_set=df_test_set.copy(deep=True)\n",
    "    df_test_set.drop(TARGET_VAR, axis=1,inplace=True)\n",
    "    tid_1_SINGLE = df_test_set['id']\n",
    "    df_test_set.drop('id', axis=1,inplace=True)\n",
    "    df_test_set.drop('era', axis=1,inplace=True)\n",
    "    df_test_set.drop('data_type', axis=1,inplace=True)   \n",
    "    \n",
    "    # Add polynomial features \n",
    "    if poly:\n",
    "        df_test_set = addPolyFeatures(df_test_set)\n",
    "       \n",
    "    feature_cols = list(df_test_set.columns) # must be run here, we dont want the ID    \n",
    "\n",
    "    df_test_set = pd.concat([tid_1_SINGLE, df_test_set], axis=1)            \n",
    "    testX = df_test_set[feature_cols].values\n",
    "        \n",
    "    return trainX, trainY, valX, valY, testX, df_test_set\n",
    "\n",
    "\n",
    "trainX, trainY, valX, valY, testX, df_test_set = loadDataSplit()\n",
    "trainX.head(5) # with new features added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  From Numpy to PyTorch GPU tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np.values, dtype=np.float32)        \n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")\n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "    \n",
    "    print(type(X_tensor.data)) # should be 'torch.cuda.FloatTensor'            \n",
    "    print((X_tensor.data.shape)) # torch.Size([108405, 29])\n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n",
    "\n",
    "    print(type(Y_tensor.data)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))    \n",
    "    return Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How conv1d works? \n",
    "\n",
    "- conv = torch.nn.Conv1d(in_channels=5, out_channels=10, kernel_size=2)\n",
    "\n",
    "- Note that view is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use .contiguous() after transpose \n",
    "\n",
    "- Note that Conv1d expects (batch, in_channels, in_length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we learn from the AlexNet architecture? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU (inplace)\n",
      "    (5): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU (inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU (inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU (inplace)\n",
      "    (12): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Linear (9216 -> 4096)\n",
      "    (2): ReLU (inplace)\n",
      "    (3): Dropout (p = 0.5)\n",
      "    (4): Linear (4096 -> 4096)\n",
      "    (5): ReLU (inplace)\n",
      "    (6): Linear (4096 -> 1000)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "alexnet = models.alexnet(pretrained = False) # set False so that it is not downloaded\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_input n_cnn_kernel n_padding n_input_rows n_hidden     after CNN                      stride\n",
    "# 21      1            1         108405       21*32=672    torch.Size([108405, 672, 3]    1\n",
    "# 21      1            2         108405       21*32=672    torch.Size([108405, 672, 5]    \n",
    "# 21      1            3         108405       21*32=672    torch.Size([108405, 672, 7]\n",
    "# 21      1            4         108405       21*32=672    torch.Size([108405, 672, 9]\n",
    "\n",
    "# 21      2            1         108405       21*32=672    torch.Size([108405, 672, 2]\n",
    "# 21      3            1         108405       21*32=672    torch.Size([108405, 672, 1]\n",
    "# 21      4            1         108405       21*32=672    ERROR * ERROR * ERROR \n",
    "# Given input size: (21 x 1 x 1). \n",
    "# Calculated output size: (672 x 1 x 0). Output size is too small \n",
    "\n",
    "# 21      2            2         108405       21*32=672    torch.Size([108405, 672, 4]\n",
    "# 21      3            2         108405       21*32=672    torch.Size([108405, 672, 3]\n",
    "# 21      4            2         108405       21*32=672    torch.Size([108405, 672, 2]\n",
    "# 21      5            2         108405       21*32=672    torch.Size([108405, 672, 1]\n",
    "# 21      6            2         108405       21*32=672    ERROR * ERROR * ERROR\n",
    "\n",
    "# 21      1            3         108405       21*32=672    torch.Size([108405, 672, 7]\n",
    "# 21      2            3         108405       21*32=672    torch.Size([108405, 672, 6]\n",
    "# 21      3            3         108405       21*32=672    torch.Size([108405, 672, 5]\n",
    "# 21      4            3         108405       21*32=672    torch.Size([108405, 672, 4]\n",
    "# 21      5            3         108405       21*32=672    torch.Size([108405, 672, 3]\n",
    "# 21      6            3         108405       21*32=672    torch.Size([108405, 672, 2]\n",
    "# 21      7            3         108405       21*32=672    torch.Size([108405, 672, 1]\n",
    "\n",
    "\n",
    "# 21      4            4         108405       21*32=672    torch.Size([108405, 672, 6]\n",
    "# 21      5            4         108405       21*32=672    torch.Size([108405, 672, 5]\n",
    "# 21      6            4         108405       21*32=672    torch.Size([108405, 672, 4]\n",
    "# 21      7            4         108405       21*32=672    torch.Size([108405, 672, 3]\n",
    "# 21      8            4         108405       21*32=672    torch.Size([108405, 672, 2]\n",
    "# 21      9            4         108405       21*32=672    torch.Size([108405, 672, 1]\n",
    "\n",
    "\n",
    "# 21      4            4         108405       21*32=672    torch.Size([108405, 672, 3]   2\n",
    "# 21      5            4         108405       21*32=672    torch.Size([108405, 672, 3]   2\n",
    "# 21      6            4         108405       21*32=672    torch.Size([108405, 672, 2]   2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the CPU\n",
      "INFO:__main__:CNNNumerAI (\n",
      "  (features): Sequential (\n",
      "    (0): Conv1d(21, 168, kernel_size=(5,), stride=(1,), padding=(3,))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(168, 504, kernel_size=(5,), stride=(1,), padding=(3,))\n",
      "    (4): ReLU ()\n",
      "    (5): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(504, 1008, kernel_size=(5,), stride=(1,), padding=(3,))\n",
      "    (7): ReLU ()\n",
      "    (8): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv1d(1008, 840, kernel_size=(5,), stride=(1,), padding=(3,))\n",
      "    (10): ReLU ()\n",
      "    (11): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv1d(840, 840, kernel_size=(5,), stride=(1,), padding=(3,))\n",
      "    (13): ReLU ()\n",
      "    (14): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Linear (168 -> 84)\n",
      "    (2): ReLU ()\n",
      "    (3): Dropout (p = 0.5)\n",
      "    (4): Linear (84 -> 84)\n",
      "    (5): ReLU ()\n",
      "    (6): Linear (84 -> 1)\n",
      "  )\n",
      "  (sig): Sigmoid ()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108405, 21)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "torch.Size([108405, 21])\n",
      "after view, (size():torch.Size([108405, 21, 1])\n"
     ]
    }
   ],
   "source": [
    "loss_func=torch.nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "# References:\n",
    "# https://github.com/vinhkhuc/PyTorch-Mini-Tutorials/blob/master/5_convolutional_net.py\n",
    "# https://gist.github.com/spro/c87cc706625b8a54e604fb1024106556\n",
    "\n",
    "X_tensor_train= XnumpyToTensor(trainX) # default order is NBC for a 3d tensor, but we have a 2d tensor\n",
    "X_shape=X_tensor_train.data.size()\n",
    "\n",
    "# 21      6            3         108405       21*32=672    torch.Size([108405, 672, 2] n_max_pool1d=1\n",
    "\n",
    "n_mult_factor=8\n",
    "n_input= trainX.shape[1]\n",
    "n_hidden= n_input * n_mult_factor\n",
    "n_output=1\n",
    "n_input_rows=trainX.shape[0]\n",
    "n_cnn_kernel=5\n",
    "n_padding=3\n",
    "\n",
    "n_max_pool1d=2\n",
    "\n",
    "DEBUG_ON=True\n",
    "def debug(msg, x):\n",
    "    if DEBUG_ON:\n",
    "        print (msg + ', (size():' + str (x.size()))\n",
    "    \n",
    "class CNNNumerAI(nn.Module):    \n",
    "    def __init__(self, n_input, n_hidden, n_output,n_cnn_kernel, n_mult_factor, n_padding,n_max_pool1d):\n",
    "        super(CNNNumerAI, self).__init__()    \n",
    "        self.n_input=n_input\n",
    "        self.n_hidden=n_hidden\n",
    "        self.n_output= n_output \n",
    "        self.n_cnn_kernel=n_cnn_kernel\n",
    "        self.n_mult_factor=n_mult_factor\n",
    "        self.n_padding=n_padding\n",
    "        self.n_max_pool1d=n_max_pool1d\n",
    "        self.n_l1=int((n_mult_factor * self.n_input) * (n_padding + 1) / n_max_pool1d)\n",
    "        \n",
    "            \n",
    "        self.features = nn.Sequential( # Mimicking AlexNet \n",
    "            torch.nn.Conv1d(self.n_input, self.n_hidden,kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(self.n_padding,)),                                             \n",
    "            torch.nn.ReLU(),            \n",
    "            torch.nn.MaxPool1d(kernel_size=self.n_max_pool1d),\n",
    "            \n",
    "            torch.nn.Conv1d(self.n_hidden, 3* self.n_hidden,kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(self.n_padding,)),                                             \n",
    "            torch.nn.ReLU(),            \n",
    "            torch.nn.MaxPool1d(kernel_size=self.n_max_pool1d),\n",
    "            \n",
    "            torch.nn.Conv1d(3*self.n_hidden, 6* self.n_hidden,kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(self.n_padding,)),                                             \n",
    "            torch.nn.ReLU(),            \n",
    "            torch.nn.MaxPool1d(kernel_size=self.n_max_pool1d),\n",
    "            \n",
    "            torch.nn.Conv1d(6*self.n_hidden, 5* self.n_hidden,kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(self.n_padding,)),                                             \n",
    "            torch.nn.ReLU(),            \n",
    "            torch.nn.MaxPool1d(kernel_size=self.n_max_pool1d),\n",
    "            \n",
    "            torch.nn.Conv1d(5*self.n_hidden, 5* self.n_hidden,kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(self.n_padding,)),                                             \n",
    "            torch.nn.ReLU(),            \n",
    "            torch.nn.MaxPool1d(kernel_size=self.n_max_pool1d),                        \n",
    "        )                 \n",
    "        \n",
    "        self.classifier = nn.Sequential( # Mimicking AlexNet \n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            torch.nn.Linear(int(self.n_hidden),int(self.n_hidden/2)),\n",
    "            torch.nn.ReLU(),            \n",
    "            \n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            torch.nn.Linear(int(self.n_hidden/2),int(self.n_hidden/2)),\n",
    "            torch.nn.ReLU(),            \n",
    "            \n",
    "#             torch.nn.Linear(self.n_l1,int(self.n_l1/2)),\n",
    "            torch.nn.Linear(int(self.n_hidden/2),self.n_output)            \n",
    "        )                                 \n",
    "        self.sig=nn.Sigmoid()\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         debug('raw',x)   \n",
    "        varSize=x.data.shape[0] # must be calculated here in forward() since its is a dynamic size                          \n",
    "        # for CNN  \n",
    "        x=x.contiguous() \n",
    "        x = x.view(varSize,self.n_input,1)\n",
    "        debug('after view',x)   \n",
    "        x=self.features(x)\n",
    "        debug('after CNN',x)   \n",
    "        # for Linear layer\n",
    "#         x = x.view(varSize,self.n_l1) \n",
    "        x = x.view(varSize,int(self.n_hidden)) \n",
    "        debug('after 2nd view',x)                  \n",
    "        x=self.classifier(x)   \n",
    "        debug('after self.out',x)   \n",
    "        x=self.sig(x)\n",
    "        return x\n",
    "\n",
    "net = CNNNumerAI(n_input, n_hidden, n_output,n_cnn_kernel, n_mult_factor, n_padding, n_max_pool1d)    \n",
    "if use_cuda:\n",
    "    net=net.cuda() \n",
    "lgr.info(net)\n",
    "b = net(X_tensor_train)\n",
    "print ('(b.size():' + str (b.size())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:<torch.optim.adam.Adam object at 0x7fe4fb1bee50>\n",
      "INFO:__main__:BCELoss (\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# NN params\n",
    "LR = 0.005\n",
    "MOMENTUM= 0.9\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR,weight_decay=5e-4) #  L2 regularization\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    net.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the CPU\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "INFO:__main__:Using the CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108405, 253)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "torch.Size([108405, 253])\n",
      "(108405, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n",
      "(108405, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'> <class 'torch.FloatTensor'>\n",
      "0 [ 0.69337827]\n",
      "ACC=0.0, LOG_LOSS=2.52758445641, ROC_AUC=0.492477588998 \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()    \n",
    "epochs=200\n",
    "all_losses = []\n",
    "\n",
    "DEBUG_ON=False\n",
    "\n",
    "X_tensor_train= XnumpyToTensor(trainX)\n",
    "Y_tensor_train= YnumpyToTensor(trainY)\n",
    "\n",
    "print(type(X_tensor_train.data), type(Y_tensor_train.data)) # should be 'torch.cuda.FloatTensor'\n",
    "\n",
    "# From here onwards, we must only use PyTorch Tensors\n",
    "for step in range(epochs):\n",
    "    out = net(X_tensor_train)                 # input x and predict based on x\n",
    "    cost = loss_func(out, Y_tensor_train)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    cost.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "                           \n",
    "    if step % 10 == 0:        \n",
    "        loss = cost.data[0]\n",
    "        all_losses.append(loss)\n",
    "        print(step, cost.data.cpu().numpy())        \n",
    "        prediction = (net(X_tensor_train).data).float() # probabilities             \n",
    "        pred_y = prediction.cpu().numpy().squeeze()\n",
    "        target_y = Y_tensor_train.cpu().data.numpy()                        \n",
    "        tu = ((pred_y == target_y).mean(),log_loss(target_y, pred_y),roc_auc_score(target_y,pred_y ))\n",
    "        print ('ACC={}, LOG_LOSS={}, ROC_AUC={} '.format(*tu))        \n",
    "                \n",
    "end_time = time.time()\n",
    "print ('{} {:6.3f} seconds'.format('GPU:', end_time-start_time))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_losses)\n",
    "plt.show()\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y,pred_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('LOG_LOSS=' + str(log_loss(target_y, pred_y)))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation set + ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "# Validation data\n",
    "print (valX.shape)\n",
    "print (valY.shape)\n",
    "\n",
    "X_tensor_val= XnumpyToTensor(valX)\n",
    "Y_tensor_val= YnumpyToTensor(valY)\n",
    "\n",
    "\n",
    "print(type(X_tensor_val.data), type(Y_tensor_val.data)) # should be 'torch.cuda.FloatTensor'\n",
    "\n",
    "predicted_val = (net(X_tensor_val).data).float() # probabilities \n",
    "# predicted_val = (net(X_tensor_val).data > 0.5).float() # zero or one\n",
    "pred_y = predicted_val.cpu().numpy()\n",
    "target_y = Y_tensor_val.cpu().data.numpy()                \n",
    "\n",
    "print (type(pred_y))\n",
    "print (type(target_y))\n",
    "\n",
    "tu = (str ((pred_y == target_y).mean()),log_loss(target_y, pred_y),roc_auc_score(target_y,pred_y ))\n",
    "print ('\\n')\n",
    "print ('acc={} log_loss={} roc_auc={} '.format(*tu))\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y,pred_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('LOG_LOSS=' + str(log_loss(target_y, pred_y)))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# print (pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set + Submission file for Numerai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (df_test_set.shape)\n",
    "columns = ['id', 'probability']\n",
    "df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n",
    "df_pred.id.astype(int)\n",
    "\n",
    "for index, row in df_test_set.iterrows():\n",
    "    rwo_no_id=row.drop('id')    \n",
    "#     print (rwo_no_id.values)    \n",
    "    x_data_np = np.array(rwo_no_id.values, dtype=np.float32)        \n",
    "    if use_cuda:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "                    \n",
    "    X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n",
    "    predicted_val = (net(X_tensor_test).data).float() # probabilities     \n",
    "    p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n",
    "    \n",
    "    df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n",
    "    \n",
    "df_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pred.id=df_pred.id.astype(int)\n",
    "\n",
    "def savePred(df_pred, loss):\n",
    "#     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n",
    "    csv_path = 'pred/pred_{}_{}.csv'.format(loss, (str(time.time())))\n",
    "    df_pred.to_csv(csv_path, columns=('id', 'probability'), index=None)\n",
    "    print (csv_path)\n",
    "    \n",
    "savePred (df_pred, log_loss(target_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "controls": "true",
   "history": "true",
   "mouseWheel": "true",
   "overview": "true",
   "progress": "true",
   "scroll": "true",
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
