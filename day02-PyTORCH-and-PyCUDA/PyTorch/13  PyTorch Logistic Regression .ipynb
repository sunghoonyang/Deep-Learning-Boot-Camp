{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Bootcamp November 2017, GPU Computing for Data Scientists\n",
    "\n",
    "<img src=\"../images/bcamp.png\" align=\"center\">\n",
    "\n",
    "## 13  PyTorch Logistic Regression \n",
    "\n",
    "Web: https://www.meetup.com/Tel-Aviv-Deep-Learning-Bootcamp/events/241762893/\n",
    "\n",
    "Notebooks: <a href=\"https://github.com/QuantScientist/Data-Science-PyCUDA-GPU\"> On GitHub</a>\n",
    "\n",
    "*Shlomo Kashani*\n",
    "\n",
    "<img src=\"../images/pt.jpg\" width=\"35%\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:1.2.1\n",
      "__Python VERSION: 2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "__pyTorch VERSION: 0.1.12+4eb448a\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: 5110\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n"
     ]
    }
   ],
   "source": [
    "# !pip install pycuda\n",
    "%reset -f\n",
    "import numpy\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots\n",
    "import tensorflow as tf \n",
    "print(\"tensorflow:\" + tf.__version__)\n",
    "!set \"KERAS_BACKEND=tensorflow\"\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu75/torch-0.2.0.post1-cp27-cp27mu-manylinux1_x86_64.whl\n",
    "# !pip install torchvision \n",
    "# ! pip install cv2\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Load a CSV file for Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 759 entries, 0 to 758\n",
      "Data columns (total 8 columns):\n",
      "0    759 non-null float32\n",
      "1    759 non-null float32\n",
      "2    759 non-null float32\n",
      "3    759 non-null float32\n",
      "4    759 non-null float32\n",
      "5    759 non-null float32\n",
      "6    759 non-null float32\n",
      "7    759 non-null float32\n",
      "dtypes: float32(8)\n",
      "memory usage: 29.6 KB\n"
     ]
    }
   ],
   "source": [
    "% reset -f\n",
    "\n",
    "# ! pip install tables\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "F_NAME_TRAIN= 'data-03-diabetes.csv'\n",
    "# F_NAME_TRAIN='numerai/numerai_training_data.csv'\n",
    "\n",
    "# X_df_train= pd.read_csv(F_NAME_TRAIN)\n",
    "X_df_train= pd.read_csv(F_NAME_TRAIN,header=None, dtype=np.float32)\n",
    "X_df_train_SINGLE=X_df_train.copy(deep=True)   \n",
    "\n",
    "\n",
    "# X_df_train_SINGLE.drop('id', axis=1, inplace=True)\n",
    "# X_df_train_SINGLE.drop('era', axis=1, inplace=True)\n",
    "# X_df_train_SINGLE.drop('data_type', axis=1, inplace=True)\n",
    "# drop the header\n",
    "# X_df_train_SINGLE.to_csv('numerai/numerai_training_data_clean.csv', header=False)\n",
    "# X_df_train_SINGLE= pd.read_csv('numerai/numerai_training_data_clean.csv', header=None, dtype=np.float32)\n",
    "\n",
    "# X_df_train_SINGLE=X_df_train_SINGLE.dropna()\n",
    "    \n",
    "answers_1_SINGLE = list (X_df_train_SINGLE[X_df_train_SINGLE.columns[-1]].values)\n",
    "answers_1_SINGLE= map(int, answers_1_SINGLE)\n",
    "X_df_train_SINGLE = X_df_train_SINGLE.drop(X_df_train_SINGLE.columns[-1], axis=1)\n",
    "# X_df_train_SINGLE=X_df_train_SINGLE.apply(lambda x: pandas.to_numeric(x, errors='ignore'))  \n",
    "\n",
    "print(X_df_train_SINGLE.shape)\n",
    "X_df_train_SINGLE.head(5)\n",
    "\n",
    "# (np.where(np.isnan(X_df_train_SINGLE)))\n",
    "# (np.where(np.isinf(X_df_train_SINGLE)))\n",
    "\n",
    "X_df_train_SINGLE.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Create PyTorch GPU tensors\n",
    "\n",
    "- Note how we transfrom the np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 8) (508, 1)\n",
      "<type 'numpy.ndarray'> <type 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'> <class 'torch.LongTensor'>\n",
      "<class 'torch.FloatTensor'> <class 'torch.LongTensor'>\n"
     ]
    }
   ],
   "source": [
    "use_cuda = False\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "\n",
    "# fix seed\n",
    "seed=17*19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "        \n",
    "\n",
    "# sk learn\n",
    "trainX, testX, trainY, testY = train_test_split(X_df_train_SINGLE, answers_1_SINGLE, test_size=.33, random_state=999)  \n",
    "\n",
    "# Train data\n",
    "x_data_np = np.array(trainX.values, dtype=np.float32)\n",
    "y_data_np = np.array(trainY, dtype=np.float32)\n",
    "y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "\n",
    "\n",
    "print(x_data_np.shape, y_data_np.shape)\n",
    "print(type(x_data_np), type(y_data_np))\n",
    "\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    X = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch\n",
    "    Y = Variable(torch.from_numpy(y_data_np).cuda())\n",
    "else:\n",
    "    lgr.info (\"Using the CPU\")\n",
    "    X = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "    Y = Variable(torch.from_numpy(y_data_np))    \n",
    "print(type(X.data), type(Y.data)) # should be 'torch.cuda.FloatTensor'\n",
    "    \n",
    "print(type(X.data), type(Y.data)) # should be 'torch.cuda.FloatTensor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Define the NN model\n",
    "\n",
    "- First a simple two leyer network and then a more involved version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Initial weights selection\n",
    "\n",
    "- There are many ways to select the initial weights to a neural network architecture. A common initialization scheme is random initialization, which sets the biases and weights of all the nodes in each hidden layer randomly, so they are in a random point of the space, and objective function, and then find a nearby local minima using an algorithm like SGD or Adam.\n",
    "- We use a *xavier initializer*, in effect (according to theory) initializing the weights of the network to values that would be closer to the optimal, and therefore require less epochs to train.\n",
    "\n",
    "### References: \n",
    "* **`nninit.xavier_uniform(tensor, gain=1)`** - Fills `tensor` with values according to the method described in [\"Understanding the difficulty of training deep feedforward neural networks\" - Glorot, X. and Bengio, Y.](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf), using a uniform distribution.\n",
    "* **`nninit.xavier_normal(tensor, gain=1)`** - Fills `tensor` with values according to the method described in [\"Understanding the difficulty of training deep feedforward neural networks\" - Glorot, X. and Bengio, Y.](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf), using a normal distribution.\n",
    "* **`nninit.kaiming_uniform(tensor, gain=1)`** - Fills `tensor` with values according to the method described in [\"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification\" - He, K. et al.](https://arxiv.org/abs/1502.01852) using a uniform distribution.\n",
    "* **`nninit.kaiming_normal(tensor, gain=1)`** - Fills `tensor` with values according to the method described in [\"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification\" - He, K. et al.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the CPU\n",
      "INFO:__main__:Model Sequential (\n",
      "  (0): Linear (8 -> 1)\n",
      "  (1): Sigmoid ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "keep_prob=0.85\n",
    "# p is the probability of being dropped in PyTorch\n",
    "dropout = torch.nn.Dropout(p=1 - keep_prob)\n",
    "\n",
    "\n",
    "# hiddenLayer1Size=32\n",
    "# hiddenLayer2Size=16\n",
    "# # # Hypothesis using sigmoid\n",
    "# linear1=torch.nn.Linear(x_data_np.shape[1], hiddenLayer1Size, bias=True) # size mismatch, m1: [5373 x 344], m2: [8 x 1] at /pytorch/torch/lib/TH/generic/THTensorMath.c:1293\n",
    "# # xavier initializer\n",
    "# torch.nn.init.xavier_uniform(linear1.weight)\n",
    "# linear2=torch.nn.Linear(hiddenLayer1Size, hiddenLayer2Size)\n",
    "# # xavier initializer\n",
    "# torch.nn.init.xavier_uniform(linear2.weight)\n",
    "# linear3=torch.nn.Linear(hiddenLayer2Size, 1)\n",
    "# # xavier initializer\n",
    "# torch.nn.init.xavier_uniform(linear3.weight)\n",
    "# sigmoid = torch.nn.Sigmoid()\n",
    "# tanh=torch.nn.Tanh()\n",
    "# model = torch.nn.Sequential(linear1,dropout, tanh, linear2,dropout, tanh, linear3,dropout, sigmoid)\n",
    "\n",
    "\n",
    "#Hypothesis using sigmoid\n",
    "linear1=torch.nn.Linear(x_data_np.shape[1], 1, bias=True) \n",
    "# xavier initializer\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "# model = torch.nn.Sequential(linear1,dropout, sigmoid)\n",
    "model = torch.nn.Sequential(linear1, sigmoid)\n",
    "\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")\n",
    "    model = model.cuda() # On GPU\n",
    "else:\n",
    "    lgr.info (\"Using the CPU\")   \n",
    "\n",
    "lgr.info('Model {}'.format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Optimizer <torch.optim.sgd.SGD object at 0x7fc855dbc350>\n"
     ]
    }
   ],
   "source": [
    "# see https://github.com/facebookresearch/SentEval/blob/master/senteval/tools/classifier.py\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-1,momentum=0.9, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "lgr.info('Optimizer {}'.format(optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The cross-entropy loss function\n",
    "\n",
    "\n",
    "A binary cross-entropy `Criterion` (which expects 0 or 1 valued targets) :\n",
    "\n",
    "```lua\n",
    "criterion = nn.BCECriterion()\n",
    "``` \n",
    "\n",
    "The BCE loss is defined as :\n",
    "\n",
    "<img src=\"../images/bce2.png\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\mathbf{Loss Function:} J(x, z) = -\\sum_k^d[x_k \\log z_k + (1-x_k)log(1-z_k)]$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "sp.interactive.printing.init_printing(use_latex=True)\n",
    "from IPython.display import display, Math, Latex\n",
    "maths = lambda s: display(Math(s))\n",
    "latex = lambda s: display(Latex(s))\n",
    "\n",
    "#the loss function is as follows:\n",
    "maths(\"\\mathbf{Loss Function:} J(x, z) = -\\sum_k^d[x_k \\log z_k + (1-x_k)log(1-z_k)]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Start training in Batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mul received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (int value)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-37e3ea2d5082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# cost/loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     cost = -(Y * torch.log(hypothesis) + (1 - Y)\n\u001b[0m\u001b[1;32m     12\u001b[0m              * torch.log(1 - hypothesis)).mean()\n\u001b[1;32m     13\u001b[0m     \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0m__rmul__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mMul\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/basic_ops.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, a, b)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: mul received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (int value)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()    \n",
    "epochs=20000\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "for step in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)    \n",
    "    # cost/loss function\n",
    "    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n",
    "             * torch.log(1 - hypothesis)).mean()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Keep loss \n",
    "    if step % 150 == 0:\n",
    "        loss = cost.data[0]\n",
    "        all_losses.append(loss)\n",
    "        \n",
    "    if step % 4000 == 0:\n",
    "        print(step, cost.data.cpu().numpy())\n",
    "        # RuntimeError: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). \n",
    "        # Use .cpu() to move the tensor to host memory first.\n",
    "        predicted = (model(X).data > 0.5).float()\n",
    "#         predicted = (model(X).data ).float() # This is like predict proba\n",
    "        predictions=predicted.cpu().numpy()\n",
    "        accuracy = (predicted == Y.data).float().mean()\n",
    "        print('TRAINNING Accuracy:' + str(accuracy))\n",
    "#         print ('TRAINING LOG_LOSS=' + str(log_loss(trainY, predictions)))\n",
    "#         R_SCORE=roc_auc_score(Y.data.cpu().numpy(),predictions )        \n",
    "#         print ('TRAINING ROC AUC:' + str(R_SCORE))\n",
    "                \n",
    "end_time = time.time()\n",
    "print ('{} {:6.3f} seconds'.format('GPU:', end_time-start_time))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross validation, metrics, ROC_AUC etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251, 8) (251, 1)\n",
      "<type 'numpy.ndarray'> <type 'numpy.ndarray'>\n",
      "VALIDATION ROC AUC:0.849965493444\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICisVmHMNjoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc855a65450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Validation data\n",
    "x_data_np_val = np.array(testX.values, dtype=np.float32)\n",
    "y_data_np_val = np.array(testY, dtype=np.float32)\n",
    "y_data_np_val=y_data_np_val.reshape((y_data_np_val.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "\n",
    "\n",
    "print(x_data_np_val.shape, y_data_np_val.shape)\n",
    "print(type(x_data_np_val), type(y_data_np_val))\n",
    "\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    X_val = Variable(torch.from_numpy(x_data_np_val).cuda()) # Note the conversion for pytorch\n",
    "    Y_val = Variable(torch.from_numpy(y_data_np_val).cuda())\n",
    "else:\n",
    "    lgr.info (\"Using the CPU\")\n",
    "    X_val = Variable(torch.from_numpy(x_data_np_val)) # Note the conversion for pytorch\n",
    "    Y_val = Variable(torch.from_numpy(y_data_np_val))\n",
    "\n",
    "# VALIDATION\n",
    "predicted_val = (model(X_val).data).float()\n",
    "predictions_val=predicted_val.cpu().numpy()\n",
    "accuracy_val = (predicted_val == Y_val.data).float().mean()\n",
    "R_SCORE_VAL=roc_auc_score(Y_val.data.cpu().numpy(),predictions_val)        \n",
    "print ('VALIDATION ROC AUC:' + str(R_SCORE_VAL))\n",
    "\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(testY, predictions_val)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('LOG_LOSS=' + str(log_loss(testY, predictions_val)))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lab 5 Logistic Regression Classifier\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(777)  # for reproducibility\n",
    "\n",
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Make sure the shape and data are OK\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "X = Variable(torch.from_numpy(x_data))\n",
    "Y = Variable(torch.from_numpy(y_data))\n",
    "\n",
    "# Hypothesis using sigmoid\n",
    "linear = torch.nn.Linear(8, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear, sigmoid)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n",
    "             * torch.log(1 - hypothesis)).mean()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(step, cost.data.numpy())\n",
    "\n",
    "# Accuracy computation\n",
    "predicted = (model(X).data > 0.5).float()\n",
    "accuracy = (predicted == Y.data).float().mean()\n",
    "print(\"\\nHypothesis: \", hypothesis.data.numpy(), \"\\nCorrect (Y): \", predicted.numpy(), \"\\nAccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "controls": "true",
   "history": "true",
   "mouseWheel": "true",
   "overview": "true",
   "progress": "true",
   "scroll": "true",
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
