{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9dd723cf-8dee-4234-94fe-eb7996388f62",
    "_uuid": "292c4605e2c18ebc4a1bc4879c3344dabb20fcee"
   },
   "source": [
    "# PyTorch GPU based CNN with predictions\n",
    "\n",
    "This notebook is heavily based on:\n",
    "https://www.kaggle.com/nanigans/pytorch-starter\n",
    "\n",
    "Modifications:\n",
    "1. Uses a custom data loader based on torch.utils.data.Dataset\n",
    "1. Works on a **GPU** as well as on a **CPU** out of the box\n",
    "1. Runs predictions on the **Test data set**\n",
    "1. Creates a **submission file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "303a9986-71cd-41d0-b53a-0b26ac4007cb",
    "_uuid": "c1cf1feee03e3784251c9ae345244d9ae5375af3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "import os\n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7358dba4-24e6-41ca-8ed3-8776a1914f3e",
    "_uuid": "98b66f1ed8ea1a6e776d038d0c445ea20f3e53b0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f63eb8d-84cb-44fc-8622-3fb5fa128a28",
    "_uuid": "4713772f686ceef6a2ca8f95115467444aba6341",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix seed\n",
    "seed=17*19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "90a5dbd6-75e9-4183-8e09-8e91c63c0b10",
    "_uuid": "0901ccf48a660dd6a01287cf8d648e7d3a37d715"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5861c386-86d2-4177-900e-7ed30aa76795",
    "_uuid": "db4d4d6154afcb3a5d444b0f288f04b9b15e35c0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('../input/train.json')\n",
    "\n",
    "data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n",
    "\n",
    "\n",
    "band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "full_img = np.stack([band_1, band_2], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f5c1b74a-631e-4bf6-9188-0eba5905e26c",
    "_uuid": "b4d6dd93df146fb4026832aa447b4663024dd63c"
   },
   "source": [
    "# From Numpy to PyTorch GPU tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15bfdd0f-681b-4d76-a19e-1491e83bbb7c",
    "_uuid": "31d151252657c896e29784513f00cdd555efa1e7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np, dtype=np.float32)        \n",
    "#     print(x_data_np.shape)\n",
    "#     print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "#         lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = (torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "#         lgr.info (\"Using the CPU\")\n",
    "        X_tensor = (torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "        \n",
    "#     print((X_tensor.shape)) # torch.Size([108405, 29])\n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "#     print(y_data_np.shape)\n",
    "#     print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "#         lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "#         lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n",
    "\n",
    "#     print(type(Y_tensor)) # should be 'torch.cuda.FloatTensor'\n",
    "#     print(y_data_np.shape)\n",
    "#     print(type(y_data_np))    \n",
    "    return Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5550f4d-6871-449c-a084-901f7e266ef9",
    "_uuid": "3b4b633950fe1623fb1a314fc3fb2d6458978e87"
   },
   "source": [
    "# Train / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc4ffad0-682b-4189-90ff-2ba3f7dd06d1",
    "_uuid": "03a65e001dd2f489526d69d9050ced2c7f86b278",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullTrainningDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainningDataset, self).__init__()\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.full_ds[i+self.offset]\n",
    "    \n",
    "validationRatio=0.11    \n",
    "\n",
    "def trainTestSplit(dataset, val_share=validationRatio):\n",
    "    val_offset = int(len(dataset)*(1-val_share))\n",
    "    print (\"Offest:\" + str(val_offset))\n",
    "    return FullTrainningDataset(dataset, 0, val_offset), FullTrainningDataset(dataset, val_offset, len(dataset)-val_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f845a93e-faa5-40c6-af89-14e813a8ba34",
    "_uuid": "e99daa83567a13ab4e86e9451e03995c731e6c2b"
   },
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c2b51ec-1eeb-4519-bb56-49a3dc525386",
    "_uuid": "dccb943235b8fd0cae479601e70f864526a8df86",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "transformations = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# train_imgs = torch.from_numpy(full_img_tr).float()\n",
    "train_imgs=XnumpyToTensor (full_img)\n",
    "train_targets = YnumpyToTensor(data['is_iceberg'].values)\n",
    "dset_train = TensorDataset(train_imgs, train_targets)\n",
    "\n",
    "\n",
    "train_ds, val_ds = trainTestSplit(dset_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, \n",
    "                                            num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "print (train_loader)\n",
    "print (val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9655dd9c-c792-4d15-bb8b-94a800eb9c0e",
    "_uuid": "21eacbdcf5f04eecf86da49621b5aff4d3b92d8d"
   },
   "source": [
    "## Define simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "03178a5e-e8ef-4334-a97d-f1cf7c0fecf8",
    "_uuid": "4112e0197ba91e6ab2d39c65d67ed112fdc78729",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout = torch.nn.Dropout(p=0.20)\n",
    "relu=torch.nn.LeakyReLU()\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        \n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(2, 16, kernel_size=7, padding=2),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.AvgPool2d(2,2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=2, padding=2),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.AvgPool2d(2,2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=2, padding=2),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.AvgPool2d(2,2)\n",
    "        )                               \n",
    "        self.features = nn.Sequential( \n",
    "            self.conv1,dropout,          \n",
    "            self.conv2,                       \n",
    "            self.conv3,            \n",
    "        )        \n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(256, 1),             \n",
    "        )\n",
    "        self.sig=nn.Sigmoid()        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)        \n",
    "        x = x.view(x.size(0), -1)        \n",
    "        x = self.classifier(x)                \n",
    "        x = self.sig(x)\n",
    "        return x        \n",
    "model = Net()    \n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7025d0d8-206f-4cb0-af69-7b1a85ebd7f8",
    "_uuid": "abad58921a6aeb5ea0603056bb0021436b73d039",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_func=torch.nn.BCELoss()\n",
    "# NN params\n",
    "LR = 0.005\n",
    "MOMENTUM= 0.9\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=5e-5) #  L2 regularization\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    net.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "86849fe7-7c0c-4cde-be41-b5809ec7862b",
    "_uuid": "531c3edfa7dce5409c72f87a19160f9d4150b30b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epoches = 20\n",
    "criterion=loss_func\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoches):\n",
    "    print('Epoch {}'.format(epoch + 1))\n",
    "    print('*' * 5 + ':')\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        \n",
    "        img, label = data        \n",
    "        img = Variable(img)\n",
    "        label = Variable(label)\n",
    "                        \n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss.data[0] * label.size(0)        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                                \n",
    "        if i % 10 == 0:\n",
    "            all_losses.append(running_loss / (batch_size * i))\n",
    "            print('[{}/{}] Loss: {:.6f}'.format(\n",
    "                epoch + 1, num_epoches, running_loss / (batch_size * i),\n",
    "                running_acc / (batch_size * i)))\n",
    "            \n",
    "    print('Finish {} epoch, Loss: {:.6f}'.format(epoch + 1, running_loss / (len(train_ds))))\n",
    "\n",
    "torch.save(model.state_dict(), './cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d7a94ac7-9d78-4aef-9401-0cad5840af9a",
    "_uuid": "4ace55d09023eeac33296d70c037bda67701d45a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def kFoldValidation(folds): \n",
    "    print ('K FOLD VALIDATION ...')\n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "    \n",
    "    for e in range(folds):\n",
    "        print ('Fold:' + str(e))        \n",
    "        data = pd.read_json('../input/train.json')        \n",
    "        data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "        data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "        data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n",
    "        band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "        band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "        full_img = np.stack([band_1, band_2], axis=1)\n",
    "        \n",
    "        X_train,X_val,y_train,y_val=train_test_split(full_img,data['is_iceberg'].values,\n",
    "                                                   test_size=0.22, \n",
    "                                                   random_state=(1+e))\n",
    "        val_imgs=XnumpyToTensor (X_val)\n",
    "        val_targets = YnumpyToTensor(y_val)\n",
    "        \n",
    "        dset_val = TensorDataset(val_imgs, val_targets)        \n",
    "        val_loader = torch.utils.data.DataLoader(dset_val, batch_size=batch_size, shuffle=True, \n",
    "                                                    num_workers=1)        \n",
    "        print (val_loader)\n",
    "\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        for data in val_loader:\n",
    "            img, label = data\n",
    "\n",
    "            img = Variable(img, volatile=True)\n",
    "            label = Variable(label, volatile=True)\n",
    "\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data[0] * label.size(0)\n",
    "\n",
    "        print('VALIDATION Loss: {:.6f}'.format(eval_loss / (len(dset_val))))\n",
    "        val_losses.append(eval_loss / (len(dset_val)))\n",
    "        print()\n",
    "    \n",
    "def LeavOneOutValidation(val_loader): \n",
    "    print ('Leave one out VALIDATION ...')\n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "        \n",
    "    print (val_loader)\n",
    "\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    for data in val_loader:\n",
    "        img, label = data\n",
    "\n",
    "        img = Variable(img, volatile=True)\n",
    "        label = Variable(label, volatile=True)\n",
    "\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss.data[0] * label.size(0)\n",
    "\n",
    "    print('Leave on out VALIDATION Loss: {:.6f}'.format(eval_loss / (len(val_ds))))\n",
    "    val_losses.append(eval_loss / (len(val_ds)))\n",
    "    print()\n",
    "    print()        \n",
    "    \n",
    "LeavOneOutValidation(val_loader)    \n",
    "# kFoldValidation(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3f6d0cdd-ae03-4128-9d7b-7797d58bdfb5",
    "_uuid": "151d206a8aafec78833f400179fa58b6d63bfed4"
   },
   "source": [
    "# Prediction on test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5316df6-e3fa-4372-a284-30ef1d143262",
    "_uuid": "2c73c83252ce8d9977df59a40fa000e5b10ae32d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_set = pd.read_json('../input/test.json')\n",
    "\n",
    "df_test_set['band_1'] = df_test_set['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "df_test_set['band_2'] = df_test_set['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "df_test_set['inc_angle'] = pd.to_numeric(df_test_set['inc_angle'], errors='coerce')\n",
    "\n",
    "df_test_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d80fe5d9-07bc-4254-bef9-d8721b9bd226",
    "_uuid": "c486408992e2dfcf0d1a5730d3f6bb051c839c91",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (df_test_set.shape)\n",
    "columns = ['id', 'is_iceberg']\n",
    "df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n",
    "# df_pred.id.astype(int)\n",
    "\n",
    "for index, row in df_test_set.iterrows():\n",
    "    rwo_no_id=row.drop('id')    \n",
    "    band_1_test = (rwo_no_id['band_1']).reshape(-1, 75, 75)\n",
    "    band_2_test = (rwo_no_id['band_2']).reshape(-1, 75, 75)\n",
    "    full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n",
    "\n",
    "    x_data_np = np.array(full_img_test, dtype=np.float32)        \n",
    "    if use_cuda:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "                    \n",
    "#     X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n",
    "    predicted_val = (model(X_tensor_test).data).float() # probabilities     \n",
    "    p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n",
    "    \n",
    "    df_pred = df_pred.append({'id':row['id'], 'is_iceberg':p_test},ignore_index=True)\n",
    "#     df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n",
    "\n",
    "df_pred.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "47afcb96-d2cc-4212-9b15-a33f0777c39b",
    "_uuid": "fcd82743cc346d03f97ad6c2684cb113ab71c1ac"
   },
   "source": [
    "# Create a CSV with the ID's and the coresponding probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "24ebbc8b-01e9-4f8f-8682-34b54720a082",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "949e671d86d82fe3fea05bbca355aa380a043405",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_pred.id=df_pred.id.astype(int)\n",
    "\n",
    "def savePred(df_pred):\n",
    "#     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n",
    "#     csv_path = 'pred_{}_{}.csv'.format(loss, (str(time.time())))\n",
    "    csv_path='sample_submission.csv'\n",
    "    df_pred.to_csv(csv_path, columns=('id', 'is_iceberg'), index=None)\n",
    "    print (csv_path)\n",
    "    \n",
    "savePred (df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6d70809a-43c5-40e0-bcbd-2e57edfca9d7",
    "_uuid": "a0c5d23ee263765549dcc03e80c8d9c53fbb1944",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ad22517-6a3b-4361-bc7a-f099034b356b",
    "_uuid": "4c32cd8ca4e3611704a172661b9ba759f6b309ce",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "08fd0448-01b3-4a6a-95e5-9ce36ad9bcb9",
    "_uuid": "86a735b3faa63af3b1420678e1fb812a4bb95f56",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "174e096e-6cbd-40b5-8553-2560bf28582d",
    "_uuid": "dc443e947603c93d1eeb77bb0dfde5f9f297604b",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c206f5a4-7b13-4845-8327-9c7f5406fe90",
    "_uuid": "2b02d612822db2e58f39591c8e329b97d6283765",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba365f1e-911b-401e-b9ce-9b763c8e4f10",
    "_uuid": "6bfa0dbab5b766e9fc38c1dc0bde2f0e1952b5d5",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c971cf5e-2bb2-4b82-b09a-329e66a2854c",
    "_uuid": "4ad034f7f7947e36fdaac0e2894d35de27054d2b",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a555a4c2-ab48-4e7c-8634-b0b65811ff2c",
    "_uuid": "bf2b39e71d690f3900b916176435974ab10c5918",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "afcb62b2-2b25-4873-a09f-0caacb5a7fb2",
    "_uuid": "e836872cf64407e8e2ec6d76906fa6ead82893f1",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b154a7d-69bc-4acc-9287-4b7bb38eff90",
    "_uuid": "b6581f6927bba3c816a4ef4f71ddf91956c0ef68",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
