{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## PyTorch Speech Recognition Challenge\n",
    "\n",
    "https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "\n",
    "\n",
    "Notebooks: <a href=\"https://github.com/QuantScientist/Data-Science-PyCUDA-GPU\"> On GitHub</a>\n",
    "\n",
    "\n",
    "#### References:\n",
    "\n",
    "- http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "- https://www.bountysource.com/issues/44576966-a-tutorial-on-writing-custom-datasets-samplers-and-using-transforms\n",
    "\n",
    "- https://medium.com/towards-data-science/my-first-kaggle-competition-9d56d4773607\n",
    "\n",
    "- https://github.com/sohyongsheng/kaggle-planet-forest\n",
    "\n",
    "- https://github.com/rwightman/pytorch-planet-amazon/blob/master/dataset.py\n",
    "\n",
    "\n",
    "## PyTorch dada sets\n",
    "\n",
    "- Convert the audio files into images (spectogram) \n",
    "- Create a CSV file consisting of the good labels \n",
    "- Then write a custom PyTorch data loader\n",
    "- Simple CNN\n",
    "\n",
    "## Issues:\n",
    "- Problem with the loss function for the multi-class case during training, loss is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.6.2 |Anaconda custom (64-bit)| (default, Sep 19 2017, 08:03:39) [MSC v.1900 64 bit (AMD64)]\n",
      "__pyTorch VERSION: 0.2.1+a4fc05a\n",
      "__CUDA VERSION\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2016 NVIDIA Corporation\n",
      "Built on Mon_Jan__9_17:32:33_CST_2017\n",
      "Cuda compilation tools, release 8.0, V8.0.60\n",
      "__CUDNN VERSION: None\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n",
      "Current cuda device  0\n",
      "3.6.2 |Anaconda custom (64-bit)| (default, Sep 19 2017, 08:03:39) [MSC v.1900 64 bit (AMD64)]\n",
      "1.9\n",
      "svmem(total=68627443712, available=38805884928, percent=43.5, used=29821558784, free=38805884928)\n",
      "memory GB: 1.7805747985839844\n"
     ]
    }
   ],
   "source": [
    "% reset -f\n",
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "import os\n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setting up global variables\n",
    "\n",
    "- Root folder\n",
    "- Audio folder\n",
    "- Audio Label folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT ='d:/db/data/tf/'\n",
    "IMG_PATH = DATA_ROOT + '/picts/train/'\n",
    "IMG_EXT = '.png'\n",
    "IMG_DATA_LABELS = DATA_ROOT + '/train_v2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Turn WAV into Images\n",
    "- See https://www.kaggle.com/timolee/audio-data-conversion-to-images-eda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio_path = 'd:/db/data/tf/train/audio/'\n",
    "pict_Path = 'd:/db/data/tf//picts/train/'\n",
    "test_pict_Path = 'd:/db/data/tf//picts/test/'\n",
    "test_audio_path = 'd:/db/data/tf//test/audio/'\n",
    "samples = []\n",
    "\n",
    "\n",
    "if not os.path.exists(pict_Path):\n",
    "    os.makedirs(pict_Path)\n",
    "\n",
    "if not os.path.exists(test_pict_Path):\n",
    "    os.makedirs(test_pict_Path)\n",
    "    \n",
    "subFolderList = []\n",
    "\n",
    "for x in os.listdir(audio_path):\n",
    "    if os.path.isdir(audio_path + '/' + x):\n",
    "        subFolderList.append(x)\n",
    "        if not os.path.exists(pict_Path + '/' + x):\n",
    "            os.makedirs(pict_Path +'/'+ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Function: convert audio to spectogram images\n",
    "\n",
    "# def wav2img(wav_path, targetdir='', figsize=(4,4)):\n",
    "#     \"\"\"\n",
    "#     takes in wave file path\n",
    "#     and the fig size. Default 4,4 will make images 288 x 288\n",
    "#     \"\"\"\n",
    "#     fs = 44100 # sampling frequency\n",
    "    \n",
    "#     # use soundfile library to read in the wave files\n",
    "#     test_sound, samplerate = sf.read(wav_path)\n",
    "    \n",
    "#     # make the plot\n",
    "#     fig = plt.figure(figsize=figsize)\n",
    "#     S, freqs, bins, im = plt.specgram(test_sound, NFFT=1024, Fs=samplerate, noverlap=512)\n",
    "#     plt.show\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     ## create output path\n",
    "#     output_file = wav_path.split('/')[-1].split('.wav')[0]\n",
    "#     output_file = targetdir +'/'+ output_file\n",
    "#     plt.savefig('%s.png' % output_file)\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# def wav2img_waveform(wav_path, targetdir='', figsize=(4,4)):\n",
    "#     test_sound, samplerate = sf.read(sample_audio[0])\n",
    "#     fig = plt.figure(figsize=figsize)\n",
    "#     plt.plot(test_sound)\n",
    "#     plt.axis('off')\n",
    "#     output_file = wav_path.split('/')[-1].split('.wav')[0]\n",
    "#     output_file = targetdir +'/'+ output_file\n",
    "#     plt.savefig('%s.png' % output_file)\n",
    "#     plt.close()\n",
    "\n",
    "# ### Convert Training Audio\n",
    "# #### Loop through source audio and save as pictures \n",
    "# # (may take a while) may also consider running at commandline. \n",
    "# # Code is limited to 3 folders and 10 files each, get rid of array limits to process the entire directory\n",
    "\n",
    "# # c:\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:7221: RuntimeWarning: divide by zero encountered in log10\n",
    "# #   Z = 10. * np.log10(spec)\n",
    "\n",
    "# for i, x in enumerate(subFolderList):\n",
    "#     print(i, ':', x)\n",
    "#     # get all the wave files\n",
    "#     all_files = [y for y in os.listdir(audio_path + x) if '.wav' in y]\n",
    "#     for file in all_files:\n",
    "#         try:\n",
    "#             wav2img(audio_path + x + '/' + file, pict_Path + x)                \n",
    "#         except Exception:\n",
    "#             pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generate lables into a CSV, which is easier for PyTorch Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label-str</th>\n",
       "      <th>fullpath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00176480_nohash_0.png</td>\n",
       "      <td>down</td>\n",
       "      <td>d:/db/data/tf//picts/train/down/00176480_nohas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004ae714_nohash_0.png</td>\n",
       "      <td>down</td>\n",
       "      <td>d:/db/data/tf//picts/train/down/004ae714_nohas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00b01445_nohash_0.png</td>\n",
       "      <td>down</td>\n",
       "      <td>d:/db/data/tf//picts/train/down/00b01445_nohas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     img label-str  \\\n",
       "0  00176480_nohash_0.png      down   \n",
       "1  004ae714_nohash_0.png      down   \n",
       "2  00b01445_nohash_0.png      down   \n",
       "\n",
       "                                            fullpath  label  \n",
       "0  d:/db/data/tf//picts/train/down/00176480_nohas...      0  \n",
       "1  d:/db/data/tf//picts/train/down/004ae714_nohas...      0  \n",
       "2  d:/db/data/tf//picts/train/down/00b01445_nohas...      0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "d = defaultdict(LabelEncoder)\n",
    "\n",
    "# Build the pictures path\n",
    "subFolderList = []\n",
    "for x in os.listdir(pict_Path):\n",
    "    if os.path.isdir(pict_Path + '/' + x):\n",
    "        subFolderList.append(x)        \n",
    "            \n",
    "good_labels=['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "POSSIBLE_LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "\n",
    "# print (type(POSSIBLE_LABELS))\n",
    "# print (type(good_labels))\n",
    "columns = ['img', 'label-str','fullpath']\n",
    "df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n",
    "# df_pred.id.astype(int)\n",
    "\n",
    "for i, x in enumerate(subFolderList):\n",
    "    if (x in POSSIBLE_LABELS):\n",
    "    #     print(i, ':', x)\n",
    "        # get all the wave files\n",
    "        all_files = [y for y in os.listdir(pict_Path + x) if '.png' in y]\n",
    "        for file in all_files:\n",
    "    #         print (audio_path + x + '/' + file, pict_Path + x)\n",
    "            fullPath=pict_Path + x + '/' + file\n",
    "    #         print (fullPath)\n",
    "            df_pred = df_pred.append({'img':file, 'label-str':x,'fullpath':fullPath},ignore_index=True)\n",
    "    #         print (pict_Path + x)    \n",
    "    \n",
    "\n",
    "# Encode the categorical labels as numeric data\n",
    "df_pred['label'] = LabelEncoder().fit_transform(df_pred['label-str'])\n",
    "# Make sure we dont save the header\n",
    "df_pred.to_csv(IMG_DATA_LABELS, columns=('img','label-str','fullpath', 'label'), index=None, header=False)\n",
    "df_pred.to_csv(IMG_DATA_LABELS +'_header', columns=('img','label-str','fullpath', 'label'), index=None, header=True)\n",
    "\n",
    "# img,label,fullpath\n",
    "# 00176480_nohash_0.wav,down,d:/db/data/tf/train/audio/down/00176480_nohash_0.wav\n",
    "    \n",
    "df_pred.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Torch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "d = defaultdict(LabelEncoder)\n",
    "\n",
    "# Encoding the variable\n",
    "# X_df_train_SINGLE = X_df_train_SINGLE.apply(lambda x: d[x.name].fit_transform(x))\n",
    "# X_df_train_SINGLE=X_df_train_SINGLE.apply(LabelEncoder().fit_transform)\n",
    "# Inverse the encoded\n",
    "# fit.apply(lambda x: X_df_train_SINGLE[x.name].inverse_transform(x))\n",
    "# Using the dictionary to label future data\n",
    "# df.apply(lambda x: X_df_train_SINGLE[x.name].transform(x))\n",
    "# answers_1_SINGLE = list (X_df_train_SINGLE[singleResponseVariable].values)\n",
    "# answers_1_SINGLE= map(int, answers_1_SINGLE)\n",
    "\n",
    "def one_hot(df, cols):    \n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "    \n",
    "class GenericImageDataset(Dataset):    \n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "        \n",
    "        t = time.time()        \n",
    "        lgr.info('CSV path {}'.format(csv_path))\n",
    "        lgr.info('IMG path {}'.format(img_path))        \n",
    "        \n",
    "        assert img_ext in ['.png']\n",
    "        \n",
    "        tmp_df = pd.read_csv(csv_path, header=None) # img,label,fullpath\n",
    "                        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        # Encoding the variables                \n",
    "        lgr.info(\"DF CSV:\\n\" + str (tmp_df.head(3)))\n",
    "                        \n",
    "        self.X_train = tmp_df[2]        \n",
    "        \n",
    "        self.y_train = self.mlb.fit_transform(tmp_df[1].str.split()).astype(np.float32)           \n",
    "        self.y_train=self.y_train.reshape((self.y_train.shape[0]*10,1)) # Must be reshaped for PyTorch!\n",
    "        \n",
    "        tmp_df[1]=one_hot(tmp_df[1],tmp_df[1])\n",
    "        self.y_train = tmp_df[1].astype(np.float32)   \n",
    "                \n",
    "#         self.y_train = tmp_df[3].astype(np.float32)                          \n",
    "#         self.y_train = self.mlb.fit_transform(tmp_df[1].str.split()).astype(np.float32)\n",
    "#         self.y_train = tmp_df[3].astype(np.float32)       \n",
    "#         d = defaultdict(LabelEncoder)\n",
    "#         self.y_train =tmp_df[1].apply(lambda x: d[x].fit_transform(x))\n",
    "    \n",
    "#         tmp_df=one_hot(tmp_df,tmp_df[1])\n",
    "#         self.y_train = tmp_df[1].astype(np.float32)       \n",
    "#         encoder = LabelEncoder()\n",
    "#         encoder.fit(tmp_df[1])\n",
    "#         self.y_train = encoder.transform(tmp_df[1]).astype(np.float32)\n",
    "#         self.y_train=self.y_train.reshape((self.y_train.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "                \n",
    "        lgr.info('[*]Dataset loading time {}'.format(time.time() - t))\n",
    "        lgr.info('[*] Data size is {}'.format(len(self)))\n",
    "        \n",
    "        lgr.info(\"DF CSV:\\n\" + str (tmp_df.head(5)))\n",
    "        \n",
    "        print ()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         lgr.info (\"__getitem__:\" + str(index))\n",
    "        path=self.img_path + self.X_train[index]\n",
    "        path=self.X_train[index]\n",
    "#         lgr.info (\" --- get item path:\" + path)\n",
    "        img = Image.open(path)\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None: # TypeError: batch must contain tensors, numbers, or lists; \n",
    "                                     #found <class 'PIL.Image.Image'>\n",
    "            img = self.transform(img)\n",
    "#             print (str (type(img))) # <class 'torch.FloatTensor'>                \n",
    "#         label = torch.from_numpy(self.y_train[index])\n",
    "        label = (self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        l=len(self.X_train.index)\n",
    "#         lgr.info (\"Lenght:\" +str(l))\n",
    "        return (l)       \n",
    "\n",
    "    @staticmethod        \n",
    "    def imshow(img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "    @staticmethod    \n",
    "    def flaotTensorToImage(img, mean=0, std=1):\n",
    "        \"\"\"convert a tensor to an image\"\"\"\n",
    "        img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "        img = (img*std+ mean)*255\n",
    "        img = img.astype(np.uint8)    \n",
    "        return img    \n",
    "    \n",
    "    @staticmethod\n",
    "    def toTensor(img):\n",
    "        \"\"\"convert a numpy array of shape HWC to CHW tensor\"\"\"\n",
    "        img = img.transpose((2, 0, 1)).astype(np.float32)\n",
    "        tensor = torch.from_numpy(img).float()\n",
    "        return tensor/255.0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Torch transforms.ToTensor() methood\n",
    "\n",
    "- Converts: a PIL.Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Torch DataLoader Class\n",
    "\n",
    "- Will load our GenericImageDataset\n",
    "- Can be regarded as a list (or iterator, technically). \n",
    "- Each time it is invoked will provide a minibatch of (img, label) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:CSV path d:/db/data/tf//train_v2.csv\n",
      "INFO:__main__:IMG path d:/db/data/tf//picts/train/\n",
      "INFO:__main__:DF CSV:\n",
      "                       0     1  \\\n",
      "0  00176480_nohash_0.png  down   \n",
      "1  004ae714_nohash_0.png  down   \n",
      "2  00b01445_nohash_0.png  down   \n",
      "\n",
      "                                                   2  3  \n",
      "0  d:/db/data/tf//picts/train/down/00176480_nohas...  0  \n",
      "1  d:/db/data/tf//picts/train/down/004ae714_nohas...  0  \n",
      "2  d:/db/data/tf//picts/train/down/00b01445_nohas...  0  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'down'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-207-b929b8ef4ec3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdset_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenericImageDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMG_DATA_LABELS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_EXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-205-aac2c1a615b2>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, csv_path, img_path, img_ext, transform)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Must be reshaped for PyTorch!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mtmp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtmp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-205-aac2c1a615b2>\u001b[0m in \u001b[0;36mone_hot\u001b[1;34m(df, cols)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \"\"\"\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mdummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummies\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   2475\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2476\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 2477\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   2478\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2479\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'down'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "dset_train = GenericImageDataset(IMG_DATA_LABELS,IMG_PATH,IMG_EXT,transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train Validation Split\n",
    "\n",
    "- Since there is no train_test_split method in PyTorch, we have to split a Training dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offest:21076\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001D7CB80F908>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001D7CB15F588>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 # on GTX 1080\n",
    "global_epoches = 10\n",
    "LR = 0.0005\n",
    "MOMENTUM = 0.95\n",
    "validationRatio=0.11    \n",
    "\n",
    "class FullTrainningDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainningDataset, self).__init__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.full_ds[i+self.offset]\n",
    "    \n",
    "\n",
    "\n",
    "def trainTestSplit(dataset, val_share=validationRatio):\n",
    "    val_offset = int(len(dataset)*(1-val_share))\n",
    "    print(\"Offest:\" + str(val_offset))\n",
    "    return FullTrainningDataset(dataset, 0, val_offset), FullTrainningDataset(dataset, val_offset, len(dataset)-val_offset)\n",
    "\n",
    " \n",
    "train_ds, val_ds = trainTestSplit(dset_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(train_loader)\n",
    "print(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test the DataLoader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:i=0: \n",
      "INFO:__main__:i=1: \n",
      "INFO:__main__:i=2: \n",
      "INFO:__main__:i=3: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAABvCAYAAACjFLT2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d7b6be36a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imagesToShow=4\n",
    "\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    lgr.info('i=%d: '%(i))            \n",
    "    images, labels = data            \n",
    "    num = len(images)\n",
    "    \n",
    "    ax = plt.subplot(1, imagesToShow, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    for n in range(num):\n",
    "        image=images[n]\n",
    "        label=labels[n]\n",
    "        plt.imshow (GenericImageDataset.flaotTensorToImage(image))\n",
    "        \n",
    "    if i==imagesToShow-1:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The NN model\n",
    "\n",
    "- We will use a several CNNs with conv(3x3) -> bn -> relu -> pool(4x4) -> fc.\n",
    "\n",
    "- In PyTorch, a model is defined by a subclass of nn.Module. It has two methods:\n",
    "\n",
    "- `__init__: constructor. Create layers here. Note that we don't define the connections between layers in this function.`\n",
    "\n",
    "\n",
    "- `forward(x): forward function. Receives an input variable x. Returns a output variable. Note that we actually connect the layers here dynamically.` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n",
      "INFO:__main__:Model Net (\n",
      "  (avgpool): AdaptiveAvgPool2d (output_size=1)\n",
      "  (cnn1): ConvCNN (\n",
      "    (math): Sequential (\n",
      "      (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (2): LeakyReLU (0.01)\n",
      "      (3): MaxPool2d (size=(4, 4), stride=(4, 4), dilation=(1, 1))\n",
      "    )\n",
      "    (avgpool): AvgPool2d (size=4, stride=4, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      "  (cnn2): ConvCNN (\n",
      "    (math): Sequential (\n",
      "      (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (2): LeakyReLU (0.01)\n",
      "      (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    )\n",
      "    (avgpool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      "  (cnn3): ConvCNN (\n",
      "    (math): Sequential (\n",
      "      (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (2): LeakyReLU (0.01)\n",
      "      (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    )\n",
      "    (avgpool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      "  (res1): ConvRes (\n",
      "    (math): Sequential (\n",
      "      (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(2, 2))\n",
      "      (2): PReLU (1)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential (\n",
      "    (0): ConvCNN (\n",
      "      (math): Sequential (\n",
      "        (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1), padding=(2, 2))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (2): LeakyReLU (0.01)\n",
      "        (3): MaxPool2d (size=(4, 4), stride=(4, 4), dilation=(1, 1))\n",
      "      )\n",
      "      (avgpool): AvgPool2d (size=4, stride=4, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (1): Dropout (p = 0.3)\n",
      "    (2): ConvCNN (\n",
      "      (math): Sequential (\n",
      "        (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (2): LeakyReLU (0.01)\n",
      "        (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "      )\n",
      "      (avgpool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (3): ConvCNN (\n",
      "      (math): Sequential (\n",
      "        (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (2): LeakyReLU (0.01)\n",
      "        (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "      )\n",
      "      (avgpool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (4): ConvRes (\n",
      "      (math): Sequential (\n",
      "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(2, 2))\n",
      "        (2): PReLU (1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Linear (3136 -> 1)\n",
      "  )\n",
      "  (sig): Sigmoid ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dropout = torch.nn.Dropout(p=0.30)\n",
    "class ConvRes(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(ConvRes, self).__init__()\n",
    "        drate = .3\n",
    "        self.math = nn.Sequential(\n",
    "            nn.BatchNorm2d(insize),\n",
    "            # nn.Dropout(drate),\n",
    "            torch.nn.Conv2d(insize, outsize, kernel_size=2, padding=2),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.math(x)\n",
    "\n",
    "\n",
    "class ConvCNN(nn.Module):\n",
    "    def __init__(self, insize, outsize, kernel_size=7, padding=2, pool=2, avg=True):\n",
    "        super(ConvCNN, self).__init__()\n",
    "        self.avg = avg\n",
    "        self.math = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(insize, outsize, kernel_size=kernel_size, padding=padding),\n",
    "            torch.nn.BatchNorm2d(outsize),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(pool, pool),\n",
    "        )\n",
    "        self.avgpool = torch.nn.AvgPool2d(pool, pool)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.math(x)\n",
    "        if self.avg is True:\n",
    "            x = self.avgpool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.cnn1 = ConvCNN(3, 32, kernel_size=7, pool=4, avg=False)\n",
    "        self.cnn2 = ConvCNN(32, 32, kernel_size=5, pool=2, avg=True)\n",
    "        self.cnn3 = ConvCNN(32, 32, kernel_size=5, pool=2, avg=True)\n",
    "\n",
    "        self.res1 = ConvRes(32, 64)\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            self.cnn1, dropout,\n",
    "            self.cnn2,\n",
    "            self.cnn3,\n",
    "            self.res1,\n",
    "        )\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(3136, 1),\n",
    "        )\n",
    "        self.sig = nn.Sigmoid()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "#         print (x.data.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print (x.data.shape)\n",
    "        x = self.classifier(x)\n",
    "#         print (x.data.shape)\n",
    "        x = self.sig(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")\n",
    "    model = Net().cuda() # On GPU\n",
    "else:\n",
    "    lgr.info (\"Using the CPU\")\n",
    "    model = Net() # On CPU\n",
    "\n",
    "lgr.info('Model {}'.format(model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Loss and Optimizer\n",
    "\n",
    "- Select a loss function and the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n",
      "INFO:__main__:<torch.optim.adam.Adam object at 0x000001D7CB34D630>\n",
      "INFO:__main__:MultiLabelSoftMarginLoss (\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loss_func=torch.nn.BCELoss()\n",
    "loss_func = nn.MultiLabelSoftMarginLoss()\n",
    "# loss_func = torch.nn.CrossEntropyLoss()\n",
    "# NN params\n",
    "LR = 0.005\n",
    "MOMENTUM= 0.9\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=5e-5) #  L2 regularization\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    model.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Start training in Batches\n",
    "\n",
    "See example here:\n",
    "http://codegists.com/snippet/python/pytorch_mnistpy_kernelmode_python\n",
    "\n",
    "https://github.com/pytorch/examples/blob/53f25e0d0e2710878449900e1e61d31d34b63a9d/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Train Epoch: 1 [0/21076 (0%)]\tLoss: 0.660719\n",
      "INFO:__main__:Train Epoch: 1 [1600/21076 (8%)]\tLoss: 0.387869\n",
      "INFO:__main__:Train Epoch: 1 [3200/21076 (15%)]\tLoss: 0.380889\n",
      "INFO:__main__:Train Epoch: 1 [4800/21076 (23%)]\tLoss: 0.382154\n",
      "INFO:__main__:Train Epoch: 1 [6400/21076 (30%)]\tLoss: 0.381655\n",
      "INFO:__main__:Train Epoch: 1 [8000/21076 (38%)]\tLoss: 0.381613\n",
      "INFO:__main__:Train Epoch: 1 [9600/21076 (46%)]\tLoss: 0.381652\n",
      "INFO:__main__:Train Epoch: 1 [11200/21076 (53%)]\tLoss: 0.381373\n",
      "INFO:__main__:Train Epoch: 1 [12800/21076 (61%)]\tLoss: 0.380879\n",
      "INFO:__main__:Train Epoch: 1 [14400/21076 (68%)]\tLoss: 0.368087\n",
      "INFO:__main__:Train Epoch: 1 [16000/21076 (76%)]\tLoss: 0.381020\n",
      "INFO:__main__:Train Epoch: 1 [17600/21076 (83%)]\tLoss: 0.380616\n",
      "INFO:__main__:Train Epoch: 1 [19200/21076 (91%)]\tLoss: 0.380649\n",
      "INFO:__main__:Train Epoch: 1 [20800/21076 (99%)]\tLoss: 0.380856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Train Epoch: 2 [0/21076 (0%)]\tLoss: 0.384842\n",
      "INFO:__main__:Train Epoch: 2 [1600/21076 (8%)]\tLoss: 0.382288\n",
      "INFO:__main__:Train Epoch: 2 [3200/21076 (15%)]\tLoss: 0.380787\n",
      "INFO:__main__:Train Epoch: 2 [4800/21076 (23%)]\tLoss: 0.381129\n",
      "INFO:__main__:Train Epoch: 2 [6400/21076 (30%)]\tLoss: 0.380467\n",
      "INFO:__main__:Train Epoch: 2 [8000/21076 (38%)]\tLoss: 0.380020\n",
      "INFO:__main__:Train Epoch: 2 [9600/21076 (46%)]\tLoss: 0.380397\n",
      "INFO:__main__:Train Epoch: 2 [11200/21076 (53%)]\tLoss: 0.380416\n",
      "INFO:__main__:Train Epoch: 2 [12800/21076 (61%)]\tLoss: 0.380399\n",
      "INFO:__main__:Train Epoch: 2 [14400/21076 (68%)]\tLoss: 0.380360\n",
      "INFO:__main__:Train Epoch: 2 [16000/21076 (76%)]\tLoss: 0.380539\n",
      "INFO:__main__:Train Epoch: 2 [17600/21076 (83%)]\tLoss: 0.380192\n",
      "INFO:__main__:Train Epoch: 2 [19200/21076 (91%)]\tLoss: 0.380295\n",
      "INFO:__main__:Train Epoch: 2 [20800/21076 (99%)]\tLoss: 0.380580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Train Epoch: 3 [0/21076 (0%)]\tLoss: 0.382088\n",
      "INFO:__main__:Train Epoch: 3 [1600/21076 (8%)]\tLoss: 0.380348\n",
      "INFO:__main__:Train Epoch: 3 [3200/21076 (15%)]\tLoss: 0.380432\n",
      "INFO:__main__:Train Epoch: 3 [4800/21076 (23%)]\tLoss: 0.380840\n",
      "INFO:__main__:Train Epoch: 3 [6400/21076 (30%)]\tLoss: 0.380383\n",
      "INFO:__main__:Train Epoch: 3 [8000/21076 (38%)]\tLoss: 0.380638\n",
      "INFO:__main__:Train Epoch: 3 [9600/21076 (46%)]\tLoss: 0.380375\n",
      "INFO:__main__:Train Epoch: 3 [11200/21076 (53%)]\tLoss: 0.380404\n",
      "INFO:__main__:Train Epoch: 3 [12800/21076 (61%)]\tLoss: 0.380396\n",
      "INFO:__main__:Train Epoch: 3 [14400/21076 (68%)]\tLoss: 0.380356\n",
      "INFO:__main__:Train Epoch: 3 [16000/21076 (76%)]\tLoss: 0.380327\n",
      "INFO:__main__:Train Epoch: 3 [17600/21076 (83%)]\tLoss: 0.380345\n",
      "INFO:__main__:Train Epoch: 3 [19200/21076 (91%)]\tLoss: 0.380350\n",
      "INFO:__main__:Train Epoch: 3 [20800/21076 (99%)]\tLoss: 0.380327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Train Epoch: 4 [0/21076 (0%)]\tLoss: 0.381658\n",
      "INFO:__main__:Train Epoch: 4 [1600/21076 (8%)]\tLoss: 0.380393\n",
      "INFO:__main__:Train Epoch: 4 [3200/21076 (15%)]\tLoss: 0.384536\n",
      "INFO:__main__:Train Epoch: 4 [4800/21076 (23%)]\tLoss: 0.380313\n",
      "INFO:__main__:Train Epoch: 4 [6400/21076 (30%)]\tLoss: 0.380320\n",
      "INFO:__main__:Train Epoch: 4 [8000/21076 (38%)]\tLoss: 0.380328\n",
      "INFO:__main__:Train Epoch: 4 [9600/21076 (46%)]\tLoss: 0.380326\n",
      "INFO:__main__:Train Epoch: 4 [11200/21076 (53%)]\tLoss: 0.380332\n",
      "INFO:__main__:Train Epoch: 4 [12800/21076 (61%)]\tLoss: 0.380340\n",
      "INFO:__main__:Train Epoch: 4 [14400/21076 (68%)]\tLoss: 0.380334\n",
      "INFO:__main__:Train Epoch: 4 [16000/21076 (76%)]\tLoss: 0.380366\n",
      "INFO:__main__:Train Epoch: 4 [17600/21076 (83%)]\tLoss: 0.380299\n",
      "INFO:__main__:Train Epoch: 4 [19200/21076 (91%)]\tLoss: 0.380178\n",
      "INFO:__main__:Train Epoch: 4 [20800/21076 (99%)]\tLoss: 0.380448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 295.395 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh76ZWQdx6JuZdRCHvplZB3Hom5l1kP8PZFCJwTusbn0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d7cbb450b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    " \n",
    "\n",
    "    \n",
    "clf=model \n",
    "opt= optimizer\n",
    "loss_history = []\n",
    "acc_history = []\n",
    " \n",
    "def train(epoch):\n",
    "    clf.train() # set model in training mode (need this because of dropout)\n",
    "     \n",
    "    # dataset API gives us pythonic batching \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        if use_cuda:\n",
    "            data, target = Variable(data.cuda(async=True)), Variable(target.cuda(async=True)) # On GPU                \n",
    "        else:            \n",
    "            data, target = Variable(data), Variable(target) # RuntimeError: expected CPU tensor (got CUDA tensor)                           \n",
    "                 \n",
    "        # forward pass, calculate loss and backprop!\n",
    "        opt.zero_grad()\n",
    "        preds = clf(data)\n",
    "        if use_cuda:\n",
    "            loss = F.binary_cross_entropy(preds, target).cuda()\n",
    "#             loss = F.log_softmax(preds).cuda() # TypeError: log_softmax() takes exactly 1 argument (2 given)\n",
    "#             loss = F.nll_loss(preds, target).cuda() # https://github.com/torch/cutorch/issues/227\n",
    "            \n",
    "        else:\n",
    "            loss = F.binary_cross_entropy(preds, target)\n",
    "#             loss = F.log_softmax(preds)\n",
    "#             loss = F.nll_loss(preds, target.long()) # RuntimeError: multi-target not supported at /pytorch/torch/lib/THNN/generic/ClassNLLCriterion.c:22\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            loss_history.append(loss.data[0])\n",
    "            lgr.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data[0]))              \n",
    "\n",
    "            \n",
    "start_time = time.time()    \n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    train(epoch)    \n",
    "end_time = time.time()\n",
    "print ('{} {:6.3f} seconds'.format('GPU:', end_time-start_time))\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "*****:\n",
      "[1/10] Loss: 0.694946\n",
      "[1/10] Loss: 0.694047\n",
      "[1/10] Loss: 0.693747\n",
      "[1/10] Loss: 0.693598\n",
      "[1/10] Loss: 0.693508\n",
      "[1/10] Loss: 0.693449\n",
      "[1/10] Loss: 0.693407\n",
      "[1/10] Loss: 0.693376\n",
      "[1/10] Loss: 0.693351\n",
      "[1/10] Loss: 0.693331\n",
      "[1/10] Loss: 0.693315\n",
      "[1/10] Loss: 0.693302\n",
      "[1/10] Loss: 0.693291\n",
      "Finish 1 epoch, Loss: 0.693289\n",
      "VALIDATION Loss: 0.693160\n",
      "\n",
      "Epoch 2\n",
      "*****:\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "[2/10] Loss: 0.693147\n",
      "Finish 2 epoch, Loss: 0.693147\n",
      "VALIDATION Loss: 0.693147\n",
      "\n",
      "Epoch 3\n",
      "*****:\n",
      "[3/10] Loss: 0.693147\n",
      "[3/10] Loss: 0.693147\n",
      "[3/10] Loss: 0.693147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-204-44cbb70c513c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mrunning_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-199-de5ead186a4e>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-190-5472ab23158d>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m#         lgr.info (\" --- get item path:\" + path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# TypeError: batch must contain tensors, numbers, or lists;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                                      \u001b[1;31m#found <class 'PIL.Image.Image'>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    873\u001b[0m         \"\"\"\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"P\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m                         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                         \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "criterion = loss_func\n",
    "all_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for epoch in range(global_epoches):\n",
    "        print('Epoch {}'.format(epoch + 1))\n",
    "        print('*' * 5 + ':')\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, data in enumerate(train_loader, 1):\n",
    "    \n",
    "            img, label = data\n",
    "            if use_cuda:\n",
    "                img, label = Variable(img.cuda(async=True)), Variable(label.cuda(async=True))  # On GPU\n",
    "            else:\n",
    "                img, label = Variable(img), Variable(\n",
    "                    label)  # RuntimeError: expected CPU tensor (got CUDA tensor)\n",
    "    \n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            running_loss += loss.data[0] * label.size(0)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i % 100 == 0:\n",
    "                all_losses.append(running_loss / (batch_size * i))\n",
    "                print('[{}/{}] Loss: {:.6f}'.format(\n",
    "                    epoch + 1, global_epoches, running_loss / (batch_size * i),\n",
    "                    running_acc / (batch_size * i)))\n",
    "                \n",
    "    \n",
    "#                 loss_cost = loss.data[0]                                \n",
    "#                 # RuntimeError: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). \n",
    "#                 # Use .cpu() to move the tensor to host memory first.        \n",
    "#                 prediction = (model(img).data).float() # probabilities         \n",
    "#         #         prediction = (net(X_tensor).data > 0.5).float() # zero or one\n",
    "#         #         print (\"Pred:\" + str (prediction)) # Pred:Variable containing: 0 or 1\n",
    "#         #         pred_y = prediction.data.numpy().squeeze()            \n",
    "#                 pred_y = prediction.cpu().numpy().squeeze()\n",
    "#                 target_y = label.cpu().data.numpy()\n",
    "\n",
    "#                 tu = (log_loss(target_y, pred_y),roc_auc_score(target_y,pred_y ))\n",
    "#                 print ('LOG_LOSS={}, ROC_AUC={} '.format(*tu))  \n",
    "        \n",
    "    \n",
    "        print('Finish {} epoch, Loss: {:.6f}'.format(epoch + 1, running_loss / (len(train_ds))))\n",
    "    \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        for data in val_loader:\n",
    "            img, label = data\n",
    "    \n",
    "            if use_cuda:\n",
    "                img, label = Variable(img.cuda(async=True), volatile=True),Variable(label.cuda(async=True), volatile=True)  # On GPU\n",
    "            else:\n",
    "                img = Variable(img, volatile=True)\n",
    "                label = Variable(label, volatile=True)\n",
    "    \n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.data[0] * label.size(0)\n",
    "    \n",
    "        print('VALIDATION Loss: {:.6f}'.format(eval_loss / (len(val_ds))))\n",
    "        val_losses.append(eval_loss / (len(val_ds)))\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "jupyter nbconvert \\\n",
    "    --to=slides \\\n",
    "    --reveal-prefix=https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/ \\\n",
    "    --output=py09.html \\\n",
    "    './09 PyTorch Kaggle Image Data-set loading with CNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "livereveal": {
   "controls": "true",
   "history": "true",
   "mouseWheel": "true",
   "overview": "true",
   "progress": "true",
   "scroll": "true",
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
