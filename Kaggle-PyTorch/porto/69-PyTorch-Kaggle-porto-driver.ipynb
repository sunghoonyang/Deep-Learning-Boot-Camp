{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Bootcamp November 2017, GPU Computing for Data Scientists\n",
    "\n",
    "<img src=\"../images/bcamp.png\" align=\"center\">\n",
    "\n",
    "## 69-PyTorch-Kaggle-porto-driver\n",
    "\n",
    "Web: https://www.meetup.com/Tel-Aviv-Deep-Learning-Bootcamp/events/241762893/\n",
    "\n",
    "Notebooks: <a href=\"https://github.com/QuantScientist/Data-Science-PyCUDA-GPU\"> On GitHub</a>\n",
    "\n",
    "*Shlomo Kashani*\n",
    "\n",
    "<img src=\"../images/pt.jpg\" width=\"35%\" align=\"center\">\n",
    "\n",
    "\n",
    "### Data\n",
    "- Download from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:1.3.0\n",
      "__Python VERSION: 3.6.2 |Anaconda custom (64-bit)| (default, Sep 19 2017, 08:03:39) [MSC v.1900 64 bit (AMD64)]\n",
      "__pyTorch VERSION: 0.2.1+a4fc05a\n",
      "__CUDA VERSION\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "OS:  win32\n",
      "Python:  3.6.2 |Anaconda custom (64-bit)| (default, Sep 19 2017, 08:03:39) [MSC v.1900 64 bit (AMD64)]\n",
      "PyTorch:  0.2.1+a4fc05a\n",
      "Numpy:  1.13.3\n",
      "3.6.2 |Anaconda custom (64-bit)| (default, Sep 19 2017, 08:03:39) [MSC v.1900 64 bit (AMD64)]\n",
      "0.0\n",
      "svmem(total=68627443712, available=47211114496, percent=31.2, used=21416329216, free=47211114496)\n",
      "memory GB: 0.18772506713867188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:USE CUDA=True\n"
     ]
    }
   ],
   "source": [
    "# !pip install pycuda\n",
    "%reset -f\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "import logging\n",
    "import numpy\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots\n",
    "import tensorflow as tf \n",
    "print(\"tensorflow:\" + tf.__version__)\n",
    "!set \"KERAS_BACKEND=tensorflow\"\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# print('__CUDNN VERSION:', torch.backends.cudnn.lib.cudnnGetErrorString())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu75/torch-0.2.0.post1-cp27-cp27mu-manylinux1_x86_64.whl\n",
    "# !pip install torchvision \n",
    "# ! pip install cv2\n",
    "# import cv2\n",
    "\n",
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()\n",
    "# memReport()\n",
    "\n",
    "# %%timeit\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "\n",
    "# use_cuda=False\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix seed\n",
    "seed=17*19\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)    \n",
    "# ! dir    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  View the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (595212, 59)\n",
      "Test shape: (892816, 58)\n",
      "Columns: Index(['id', 'target', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03',\n",
      "       'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin',\n",
      "       'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
      "       'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15',\n",
      "       'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01',\n",
      "       'ps_reg_02', 'ps_reg_03', 'ps_car_01_cat', 'ps_car_02_cat',\n",
      "       'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat',\n",
      "       'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat',\n",
      "       'ps_car_11_cat', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14',\n",
      "       'ps_car_15', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04',\n",
      "       'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09',\n",
      "       'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14',\n",
      "       'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin',\n",
      "       'ps_calc_19_bin', 'ps_calc_20_bin'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  \\\n",
       "0   7       0          2              2          5              1   \n",
       "1   9       0          1              1          7              0   \n",
       "2  13       0          5              4          9              1   \n",
       "3  16       0          0              1          2              0   \n",
       "4  17       0          0              2          0              1   \n",
       "\n",
       "   ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin       ...        \\\n",
       "0              0              0              1              0       ...         \n",
       "1              0              0              0              1       ...         \n",
       "2              0              0              0              1       ...         \n",
       "3              0              1              0              0       ...         \n",
       "4              0              1              0              0       ...         \n",
       "\n",
       "   ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0           9           1           5           8               0   \n",
       "1           3           1           1           9               0   \n",
       "2           4           2           7           7               0   \n",
       "3           2           2           4           9               0   \n",
       "4           3           1           1           3               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               1               1               0               1   \n",
       "2               1               1               0               1   \n",
       "3               0               0               0               0   \n",
       "4               0               0               1               1   \n",
       "\n",
       "   ps_calc_20_bin  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc; gc.enable()\n",
    "# !pip install xgboost\n",
    "import xgboost as xgb\n",
    "# http://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "import sklearn\n",
    "\n",
    "# Data params\n",
    "TARGET_VAR= 'target'\n",
    "BASE_FOLDER = 'd:/db/data/porto/'\n",
    "\n",
    "# Read in our input data\n",
    "df_train = pd.read_csv(BASE_FOLDER + '/train.csv')\n",
    "df_test = pd.read_csv(BASE_FOLDER + '/test.csv')\n",
    "# This prints out (rows, columns) in each dataframe\n",
    "print('Train shape:', df_train.shape)\n",
    "print('Test shape:', df_test.shape)\n",
    "\n",
    "print('Columns:', df_train.columns)\n",
    "\n",
    "# y_train = df_train['target'].values\n",
    "# id_train = df_train['id'].values\n",
    "# id_test = df_test['id'].values\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595200, 130) (892816, 131)\n"
     ]
    }
   ],
   "source": [
    "# from rgf.sklearn import RGFClassifier     # https://github.com/fukatani/rgf_python\n",
    "\n",
    "train = df_train\n",
    "test = df_test\n",
    "\n",
    "# Preprocessing (Forza Baseline)\n",
    "id_test = test['id'].values\n",
    "\n",
    "col = [c for c in train.columns if c not in ['id','target']]\n",
    "col = [c for c in col if not c.startswith('ps_calc_')]\n",
    "\n",
    "train = train.replace(-1, np.NaN)\n",
    "d_median = train.median(axis=0)\n",
    "d_mean = train.mean(axis=0)\n",
    "train = train.fillna(-1)\n",
    "one_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}\n",
    "\n",
    "\n",
    "def transform(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    dcol = [c for c in df.columns if c not in ['id','target']]\n",
    "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n",
    "    for c in dcol:\n",
    "        if '_bin' not in c:\n",
    "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n",
    "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n",
    "\n",
    "    for c in one_hot:\n",
    "        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\n",
    "            for val in one_hot[c]:\n",
    "                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = transform(train)\n",
    "test = transform(test)\n",
    "\n",
    "col = [c for c in train.columns if c not in ['id','target']]\n",
    "col = [c for c in col if not c.startswith('ps_calc_')]\n",
    "\n",
    "col_test = [c for c in test.columns if c not in ['target']]\n",
    "col_test = [c for c in col_test if not c.startswith('ps_calc_')]\n",
    "\n",
    "\n",
    "dups = train[train.duplicated(subset=col, keep=False)]\n",
    "\n",
    "train = train[~(train['id'].isin(dups['id'].values))]\n",
    "\n",
    "y_train = train['target'].values\n",
    "id_train = train['id'].values\n",
    "id_test = test['id'].values\n",
    "\n",
    "target_train = train['target']\n",
    "train = train[col]\n",
    "test = test[col_test]\n",
    "print(train.values.shape, test.values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Train / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 476160 Validation samples: 119040\n"
     ]
    }
   ],
   "source": [
    "x_train = train\n",
    "x_test = test\n",
    "\n",
    "# Take a random 20% of the dataset as validation data\n",
    "trainX, valX, trainY, valY = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n",
    "print('Train samples: {} Validation samples: {}'.format(len(trainX), len(valX)))\n",
    "\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# n_components = 35\n",
    "# pca = PCA(n_components=n_components, svd_solver='randomized',whiten=True).fit(trainX)\n",
    "# trainX = pca.transform(trainX)\n",
    "\n",
    "\n",
    "N_FEATURES=trainX.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  From Numpy to PyTorch GPU tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np, dtype=np.float32)        \n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")\n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "    \n",
    "    print(type(X_tensor.data)) # should be 'torch.cuda.FloatTensor'            \n",
    "    print((X_tensor.data.shape)) # torch.Size([108405, 29])\n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n",
    "\n",
    "    print(type(Y_tensor.data)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))    \n",
    "    return Y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # p is the probability of being dropped in PyTorch\n",
    "\n",
    "# # NN params\n",
    "# DROPOUT_PROB = 0.65\n",
    "\n",
    "# LR = 0.005\n",
    "# MOMENTUM= 0.85\n",
    "# dropout = torch.nn.Dropout(p=1 - (DROPOUT_PROB))\n",
    "\n",
    "# lgr.info(dropout)\n",
    "\n",
    "# hiddenLayer1Size=int(N_FEATURES/4)\n",
    "# hiddenLayer2Size=int(N_FEATURES)\n",
    "\n",
    "# linear1=torch.nn.Linear(N_FEATURES, hiddenLayer1Size, bias=True) \n",
    "# torch.nn.init.xavier_uniform(linear1.weight)\n",
    "\n",
    "# linear2=torch.nn.Linear(hiddenLayer1Size, N_FEATURES)\n",
    "# torch.nn.init.xavier_uniform(linear2.weight)\n",
    "\n",
    "# linear3=torch.nn.Linear(N_FEATURES, hiddenLayer2Size)\n",
    "# torch.nn.init.xavier_uniform(linear3.weight)\n",
    "\n",
    "# linear4=torch.nn.Linear(hiddenLayer2Size, hiddenLayer1Size)\n",
    "# torch.nn.init.xavier_uniform(linear4.weight)\n",
    "\n",
    "# linear5=torch.nn.Linear(hiddenLayer2Size, hiddenLayer1Size)\n",
    "# torch.nn.init.xavier_uniform(linear5.weight)\n",
    "\n",
    "# linear6=torch.nn.Linear(hiddenLayer1Size, 1)\n",
    "# torch.nn.init.xavier_uniform(linear6.weight)\n",
    "\n",
    "# sigmoid = torch.nn.Sigmoid()\n",
    "# tanh=torch.nn.Tanh()\n",
    "# relu=torch.nn.LeakyReLU()\n",
    "# prelu=torch.nn.PReLU()\n",
    "\n",
    "\n",
    "# net = torch.nn.Sequential(linear1,prelu,nn.BatchNorm1d(hiddenLayer1Size),dropout,\n",
    "#                           linear2,prelu,nn.BatchNorm1d(N_FEATURES),dropout,                                                    \n",
    "#                           linear3,prelu,nn.BatchNorm1d(hiddenLayer2Size),dropout,\n",
    "#                           linear4,prelu,nn.BatchNorm1d(hiddenLayer1Size),dropout,\n",
    "#                           linear6,sigmoid\n",
    "#                           )\n",
    "\n",
    "\n",
    "# lgr.info(net)  # net architecture\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=LR,weight_decay=5e-4) #  L2 regularization\n",
    "# loss_func=torch.nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "\n",
    "# if use_cuda:\n",
    "#     lgr.info (\"Using the GPU\")    \n",
    "#     net.cuda()\n",
    "#     loss_func.cuda()\n",
    "# #     cudnn.benchmark = True\n",
    "\n",
    "# lgr.info (optimizer)\n",
    "# lgr.info (loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476160, 130)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([476160, 130])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:CNNNumerAI (\n",
      "  (features): Sequential (\n",
      "    (0): Conv1d(130, 130, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "    (1): LeakyReLU (0.01)\n",
      "    (2): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Linear (130 -> 1)\n",
      "    (1): Dropout (p = 0.25)\n",
      "    (2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (sig): Sigmoid ()\n",
      ")\n",
      "INFO:__main__:Using the GPU\n",
      "INFO:__main__:<torch.optim.adam.Adam object at 0x00000257918797F0>\n",
      "INFO:__main__:BCELoss (\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after view, (size():torch.Size([476160, 130, 1])\n",
      "after CNN, (size():torch.Size([476160, 130, 1])\n",
      "after 2nd view, (size():torch.Size([476160, 130])\n",
      "after self.out, (size():torch.Size([476160, 1])\n",
      "(b.size():torch.Size([476160, 1])\n"
     ]
    }
   ],
   "source": [
    "use_cuda=True\n",
    "torch.backends.cudnn.enabled=False\n",
    "N_FEATURES=trainX.shape[1]\n",
    "LR=0.005\n",
    "X_tensor_train= XnumpyToTensor(trainX) # default order is NBC for a 3d tensor, but we have a 2d tensor\n",
    "X_shape=X_tensor_train.data.size()\n",
    "\n",
    "# 21      6            3         108405       21*32=672    torch.Size([108405, 672, 2] n_max_pool1d=1\n",
    "\n",
    "n_mult_factor=1\n",
    "n_input= trainX.shape[1]\n",
    "n_hidden= n_input * n_mult_factor\n",
    "n_output=1\n",
    "n_input_rows=trainX.shape[0]\n",
    "n_cnn_kernel=3\n",
    "n_padding=2\n",
    "\n",
    "n_max_pool1d=2\n",
    "\n",
    "DEBUG_ON=True\n",
    "def debug(msg, x):\n",
    "    if DEBUG_ON:\n",
    "        print (msg + ', (size():' + str (x.size()))\n",
    "    \n",
    "class CNNNumerAI(nn.Module):    \n",
    "    def __init__(self, n_input, n_hidden, n_output,n_cnn_kernel, n_mult_factor, n_padding,n_max_pool1d):\n",
    "        super(CNNNumerAI, self).__init__()    \n",
    "        self.n_input=n_input\n",
    "        self.n_hidden=n_hidden\n",
    "        self.n_output= n_output \n",
    "        self.n_cnn_kernel=n_cnn_kernel\n",
    "        self.n_mult_factor=n_mult_factor\n",
    "        self.n_padding=n_padding\n",
    "        self.n_max_pool1d=n_max_pool1d\n",
    "        self.n_l1=int((n_mult_factor * self.n_input) * (n_padding + 1) / n_max_pool1d)\n",
    "                    \n",
    "#         drate = .3\n",
    "#         self.math = nn.Sequential(\n",
    "#                                  nn.BatchNorm1d(insize),\n",
    "#                                  nn.Dropout(drate),\n",
    "#                                  nn.Linear(insize, outsize),\n",
    "#                                  nn.PReLU(),\n",
    "#                                 )\n",
    "        \n",
    "        self.features = nn.Sequential( # Mimicking AlexNet \n",
    "            torch.nn.Conv1d(self.n_input, self.n_hidden,kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(self.n_padding,)),                                             \n",
    "            torch.nn.LeakyReLU(),            \n",
    "            torch.nn.MaxPool1d(kernel_size=self.n_max_pool1d),\n",
    "            \n",
    "#             torch.nn.Conv1d(self.n_hidden,self.n_hidden*2,kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(self.n_padding,)),                                             \n",
    "#             torch.nn.LeakyReLU(),            \n",
    "#             torch.nn.MaxPool1d(kernel_size=self.n_max_pool1d),\n",
    "            \n",
    "#             torch.nn.Conv1d(self.n_hidden*2,self.n_hidden*2,kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(self.n_padding,)),                                             \n",
    "#             torch.nn.LeakyReLU(),            \n",
    "#             torch.nn.MaxPool1d(kernel_size=self.n_max_pool1d),                                                \n",
    "        )   \n",
    "                        \n",
    "        hiddenLayer2Size=int(self.n_hidden)      \n",
    "        \n",
    "        linear1=torch.nn.Linear(hiddenLayer2Size, 1)\n",
    "        torch.nn.init.xavier_uniform(linear1.weight)\n",
    "\n",
    "        dropout = torch.nn.Dropout(p=1 - (0.75))\n",
    "        relu=torch.nn.LeakyReLU()\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "                                  linear1,dropout,nn.BatchNorm1d(self.n_output)\n",
    "#                                   linear2,dropout,relu,\n",
    "#                                   linear3,dropout,relu,\n",
    "#                                   linear4,dropout,\n",
    "#                                     linear1\n",
    "                                  )                                 \n",
    "        self.sig=nn.Sigmoid()\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         debug('raw',x)   \n",
    "        varSize=x.data.shape[0] # must be calculated here in forward() since its is a dynamic size                          \n",
    "        # for CNN  \n",
    "        x=x.contiguous() \n",
    "        x = x.view(varSize,self.n_input,1)\n",
    "        debug('after view',x)   \n",
    "        x=self.features(x)\n",
    "        debug('after CNN',x)   \n",
    "        # for Linear layer\n",
    "#         x = x.view(varSize,self.n_l1) \n",
    "        x = x.view(varSize,int(self.n_hidden)) \n",
    "        debug('after 2nd view',x)                  \n",
    "        x=self.classifier(x)   \n",
    "        debug('after self.out',x)   \n",
    "        x=self.sig(x)\n",
    "        return x\n",
    "\n",
    "net = CNNNumerAI(n_input, n_hidden, n_output,n_cnn_kernel, n_mult_factor, n_padding, n_max_pool1d)    \n",
    "lgr.info(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR,weight_decay=5e-2) #  L2 regularization\n",
    "loss_func=torch.nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    net.cuda()\n",
    "    loss_func.cuda()\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)\n",
    "\n",
    "b = net(X_tensor_train)\n",
    "print ('(b.size():' + str (b.size()))\n",
    "\n",
    "\n",
    "\n",
    "# Only on windows\n",
    "torch.backends.cudnn.enabled=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNumerAI (\n",
      "  (features): Sequential (\n",
      "    (0): Conv1d(130, 130, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "    (1): LeakyReLU (0.01)\n",
      "    (2): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Linear (130 -> 1)\n",
      "    (1): Dropout (p = 0.25)\n",
      "    (2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (sig): Sigmoid ()\n",
      ")\n",
      "(476160, 130)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([476160, 130])\n",
      "(476160, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'>\n",
      "(476160, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.cuda.FloatTensor'> <class 'torch.cuda.FloatTensor'>\n",
      "0 [ 0.79993457]\n",
      "LOG_LOSS=0.8019744381691443, ROC_AUC=0.5049082783364331, GINI=0.009816556672866295\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "# for windows\n",
    "torch.backends.cudnn.enabled=False\n",
    "\n",
    "start_time = time.time()    \n",
    "epochs=1000 # change to 2000 for better results\n",
    "div_factor=100\n",
    "all_losses = []\n",
    "loss_arr =[]\n",
    "DEBUG_ON=False\n",
    "\n",
    "print (net)\n",
    "\n",
    "\n",
    "\n",
    "X_tensor_train= XnumpyToTensor(trainX)\n",
    "Y_tensor_train= YnumpyToTensor(trainY)\n",
    "print(type(X_tensor_train.data), type(Y_tensor_train.data)) # should be 'torch.cuda.FloatTensor'\n",
    "\n",
    "# CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\n",
    "# X_tensor_train=X_tensor_train.contiguous()\n",
    "# Y_tensor_train=Y_tensor_train.contiguous()\n",
    "                \n",
    "# From here onwards, we must only use PyTorch Tensors\n",
    "for step in range(epochs):    \n",
    "    out = net(X_tensor_train)                 # input x and predict based on x\n",
    "    cost = loss_func(out, Y_tensor_train)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    cost.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "                   \n",
    "        \n",
    "    if step % div_factor == 0:        \n",
    "        loss = cost.data[0]\n",
    "        all_losses.append(loss)\n",
    "        print(step, cost.data.cpu().numpy())\n",
    "        # RuntimeError: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). \n",
    "        # Use .cpu() to move the tensor to host memory first.        \n",
    "        prediction = (net(X_tensor_train).data).float() # probabilities         \n",
    "#         prediction = (net(X_tensor).data > 0.5).float() # zero or one\n",
    "#         print (\"Pred:\" + str (prediction)) # Pred:Variable containing: 0 or 1\n",
    "#         pred_y = prediction.data.numpy().squeeze()            \n",
    "        pred_y = prediction.cpu().numpy().squeeze()\n",
    "        target_y = Y_tensor_train.cpu().data.numpy()\n",
    "                        \n",
    "        tu = (log_loss(target_y, pred_y),roc_auc_score(target_y,pred_y ), 2*roc_auc_score(target_y,pred_y ) - 1)\n",
    "        print ('LOG_LOSS={}, ROC_AUC={}, GINI={}'.format(*tu))  \n",
    "        \n",
    "        loss_arr.append(cost.cpu().data.numpy()[0])\n",
    "                \n",
    "end_time = time.time()\n",
    "print ('{} {:6.3f} seconds'.format('GPU:', end_time-start_time))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_losses)\n",
    "plt.show()\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y,pred_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('GINI:' + str(2*roc_auc_score(target_y,pred_y ) - 1))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "# Validation data\n",
    "\n",
    "# trainX, valX, trainY, valY = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n",
    "\n",
    "print (valX.shape)\n",
    "print (valY.shape)\n",
    "\n",
    "# trainX, valX, trainY, valY = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n",
    "# pca = PCA(n_components=n_components, svd_solver='randomized',whiten=True).fit(valX)\n",
    "# valX = pca.transform(valX)\n",
    "\n",
    "X_tensor_val= XnumpyToTensor(valX)\n",
    "Y_tensor_val= YnumpyToTensor(valY)\n",
    "\n",
    "\n",
    "print(type(X_tensor_val.data), type(Y_tensor_val.data)) # should be 'torch.cuda.FloatTensor'\n",
    "\n",
    "predicted_val = (net(X_tensor_val).data).float() # probabilities \n",
    "# predicted_val = (net(X_tensor_val).data > 0.5).float() # zero or one\n",
    "pred_y = predicted_val.cpu().numpy()\n",
    "target_y = Y_tensor_val.cpu().data.numpy()                \n",
    "\n",
    "print (type(pred_y))\n",
    "print (type(target_y))\n",
    "\n",
    "\n",
    "print ('\\n')\n",
    "tu = (log_loss(target_y, pred_y),roc_auc_score(target_y,pred_y ), 2*roc_auc_score(target_y,pred_y ) - 1)\n",
    "print ('LOG_LOSS={}, ROC_AUC={}, GINI={}'.format(*tu))  \n",
    "        \n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y,pred_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('GINI=' + str(2*roc_auc_score(target_y,pred_y ) - 1))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# print (pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_df_test = x_test\n",
    "print('Test shape:', X_df_test.shape)\n",
    "print('Columns:', X_df_test.columns)\n",
    "id_test = X_df_test['id'].values\n",
    "X_df_test=X_df_test.apply(lambda x: pandas.to_numeric(x, errors='ignore'))\n",
    "\n",
    "print (X_df_test.shape)\n",
    "columns = ['id', 'target']\n",
    "df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n",
    "\n",
    "\n",
    "for index, row in X_df_test.iterrows():\n",
    "    rwo_no_id=row.drop('id')    \n",
    "#     print (rwo_no_id.values)    \n",
    "    x_data_np = np.array(rwo_no_id.values, dtype=np.float32)        \n",
    "    if use_cuda:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "                    \n",
    "    X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n",
    "    predicted_val = (net(X_tensor_test).data).float() # probabilities     \n",
    "    p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n",
    "    \n",
    "    df_pred = df_pred.append({'id':row['id'], 'target':p_test},ignore_index=True)\n",
    "#     df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n",
    "\n",
    "df_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pred.id=df_pred.id.astype(int)\n",
    "\n",
    "def savePred(df_pred, loss):\n",
    "#     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n",
    "    csv_path = 'pred/pred_{}_{}.csv'.format(loss, (str(time.time())))\n",
    "    df_pred.to_csv(csv_path, columns=('id', 'target'), index=None)\n",
    "    print (csv_path)\n",
    "    \n",
    "savePred (df_pred, str(2*roc_auc_score(target_y,pred_y ) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "livereveal": {
   "controls": "true",
   "history": "true",
   "mouseWheel": "true",
   "overview": "true",
   "progress": "true",
   "scroll": "true",
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
