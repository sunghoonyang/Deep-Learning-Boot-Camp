{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Policy Gradients (PG) PyTorch Tutorial\n",
    "### Author: [Nir Ben-Zvi](nirbenz@gmail.com), on top of PyTorch's original tutorial\n",
    "\n",
    "This tutorial is a Jupyter notebook version of PyTorch's [original example code](https://github.com/pytorch/examples/tree/master/reinforcement_learning), made to run inside a Docker container. \n",
    "\n",
    "#### Task\n",
    "\n",
    "The agent has to decide between two actions - moving the cart left or right - so that the pole attached to it stays upright. You can find an official leaderboard with various algorithms and visualizations at the [Gym website](http://gym.openai.com).\n",
    "\n",
    "![alt text](./cartpole1.gif \"Cartpole\")\n",
    "\n",
    "As the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and also returns a reward that indicates the consequences of the action. In this task, the environment terminates if the pole falls over too far.\n",
    "\n",
    "The CartPole task is designed so that the inputs to the agent are 4 real values representing the environment state (position, velocity, etc.). This is a much simpler task, compared to one where the input is a raw input from the game screen - which allows us to quickly experience the agent's improvement on our screen.\n",
    "\n",
    "#### Packages\n",
    "\n",
    "Nothing really interesting here;\n",
    "- `torch` is the main PyTorch module\n",
    "- `torch.nn` for Neural Networks\n",
    "- `torch.nn.functional` \n",
    "- `torch.optim` is an optimization package\n",
    "- `torch.autograd` for auto differentiation\n",
    "- `torch.autograd.Variable` an auto-differentiable Variable (Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Now the jupyter/gym render part comes in\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# iPython\n",
    "from IPython import display, get_ipython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Input\n",
    "Keeping original code commented out, but we will have the parameters hard coded so we could run this in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "#                         help='discount factor (default: 0.99)')\n",
    "#     parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "#                         help='random seed (default: 543)')\n",
    "#     parser.add_argument('--render', action='store_true',\n",
    "#                         help='render the environment', default=False)\n",
    "#     parser.add_argument('--log_interval', type=int, default=10, metavar='N',\n",
    "#                         help='interval between training status logs (default: 10)')\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "    dictionary = {'gamma': 0.99, 'seed': 543, 'render': False, 'log_interval': 10}\n",
    "    args = namedtuple('GenericDict', dictionary.keys())(**dictionary)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Policy Reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    This defines the Policy Network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is our network's forward pass; Backward pass is created implicitly\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Framework\n",
    "\n",
    "#### Selecting Actions\n",
    "\n",
    "A state is given, for which the network (policy) computes the next action probability distribution\n",
    "From this, a new action is created and is also appended to saved_actions\n",
    "\n",
    "#### When an Episode Ends\n",
    "\n",
    "`reward` will denote a vector $\\in\\mathbb{R}^T$, such that $\\mathrm{reward}_t$ is a normalized sum of rewards up to timestep $t$. Dicount factor $\\gamma$ is taken into account.\n",
    "\n",
    "Following this, the REINFORCE algorithm is performed for every action. As we've seen in the theoretical part, this is an accepted way of differentiating stochastic units. `reinforce` is a method of `pytorch.autograd.Variable`. Note that `action` instances are probability distributions, and each one will be differentiated w.r.t. the entire vector of rewards.\n",
    "\n",
    "Following this, the optimizer is advanced as usual per neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(Variable(state))\n",
    "    action = probs.multinomial()\n",
    "    policy.saved_actions.append(action)\n",
    "    return action.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(policy, optimizer, gamma):\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for action, r in zip(policy.saved_actions, rewards):\n",
    "        action.reinforce(r)\n",
    "    optimizer.zero_grad()\n",
    "    autograd.backward(policy.saved_actions, [None for _ in policy.saved_actions])\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_actions[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Policy Network\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "We first load the `gym` environment, initialize a Policy instance and create an optimizer for it. \n",
    "\n",
    "#### Training\n",
    "\n",
    "Following this, we iterate for a number of episodes. As we've seen before, episodes are standalone interactions with the environment - each composed of $T$ timesteps. An environment interaction is roughly:\n",
    "\n",
    "- Receive an action from the model (based on current state, $s_t$)\n",
    "- Advance, receiving the tuple $(s_{t+1}, a_{t+1}, r_{t+1})$\n",
    "- Append the reward to the running list of per-step rewards\n",
    "\n",
    "We will stop when 'running_reward' raises above some predefined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast length:    13\tAverage length: 10.64\n",
      "\n",
      "Episode 20\tLast length:    24\tAverage length: 11.37\n",
      "\n",
      "Episode 30\tLast length:   115\tAverage length: 15.63\n",
      "\n",
      "Episode 40\tLast length:    17\tAverage length: 19.16\n",
      "\n",
      "Episode 50\tLast length:    77\tAverage length: 22.33\n",
      "\n",
      "Episode 60\tLast length:    52\tAverage length: 24.56\n",
      "\n",
      "Episode 70\tLast length:    67\tAverage length: 28.63\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6c554dd65043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-6c554dd65043>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mrunning_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_reward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.99\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mfinish_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             msgs.append('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}\\n'.format(\n",
      "\u001b[0;32m<ipython-input-20-d5530db6c26a>\u001b[0m in \u001b[0;36mfinish_episode\u001b[0;34m(policy, optimizer, gamma)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    policy = Policy()\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "    def show_state(env, step=0, episode=0, info=\"\"):\n",
    "        plt.figure(3)\n",
    "        plt.clf()\n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        plt.title(\"{} | Episode: {:3d}, Step: {:4d}\\n{}\".format(env.spec.id, episode, step, info))\n",
    "        plt.axis('off')\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    \n",
    "    running_reward = 10\n",
    "    msgs = ['']\n",
    "    for i_episode in count(1):\n",
    "        state = env.reset()\n",
    "        frames = []\n",
    "        for t in range(10000): # Don't infinite loop while learning\n",
    "            action = select_action(state, policy)\n",
    "            state, reward, done, _ = env.step(action[0,0])\n",
    "            if args.render:\n",
    "                show_state(env, step=t, episode=i_episode, info='\\n'.join(msgs))\n",
    "            policy.rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = running_reward * 0.99 + t * 0.01\n",
    "        finish_episode(policy, optimizer, args.gamma)\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            msgs.append('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}\\n'.format(\n",
    "                i_episode, t, running_reward))\n",
    "            if not args.render:\n",
    "                print(msgs[-1])\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "    env.render(close=True)\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
